% locking/locking.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Locking}{Locking}{qqzlocking}
%
\Epigraph{Locking is the worst general-purpose synchronization mechanism
	  except for all those other mechanisms that
	  have been tried from time to time.}{\emph{With apologies
	  to the memory of Winston Churchill and to whoever he was
	  quoting}}

최근의 동시성 연구에서, 락킹은 종종 악당의 역할을 맡게 됩니다.
락킹은 deadlock, convoying, starvation, unfairness, data race, 그리고 모든 다른
동시성에서의 죄악적인 것들을 일으킨다는 비난을 받고 있습니다.
흥미롭게도, 제품 수준 공유 메모리 병렬 소프트웨어에서의 아주 많은 일을 처리하는
사람의 역할 역시 락킹이 맡고 있습니다.
이 챕터는
Figure~\ref{fig:locking:Locking: Villain or Slob?}
와~\ref{fig:locking:Locking: Workhorse or Hero?}
에서 그린 것과 같은 이 악당과 영웅 사이의 이분법을 들여다 봅니다.

이 지킬과 하이드 이분법에는 여러 이유들이 있습니다:

\iffalse

In recent concurrency research, locking often plays the role of villain.
Locking stands accused of inciting deadlocks, convoying, starvation,
unfairness, data races, and all manner of other concurrency sins.
Interestingly enough, the role of workhorse in production-quality
shared-memory parallel software is also played by locking.
This chapter will look into this dichotomy between villain and
hero, as fancifully depicted in
Figures~\ref{fig:locking:Locking: Villain or Slob?}
and~\ref{fig:locking:Locking: Workhorse or Hero?}.

There are a number of reasons behind this Jekyll-and-Hyde dichotomy:

\fi

\begin{enumerate}
\item	락킹의 죄악 중 많은 것들은 대부분의 경우에 잘 동작하는 실용적 설계
	디자인이 있는데, 예를 들면:
	\begin{enumerate}
	\item	데드락 방지를 위해 락 계층을 사용하는 것.
	\item	리눅스 커널의 lockdep~\cite{JonathanCorbet2006lockdep} 과 같은
		데드락 탐지 도구들.
	\item	배열, 해시 테이블, radix tree 와 같은,
		챕터~\ref{chp:Data Structures} 에서 다루어질 락킹에 친화적인
		데이터 구조들.
	\end{enumerate}
\item	락킹의 죄들 중 일부는 잘 설계되지 못한 프로그램에서만 이를 수 있는 높은
	수준의 contention 에서만 존재합니다.
\item	락킹의 죄들 중 일부는 락킹과 함께 다른 동기화 메커니즘들을 사용함으로써
	방지될 수 있습니다.
	이런 다른 메커니즘들에는 통계적 카운터
	(Chapter~\ref{chp:Counting} 를 참고하세요),
	레퍼런스 카운터
	(Section~\ref{sec:defer:Reference Counting} 를 참고하세요),
	해저드 포인터
	(Section~\ref{sec:defer:Hazard Pointers} 를 참고하세요),
	시퀀스 락킹 읽기 쓰레드
	(Section~\ref{sec:defer:Sequence Locks} 를 참고하세요),
	RCU
	(see Section~\ref{sec:defer:Read-Copy Update (RCU)}),
	그리고 간단한 non-blocking 데이터 구조들
	(Section~\ref{sec:advsync:Non-Blocking Synchronization} 를 참고하세요)
	이 있습니다.

\iffalse

\item	Many of locking's sins have pragmatic design solutions that
	work well in most cases, for example:
	\begin{enumerate}
	\item	Use of lock hierarchies to avoid deadlock.
	\item	Deadlock-detection tools, for example, the Linux kernel's
		lockdep facility~\cite{JonathanCorbet2006lockdep}.
	\item	Locking-friendly data structures, such as
		arrays, hash tables, and radix trees, which will
		be covered in Chapter~\ref{chp:Data Structures}.
	\end{enumerate}
\item	Some of locking's sins are problems only at high levels of
	contention, levels reached only by poorly designed programs.
\item	Some of locking's sins are avoided by using other synchronization
	mechanisms in concert with locking.
	These other mechanisms include
	statistical counters
	(see Chapter~\ref{chp:Counting}),
	reference counters
	(see Section~\ref{sec:defer:Reference Counting}),
	hazard pointers
	(see Section~\ref{sec:defer:Hazard Pointers}),
	sequence-locking readers
	(see Section~\ref{sec:defer:Sequence Locks}),
	RCU
	(see Section~\ref{sec:defer:Read-Copy Update (RCU)}),
	and simple non-blocking data structures
	(see Section~\ref{sec:advsync:Non-Blocking Synchronization}).

\fi

\item	최근 전까지, 거의 모든 거대 공유 메모리 병렬 프로그램들이 비밀리에
	개발되었으며, 따라서 이런 실용적 해결책을 배우기가 쉽지 않았습니다.
\item	락킹은 일부 소프트웨어 작품에 대해서는 무척 잘 동작하지만 다른 것들에는
	무척 잘 동작하지 못합니다.
	락킹이 잘 동작하는 작품을 위해 일해온 개발자들은 락킹이 잘 동작하지
	못하는 작품을 위해 일해온 사람들에 비해 락킹에 대해 훨씬 긍정적인
	의견을 가지게 될 것이라 예상할 수 있는데, 이에 대해
	Section~\ref{sec:locking:Locking: Hero or Villain?} 에서 다룹니다.
\item	모든 좋은 이야기에는 악당이 필요하기 마련이며, 락킹은 연구 논문의
	희생양으로 일하는 길고 영광스러운 역사를 가지게 되었습니다.

\iffalse

\item	Until quite recently, almost all large shared-memory parallel
	programs were developed in secret, so that it was not easy
	to learn of these pragmatic solutions.
\item	Locking works extremely well for some software artifacts
	and extremely poorly for others.
	Developers who have worked on artifacts for which locking
	works well can be expected to have a much more positive
	opinion of locking than those who have worked on artifacts
	for which locking works poorly, as will be discussed in
	Section~\ref{sec:locking:Locking: Hero or Villain?}.
\item	All good stories need a villain, and locking has a long and
	honorable history serving as a research-paper whipping boy.
\fi

\end{enumerate}

\QuickQuiz{
	희생양 역할을 하는게 어떻게 영광스러운 것일 수 있죠???

	\iffalse

	Just how can serving as a whipping boy be considered to be
	in any way honorable???

	\fi

}\QuickQuizAnswer{
	락킹이 연구 논문의 희생양이 된 이유는 그것이 실제 세계에서 굉장히 많이
	사용되었기 때문입니다.
	반면에, 누구도 락킹을 사용하지 않거나 신경쓰지 않았다면, 대부분의 연구
	논문은 그걸 언급조차 하지 않았을 겁니다.

	\iffalse

	The reason locking serves as a research-paper whipping boy is
	because it is heavily used in practice.
	In contrast, if no one used or cared about locking, most research
	papers would not bother even mentioning it.

	\fi

}\QuickQuizEnd

이 챕터는 락킹의 더 심각한 죄들을 막기 위한 여러 방법을 알아봅니다.

\iffalse

This chapter will give an overview of a number of ways to avoid locking's
more serious sins.

\fi

\begin{figure}[tb]
\centering
\resizebox{2in}{!}{\includegraphics{cartoons/r-2014-Locking-the-Slob}}
\caption{Locking: Villain or Slob?}
\ContributedBy{Figure}{fig:locking:Locking: Villain or Slob?}{Melissa Broussard}
\end{figure}

\begin{figure}[tb]
\centering
\resizebox{2in}{!}{\includegraphics{cartoons/r-2014-Locking-the-Hero}}
\caption{Locking: Workhorse or Hero?}
\ContributedBy{Figure}{fig:locking:Locking: Workhorse or Hero?}{Melissa Broussard}
\end{figure}

\section{Staying Alive}
\label{sec:locking:Staying Alive}
%
\epigraph{I work to stay alive.}{\emph{Bette Davis}}

락킹이 deadlock 과 starvation 으로 인해 비난받는다는 점을 놓고 볼 때, 공유
메모리 병렬 개발자들의 중요한 걱정 중 하나는 단순히 살아있음을 유지하는
것입니다.
따라서 다음 섹션들에서는 deadlock, lovelock, starvation, unfairness, 그리고
비효율성에 대해 다룹니다.

\iffalse

Given that locking stands accused of deadlock and starvation,
one important concern for shared-memory parallel developers is
simply staying alive.
The following sections therefore cover deadlock, livelock, starvation,
unfairness, and inefficiency.

\fi

\subsection{Deadlock}
\label{sec:locking:Deadlock}

Deadlock 은 어느 쓰레드 그룹의 각 멤버가 같은 그룹 내의 다른 멤버의 락을
기다리면서 각자 최소 하나씩 락을 쥐고 있을 때 발생합니다.
이는 하나의 쓰레드만 가지고 있는 그룹들에서도 이 쓰레드가 이미 쥐고 있는 비
회귀적인 락을 획득하려 할 때 발생합니다.
Deadlock 은 따라서 하나의 쓰레드와 하나의 락만 존재할 때조차도 일어날 수
있습니다!

어떤 외부 개입이 없이는, deadlock 은 영원히 갑니다.
어떤 쓰레드도 그것을 쥐고 있는 쓰레드가 그 락을 해제하기 전까지는 그 락을
획득할 수 없으나, 그 락을 쥐고 있는 쓰레드는 기다리고 있는 락을 획득하기
전까지는 쥐고 있는 락을 놓을 수 없습니다.

\iffalse

Deadlock occurs when each member of a group of threads is holding at
least one lock while at the same time waiting on a lock held by a member
of that same group.
This happens even in groups containing a single thread when that thread
attempts to acquire a non-recursive lock that it already holds.
Deadlock can therefore occur even given but one thread and one lock!

Without some sort of external intervention, deadlock is forever.
No thread can acquire the lock it is waiting on until that
lock is released by the thread holding it, but the thread holding
it cannot release it until the holding thread acquires the lock that
it is in turn waiting on.

\fi

\begin{figure}[tb]
\centering
\resizebox{1.5in}{!}{\includegraphics{locking/DeadlockCycle}}
\caption{Deadlock Cycle}
\label{fig:locking:Deadlock Cycle}
\end{figure}

Figure~\ref{fig:locking:Deadlock Cycle} 에 보인 것과 같이 쓰레드와 락을 노드로
표현하고 방향성 있는 그래프로 deadlock 시나리오를 표현할 수 있습니다.
락으로부터 쓰레드로의 화살표는 이 쓰레드가 이 락을 쥐고 있음을 의미하는데, 예를
들어 쓰레드~B 는 락~2 와~4 를 쥐고 있습니다.
쓰레드로부터 락으로의 화살표는 이 쓰레드가 이 락을 기다리고 있음을 의미하는데,
예를 들어 쓰레드~B 는 락~3 을 기다리고 있습니다.

Deadlock 시나리오는 항상 최소 하나의 deadlock 사이클을 가질 겁니다.
Figure~\ref{fig:locking:Deadlock Cycle} 에서, 이 사이클은 쓰레드~B, 락~3,
쓰레드~C, 락~4, 그리고 다시 쓰레드~B 로 구성됩니다.

\iffalse

We can create a directed-graph representation of a deadlock scenario
with nodes for threads and locks, as shown in
Figure~\ref{fig:locking:Deadlock Cycle}.
An arrow from a lock to a thread indicates that the thread holds
the lock, for example, Thread~B holds Locks~2 and~4.
An arrow from a thread to a lock indicates that the thread is waiting
on the lock, for example, Thread~B is waiting on Lock~3.

A deadlock scenario will always contain at least one deadlock cycle.
In Figure~\ref{fig:locking:Deadlock Cycle}, this cycle is
Thread~B, Lock~3, Thread~C, Lock~4, and back to Thread~B.

\fi

\QuickQuiz{
	하지만 락 기반의 deadlock 의 정의는 각 쓰레드가 최소 하나의 락을 쥔 채
	어떤 다른 쓰레드가 쥐고 있는 락을 기다린다였습니다.
	사이클이 존재하는지 어떻게 알 수 있죠?

	\iffalse

	But the definition of lock-based deadlock only said that each
	thread was holding at least one lock and waiting on another lock
	that was held by some thread.
	How do you know that there is a cycle?

	\fi

}\QuickQuizAnswer{
	이 그래프에 사이클이 존재하지 않는다고 생각해 봅시다.
	그러면 우린 방향성을 directed acyclic graph (DAG) 를 갖게 될텐데,
	여기엔 최소 하나의 leaf 노드가 존재할 겁니다.

	만약 이 leaf 노드가 락이라면, 우린 어떤 쓰레드에 의해서도 쥐여지지 않은
	어떤 락을 기다리는 쓰레드가 존재한다는 것인데, 이는 앞의 정의에 반하는
	일입니다.
	이 경우 이 쓰레드는 곧바로 이 락을 획득할 겁니다.

	반면, 이 leaf 노드가 쓰레드였다면, 우린 어떤 락도 기다리고 있지 않은
	쓰레드를 가지고 있다는 의미이며, 이는 또다시 정의에 반하는 일입니다.
	그리고 이 경우, 이 쓰레드는 수행 중이거나 락이 아닌 다른 무언가에
	블록되어 있을 겁니다.
	첫번째 경우라면, 무한 루프 버그가 존재하지 않는다면, 이 쓰레드는
	결과적으로 락을 해제할 겁니다.
	두번째 경우, 깨어나기 실패하는 버그가 존재하지 않는다면, 이 쓰레드는
	결국은 깨어나서 락을 해제할 겁니다.\footnote{
		물론, 깨어나기 실패하는 버그의 한 종류는 락만이 아니라 다른
		락이 아닌 자원이 연관되는 deadlock 입니다.
		하지만 여기서 질문은 ``락 기반의 deadlock'' 이었습니다!}

	따라서, 이 락 기반 deadlock 의 정의에 기반하여, 연관된 그래프에는
	사이클이 존재해야만 합니다.

	\iffalse

	Suppose that there is no cycle in the graph.
	We would then have a directed acyclic graph (DAG), which would
	have at least one leaf node.

	If this leaf node was a lock, then we would have a thread
	that was waiting on a lock that wasn't held by any thread,
	counter to the definition.
	In this case the thread would immediately acquire the lock.

	On the other hand, if this leaf node was a thread, then
	we would have a thread that was not waiting on any lock,
	again counter to the definition.
	And in this case, the thread would either be running or
	be blocked on something that is not a lock.
	In the first case, in the absence of infinite-loop bugs,
	the thread will eventually release the lock.
	In the second case, in the absence of a failure-to-wake bug,
	the thread will eventually wake up and release the lock.\footnote{
		Of course, one type of failure-to-wake bug is a
		deadlock that involves not only locks, but also non-lock
		resources.
		But the question really did say ``lock-based deadlock''!}

	Therefore, given this definition of lock-based deadlock, there
	must be a cycle in the corresponding graph.

	\fi

}\QuickQuizEnd

존재하는 deadlock 으로부터의 회복을 할 수 있는 데이터베이스 시스템과 같은
소프트웨어 환경이 존재하지만, 이 방법은 쓰레드 중 하나가 죽임당하거나 락이 어떤
쓰레드들로부터 강제로 빼앗겨져야 할 것을 필요로 합니다.
이런 죽임과 강제 빼앗기는 트랜잭션들에 대해서는 잘 동작합니다만, 커널과
어플리케이션 수준의 락킹 사용에서는 종종 문제가 됩니다: 결과적으로 부분적으로만
업데이트 된 구조들을 다루는 것은 무척 복잡하고, 재앙에 가깝고, 에러를 만들기
내기 쉽습니다.

따라서, 커널과 어플리케이션은 deadlock 의 존재를 막아야 합니다.
Deadlock 방지 전략에는 락킹 계층
(Section~\ref{sec:locking:Locking Hierarchies}),
지역적 락킹 계층
(Section~\ref{sec:locking:Local Locking Hierarchies}),
레이어 기반 락킹 계층
(Section~\ref{sec:locking:Layered Locking Hierarchies}),
락들로의 포인터들을 포함하는 API 들을 다루는 전략
(Section~\ref{sec:locking:Locking Hierarchies and Pointers to Locks}),
조건적 락킹
(Section~\ref{sec:locking:Conditional Locking}),
필요한 락들을 모두 획득하고 시작하기
(Section~\ref{sec:locking:Acquire Needed Locks First}),
한번에 하나의 락만 잡기 설계
(Section~\ref{sec:locking:Single-Lock-at-a-Time Designs}),
그리고 시그널/인터럽트 핸들러를 위한 전략
(Section~\ref{sec:locking:Signal/Interrupt Handlers}) 들이 있습니다.
모든 경우에 완벽하게 동작하는 deadlock 방지 전략은 존재하지 않지만, 사용 가능한
것들 중에서 적절한 도구를 선택하는 것은 가능합니다.

\iffalse

Although there are some software environments such as database systems
that can recover from an existing deadlock, this approach requires either
that one of the threads be killed or that a lock be forcibly stolen from
one of the threads.
This killing and forcible stealing works well for transactions,
but is often problematic for kernel and application-level use of locking:
dealing with the resulting partially updated structures can be extremely
complex, hazardous, and error-prone.

Therefore, kernels and applications should instead avoid deadlocks.
Deadlock-avoidance strategies include locking hierarchies
(Section~\ref{sec:locking:Locking Hierarchies}),
local locking hierarchies
(Section~\ref{sec:locking:Local Locking Hierarchies}),
layered locking hierarchies
(Section~\ref{sec:locking:Layered Locking Hierarchies}),
strategies for dealing with APIs containing pointers to locks
(Section~\ref{sec:locking:Locking Hierarchies and Pointers to Locks}),
conditional locking
(Section~\ref{sec:locking:Conditional Locking}),
acquiring all needed locks first
(Section~\ref{sec:locking:Acquire Needed Locks First}),
single-lock-at-a-time designs
(Section~\ref{sec:locking:Single-Lock-at-a-Time Designs}),
and strategies for signal/interrupt handlers
(Section~\ref{sec:locking:Signal/Interrupt Handlers}).
Although there is no deadlock-avoidance strategy that works perfectly
for all situations, there is a good selection of tools to choose from.

\fi

\subsubsection{Locking Hierarchies}
\label{sec:locking:Locking Hierarchies}

락킹 계층은 락들을 순서지어서 순서에 맞지 않게 락을 획득하는 것을 막습니다.
Figure~\ref{fig:locking:Deadlock Cycle} 에서, 우린 락들을 숫자 기반으로 순서를
지어서, 어떤 쓰레드가 같거나 큰 수의 락을 쥐고 있다면 락을 획득하지 못하게 할
수 있습니다.
쓰레드~B 는 락~4 를 쥔 상태에서 락~3 을 획득하려 하고 있으므로 이 계층 규칙을
위반하고 있습니다.
이 위반이 이 deadlock 이 일어날 수 있게 했습니다.

다시 말하지만, 락킹 계층을 적용하기 위해선, 락들에 순서를 짓고 순서외의 락
획득을 막아야 합니다.
거대한 프로그램에서는 락킹 계층을 강제하기 위해 리눅스 커널의
\co{lockdep}~\cite{JonathanCorbet2006lockdep} 과 같은 도구들을 사용하는 게
현명합니다.

\iffalse

Locking hierarchies order the locks and prohibit acquiring locks out
of order.
In Figure~\ref{fig:locking:Deadlock Cycle},
we might order the locks numerically, thus forbidding a thread
from acquiring a given lock if it already holds a lock
with the same or a higher number.
Thread~B has violated this hierarchy because it is attempting to
acquire Lock~3 while holding Lock~4.
This violation permitted the deadlock to occur.

Again, to apply a locking hierarchy, order the locks and prohibit
out-of-order lock acquisition.
In large program, it is wise to use tools such as the Linux-kernel
\co{lockdep}~\cite{JonathanCorbet2006lockdep}
to enforce your locking hierarchy.

\fi

\subsubsection{Local Locking Hierarchies}
\label{sec:locking:Local Locking Hierarchies}

하지만, 락킹 계층의 전역적 특성은 그것을 라이브러리 함수에 적용하기 어렵게
합니다.
무엇보다도, 특정 라이브러리 함수를 사용하는 어떤 프로그램이 아직 작성되지
않았을 때, 이 불쌍한 라이브러리 함수 구현자는 아직 정의되지 않은 락킹 계층을
따를 수 있겠습니까?

한가지 특수한 (하지만 흔한) 경우는 이 라이브러리 함수가 호출자의 코드를
호출하지 않을 때입니다.
이 경우, 이 호출자의 락은 이 라이브러리의 락을 잡고 있는 사이에 얻어지려 하지
않을 것이며, 따라서 라이브러리와 호출자 모두의 락이 포함된 데드락 사이클은
존재하지 않을 것입니다.

\iffalse

However, the global nature of locking hierarchies makes them difficult to
apply to library functions.
After all, when a program using a given library function has not yet
been written, how can the poor library-function implementor possibly
follow the yet-to-be-defined locking hierarchy?

One special (but common) case is when the library function does not
invoke any of the caller's code.
In this case, the caller's locks will never be acquired while holding
any of the library's locks, so that there cannot be a deadlock cycle
containing locks from both the library and the caller.

\fi

\QuickQuiz{
	이 규칙에 대한 어떤 예외가 있어서, 라이브러리 코드가 호출자의 함수를
	전혀 호출하지 않음에도 라이브러리와 호출자 양쪽의 락이 포함된 데드락
	사이클이 존재할 수도 있나요?

	\iffalse

	Are there any exceptions to this rule, so that there really could be
	a deadlock cycle containing locks from both the library and
	the caller, even given that the library code never invokes
	any of the caller's functions?

	\fi

}\QuickQuizAnswer{
	실제로 그런 예외가 있습니다!
	여기 그 중 일부가 있습니다:
	\begin{enumerate}
	\item	이 라이브러리 함수의 인자 중 하나가 이 라이브러리 함수가
		획득해야 하는 어떤 락으로의 포인터이고 이 라이브러리 함수가 이
		호출자의 락을 잡는 동안 자신의 락을 잡고 있다면, 호출자와
		라이브러리의 락 모두가 포함된 데드락 사이클을 가질 수 있습니다.
	\item	이 라이브러리 함수 중 하나가 호출자에 의해 획득된 락으로의
		포인터를 리턴한다면, 그리고 이 호출자가 이 라이브러리의 락이
		잡혀 있는 사이 자신의 락을 잡았다면, 우린 호출자와 라이브러리의
		락들이 포함된 데드락 사이클을 가질 수 있습니다.
	\item	이 라이브러리 함수들 중 하나가 락을 획득하고 그걸 잡고 있는
		채로 리턴한다면, 그리고 이 호출자가 자신의 락 중 하나를
		잡는다면, 우린 호출자와 라이브러리의 락들이 모두 포함된 데드락
		사이클을 만들 수 있는 또다른 방법을 가지고 있게 됩니다.
	\item	이 호출자가 락을 잡는 시그널 핸들러를 가지고 있다면, 데드락
		사이클은 호출자와 라이브러리 락 모두를 가질 수 있습니다.
		하지만, 이 경우 라이브러리의 락은 이 데드락 사이클에서 결백한
		방관자입니다.
		그러나, 시그널 핸들러에서 락을 획득하는 것은 대부분의 경우 하면
		안될 일임을 알아두시기 바랍니다---그건 그냥 나쁜 생각이 아니라,
		하면 안되는 행동입니다.
		하지만 여러분이 시그널 핸들러 내에서 명확히 락을 잡아야만
		한다면, 해당 쓰레드 컨텍스트 내에서 같은 락을 잡는 것을 막기
		위해 그 시그널을 막아둘 것을 분명히 하십시오.
	\end{enumerate}

	\iffalse

	Indeed there are!
	Here are a few of them:
	\begin{enumerate}
	\item	If one of the library function's arguments is a pointer
		to a lock that this library function acquires, and if
		the library function holds one of its locks while
		acquiring the caller's lock, then we could have a
		deadlock cycle involving both caller and library locks.
	\item	If one of the library functions returns a pointer to
		a lock that is acquired by the caller, and if the
		caller acquires one of its locks while holding the
		library's lock, we could again have a deadlock
		cycle involving both caller and library locks.
	\item	If one of the library functions acquires a lock and
		then returns while still holding it, and if the caller
		acquires one of its locks, we have yet another way
		to create a deadlock cycle involving both caller
		and library locks.
	\item	If the caller has a signal handler that acquires
		locks, then the deadlock cycle can involve both
		caller and library locks.
		In this case, however, the library's locks are
		innocent bystanders in the deadlock cycle.
		That said, please note that acquiring a lock from
		within a signal handler is a no-no in most
		environments---it is not just a bad idea, it
		is unsupported.
		But if you absolutely must acquire a lock in a signal
		handler, be sure to block that signal while holding that
		same lock in thread context.
	\end{enumerate}

	\fi

}\QuickQuizEnd

하지만 이 라이브러리 함수는 호출자의 코드를 수행한다고 가정해 봅시다.
예를 들어, \co{qsort()} 는 호출자가 제공한 비교 함수를 호출합니다.
이 비교 함수는 일반적으로는 변하지 않는 로컬 데이터를 가지고 동작해서,
\cref{fig:locking:No qsort() Compare-Function Locking} 에 보인 것처럼 락을 잡을
필요가 없을 겁니다.
하지만 어떤 사람은 키가 변화하는 데이터 모음을 정렬할 만큼 미쳐 있어서 이 비교
함수가 락을 잡을 것을 필요로 할 수도 있는데, 이것은
\cref{fig:locking:Without qsort() Local Locking Hierarchy} 에 보인 것처럼
데드락을 유발할 수도 있습니다.
이 라이브러리 함수는 어떻게 데드락을 막을 수 있을까요?

이 경우의 황금 규칙은 ``알려지지 않은 코드를 수행하기 전에 모든 락을 해제하라''
입니다.
이 규칙을 따르기 위해, \co{qsort()} 함수는 비교 함수를 호출하기 전에 자신의
모든 락을 내려 놓아야 합니다.
따라서 \co{qsort()} 는 비교 함수가 호출자의 락을 잡는 동안 자신의 어떤 락도
쥐고 있지 않으며, 따라서 데드락이 방지됩니다.

\iffalse

But suppose that a library function does invoke the caller's code.
For example, \co{qsort()} invokes a caller-provided comparison function.
Now, normally this comparison function will operate on unchanging local
data, so that it need not acquire locks, as shown in
\cref{fig:locking:No qsort() Compare-Function Locking}.
But maybe someone is crazy enough to sort a collection whose keys
are changing, thus requiring that the comparison function acquire locks,
which might result in deadlock, as shown in
\cref{fig:locking:Without qsort() Local Locking Hierarchy}.
How can the library function avoid this deadlock?

The golden rule in this case is ``Release all locks before invoking
unknown code.''
To follow this rule, the \co{qsort()} function must release all of its
locks before invoking the comparison function.
Thus \co{qsort()} will not be holding any of its locks while the comparison
function acquires any of the caller's locks, thus avoiding deadlock.

\fi

\QuickQuiz{
	하지만 \co{qsort()} 가 비교 함수를 호출하기 전에 자신의 락을 모두
	내려놓는다면, 다른 \co{qsort()} 쓰레드와의 레이스로부터 어떻게 자신을
	보호하나요?

	\iffalse

	But if \co{qsort()} releases all its locks before invoking
	the comparison function, how can it protect against races
	with other \co{qsort()} threads?

	\fi

}\QuickQuizAnswer{
	비교되는 데이터 원소들의 소유권을 자신의 것으로 하거나
	(Chapter~\ref{chp:Data Ownership} 에서 이야기 되었습니다) 레퍼런스
	카운팅과 같은
	미루기 메커니즘을 사용해서
	(Chapter~\ref{chp:Deferred Processing} 에서 이야기 되었습니다) 그럴 수
	있습니다.
	또는 \cref{sec:locking:Layered Locking Hierarchies} 에서 이야기 되었듯
	레이어 기반 락킹 계층을 사용할 수 있습니다. 

	다른 한편, 정렬되고 있는 키를 변경하는 것은 아무리 좋게 이야기 해도
	용감하다고 밖에 할 도리가 없습니다.

	\iffalse

	By privatizing the data elements being compared
	(as discussed in Chapter~\ref{chp:Data Ownership})
	or through use of deferral mechanisms such as
	reference counting (as discussed in
	Chapter~\ref{chp:Deferred Processing}).
	Or through use of layered locking hierarchies, as described
	in \cref{sec:locking:Layered Locking Hierarchies}.

	On the other hand, changing a key in a list that is
	currently being sorted is at best rather brave.

	\fi

}\QuickQuizEnd

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{locking/NoLockHierarchy}}
\caption{No \tco{qsort()} Compare-Function Locking}
\label{fig:locking:No qsort() Compare-Function Locking}
\end{figure}

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{locking/NonLocalLockHierarchy}}
\caption{Without \tco{qsort()} Local Locking Hierarchy}
\label{fig:locking:Without qsort() Local Locking Hierarchy}
\end{figure}

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{locking/LocalLockHierarchy}}
\caption{Local Locking Hierarchy for \tco{qsort()}}
\label{fig:locking:Local Locking Hierarchy for qsort()}
\end{figure}

로컬 락킹 계층화의 장점을 보기 위해,
Figure~\ref{fig:locking:Without qsort() Local Locking Hierarchy} 와
\ref{fig:locking:Local Locking Hierarchy for qsort()}
를 비교해 보시기 바랍니다.
두 그림에서, 어플리케이션 함수 \co{foo()} 와 \co{bar()} 는 각각 락~A 와~B 를
잡고서 \co{qsort()} 를 호출합니다.
이것은 \co{qsort()} 의 병렬 구현이므로, 락~C 를 잡습니다.
함수 \co{foo()} 는 함수 \co{cmp()} 를 \co{qsort()} 에 넘기고, \co{cmp(()} 는
락~B 를 잡습니다.
함수 \co{bar()} 는 간단한 정수 비교 함수 (여기엔 보이지 않았습니다) 를
\co{qsort()} 에 넘기고, 이 간단한 함수는 어떤 락도 잡지 않습니다.

이제, \co{qsort()} 가
Figure~\ref{fig:locking:Without qsort() Local Locking Hierarchy} 에 보인 것처럼
\co{cmp()} 를 호출하는 동안 락~C 를 잡고 있어서 앞의 황금 규칙을 어긴다면,
데드락이 발생할 수 있습니다.
이를 자세히 보기 위해, 한 쓰레드가 두번째 쓰레드가 동시에 \co{bar()} 를
수행하는 사이에 \co{foo()} 를 호출한다고 생각해 봅시다.
첫번째 쓰레드의 \co{qsort()} 호출은 락~C 를 잡을 것이고, 이어서 \co{cmp()} 를
호출할 때 락~B 를 잡을 수 없을 겁니다.
하지만 첫번째 쓰레드는 락~C 를 잡고 있고, 따라서 두번째 쓰레드의 \co{qsort()}
는 그 락을 잡을 수 없고, 따라서 락~B 를 해제할 수 없어서 데드락에 이르게
됩니다.

\iffalse

To see the benefits of local locking hierarchies, compare
Figures~\ref{fig:locking:Without qsort() Local Locking Hierarchy} and
\ref{fig:locking:Local Locking Hierarchy for qsort()}.
In both figures, application functions \co{foo()} and \co{bar()}
invoke \co{qsort()} while holding Locks~A and~B, respectively.
Because this is a parallel implementation of \co{qsort()}, it acquires
Lock~C\@.
Function \co{foo()} passes function \co{cmp()} to \co{qsort()},
and \co{cmp()} acquires Lock~B\@.
Function \co{bar()} passes a simple integer-comparison function (not
shown) to \co{qsort()}, and this simple function does not acquire any
locks.

Now, if \co{qsort()} holds Lock~C while calling \co{cmp()} in violation
of the golden release-all-locks rule above, as shown in
Figure~\ref{fig:locking:Without qsort() Local Locking Hierarchy},
deadlock can occur.
To see this, suppose that one thread invokes \co{foo()} while a second
thread concurrently invokes \co{bar()}.
The first thread will acquire Lock~A and the second thread will acquire
Lock~B\@.
If the first thread's call to \co{qsort()} acquires Lock~C, then it
will be unable to acquire Lock~B when it calls \co{cmp()}.
But the first thread holds Lock~C, so the second thread's call to
\co{qsort()} will be unable to acquire it, and thus unable to release
Lock~B, resulting in deadlock.

\fi

대조적으로, \co{qsort()} 각 락~C 를 \co{qsort()} 의 관점에서는 알려지지 않은
코드인 이 비교 함수를 호출하기 전에 내려놓는다면, 
Figure~\ref{fig:locking:Local Locking Hierarchy for qsort()} 에 보인 것과 같이
데드락이 방지됩니다.

각 모듈이 알려지지 않은 코드를 호출하기 전에 모든 락을 내려놓는다면, 각 모듈이
개별적으로 데드락을 방지하고 있는 한 전체 데드락이 방지됩니다.
따라서 이 규칙은 데드락 분석을 상당히 단순화 하고 모듈성을 크게 개선합니다.

\iffalse

In contrast, if \co{qsort()} releases Lock~C before invoking the
comparison function, which is unknown code from \co{qsort()}'s perspective,
then deadlock is avoided as shown in
Figure~\ref{fig:locking:Local Locking Hierarchy for qsort()}.

If each module releases all locks before invoking unknown code, then
deadlock is avoided if each module separately avoids deadlock.
This rule therefore greatly simplifies deadlock analysis and greatly
improves modularity.

\fi

\subsubsection{Layered Locking Hierarchies}
\label{sec:locking:Layered Locking Hierarchies}

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{locking/LayeredLockHierarchy}}
\caption{Layered Locking Hierarchy for \tco{qsort()}}
\label{fig:locking:Layered Locking Hierarchy for qsort()}
\end{figure}

불행히도, \co{qsort()} 가 비교 함수를 호출하기 전에 자신의 락들을 모두 내려놓는
것은 불가능할 수도 있습니다.
이런 경우, 우린 알려지지 않은 코드를 호출하기 전에 모든 락을 내려놓아 로컬 락킹
계층을 구성하지 못할 겁니다.
하지만, 우리는 그 대신에
Figure~\ref{fig:locking:Layered Locking Hierarchy for qsort()} 에 보인 것처럼
레이어 기반 락킹 계층을 만들 수 있습니다.
여기서, \co{cmp()} 함수는 락~A, B, 그리고~C 를 모두 획득한 후에 새로운 락~D 를
사용해서 데드락을 막습니다.
따라서 우리는 글로벌 데드락 계층에 세개의 레이어를 갖는 셈인데, 첫번째 레이어는
락~A 와~B 를, 두번째 레이어는 락~C 를, 그리고 세번째 레이어는 락~D 를 갖습니다.

\iffalse

Unfortunately, it might not be possible for \co{qsort()} to release
all of its locks before invoking the comparison function.
In this case, we cannot construct a local locking hierarchy by
releasing all locks before invoking unknown code.
However, we can instead construct a layered locking hierarchy, as shown in
Figure~\ref{fig:locking:Layered Locking Hierarchy for qsort()}.
here, the \co{cmp()} function uses a new Lock~D that is acquired after
all of Locks~A, B, and~C, avoiding deadlock.
We therefore have three layers to the global deadlock hierarchy, the
first containing Locks~A and~B, the second containing Lock~C, and
the third containing Lock~D\@.

\fi

\begin{listing}[tbp]
\input{CodeSamples/locking/locked_list@start_next.fcv}
\caption{Concurrent List Iterator}
\label{lst:locking:Concurrent List Iterator}
\end{listing}

기계적으로 \co{cmp()} 를 새 락~D 를 사용하도록 바꾸는 것은 일반적으로
불가능함을 알아 두시기 바랍니다.
상당히 그 반대입니다: 설계 수준에서의 근본적 변경이 종종 필요합니다.
그러나, 그런 변경에 필요한 노력은 일반적으로 데드락을 방지하기 위한 비용으로는
작은 것입니다.
이 부분에 대해 더 말해보자면, 이 잠재적 데드락은 어떤 코드가 만들어지기 전,
설계 시간에 발견되는 것이 선호되어야 합니다!

알려지지 않은 코드를 호출하기 전에 모든 락을 내려놓는 것이 불가능한 또다른 예를
위해서는
Listing~\ref{lst:locking:Concurrent List Iterator} (\path{locked_list.c}) 에
보인 것과 같은 링크드 리스트를 순회하는 코드를 생각해 보시기 바랍니다.
\co{list_start()} 함수는 이 리스트에서 락을 획득하고 첫번째 원소를 (존재한다면)
리턴하고, \co{list_next()} 는 이 리스트의 다음 원소로의 포인터를 리턴하거나 이
리스트의 끝에 도달했다면 이 락을 해제하고 \co{NULL} 을 리턴합니다.

\iffalse

Please note that it is not typically possible to mechanically
change \co{cmp()} to use the new Lock~D\@.
Quite the opposite: It is often necessary to make profound design-level
modifications.
Nevertheless, the effort required for such modifications is normally
a small price to pay in order to avoid deadlock.
More to the point, this potential deadlock should preferably be detected
at design time, before any code has been generated!

For another example where releasing all locks before invoking unknown
code is impractical, imagine an iterator over a linked list, as shown in
Listing~\ref{lst:locking:Concurrent List Iterator} (\path{locked_list.c}).
The \co{list_start()} function acquires a lock on the list and returns
the first element (if there is one), and
\co{list_next()} either returns a pointer to the next element in the list
or releases the lock and returns \co{NULL} if the end of the list has
been reached.

\fi

\begin{listing}[tbp]
\input{CodeSamples/locking/locked_list@list_print.fcv}
\caption{Concurrent List Iterator Usage}
\label{lst:locking:Concurrent List Iterator Usage}
\end{listing}

\begin{fcvref}[ln:locking:locked_list:list_print:ints]
Listing~\ref{lst:locking:Concurrent List Iterator Usage} 이 이 리스트 순회자가
어떻게 사용될 수 있는지 보입니다.
\Clnrefrange{b}{e} 는 하나의 정수를 담는 \co{list_ints} 원소를 정의하며,
\end{fcvref}
\begin{fcvref}[ln:locking:locked_list:list_print:print]
\clnrefrange{b}{e} 는 이 리스트를 어떻게 순회하는지 보입니다.
라인~\lnref{start} 는 리스트를 잠그고 첫번째 원소로의 포인터를 가져오고,
라인~\lnref{entry} 는 우리의 \co{list_ints} 구조체로의 포인터를 제공하며,
라인~\lnref{print} 는 연관된 정수를 출력하고,
라인~\lnref{next} 는 다음 원소로 넘어갑니다.
이는 상당히 간단하며, 모든 락킹을 숨기고 있습니다.
\end{fcvref}

\iffalse

\begin{fcvref}[ln:locking:locked_list:list_print:ints]
Listing~\ref{lst:locking:Concurrent List Iterator Usage} shows how
this list iterator may be used.
\Clnrefrange{b}{e} define the \co{list_ints} element
containing a single integer,
\end{fcvref}
\begin{fcvref}[ln:locking:locked_list:list_print:print]
and \clnrefrange{b}{e} show how to iterate over the list.
Line~\lnref{start} locks the list and fetches a pointer to the first element,
line~\lnref{entry} provides a pointer to our enclosing \co{list_ints} structure,
line~\lnref{print} prints the corresponding integer, and
line~\lnref{next} moves to the next element.
This is quite simple, and hides all of the locking.
\end{fcvref}

\fi

즉, 각 리스트 원소를 처리하는 코드가 데드락을 초래하게끔 그 스스로
\co{list_start()} 나 \co{list_next()} 호출을 건너 잡히는 락을 획득하지 않는다면
락킹은 숨겨져 있습니다.
우린 이 락킹 계층이 리스트 순회자 락킹을 처리하게끔 레이어를 추가함으로써
데드락을 막을 수 있습니다.

이 레이어 기반 접근법은 임의의 많은 수의 레이어들로 확장될 수 있습니다만,
추가되는 각 레이어는 락킹 설계의 복잡도를 증가시킵니다.
그런 복잡도 증가는 일반적으로 몇몇 종류의 객체 지향 설계에 불편을 가중시키는데,
제멋대로 많은 객체 그룹 사이를 오가는 컨트롤의 경우 그렇습니다.\footnote{
	이에 대한 이름들 중 하나는 ``객체 지향 스파게티 코드'' 입니다.}
이 객체 지향 설계 습관과 데드락 방지의 필요성 사이의 미스매치는 병렬
프로그래밍이 누군가에게는 어렵다고 인식되는 중요한 이유입니다.

고도의 레이어 기반 계층에 대한 일부 대안이
Chapter~\ref{chp:Deferred Processing} 에서 다뤄집니다.

\iffalse

That is, the locking remains hidden as long as the code processing each
list element does not itself acquire a lock that is held across some
other call to \co{list_start()} or \co{list_next()}, which results in
deadlock.
We can avoid the deadlock by layering the locking hierarchy
to take the list-iterator locking into account.

This layered approach can be extended to an arbitrarily large number of layers,
but each added layer increases the complexity of the locking design.
Such increases in complexity are particularly inconvenient for some
types of object-oriented designs, in which control passes back and forth
among a large group of objects in an undisciplined manner.\footnote{
	One name for this is ``object-oriented spaghetti code.''}
This mismatch between the habits of object-oriented design and the
need to avoid deadlock is an important reason why parallel programming
is perceived by some to be so difficult.

Some alternatives to highly layered locking hierarchies are covered in
Chapter~\ref{chp:Deferred Processing}.

\fi

\subsubsection{Locking Hierarchies and Pointers to Locks}
\label{sec:locking:Locking Hierarchies and Pointers to Locks}

일부 예외가 있기는 하지만, 락으로의 포인터를 포함하는 외부 API 는 종종 잘못
설계된 API 입니다.
내부의 락을 다른 소프트웨어 컴포넌트에 넘기는 것은 어쨌건 핵심 설계 교리인 정보
은닉의 안티테제입니다.

\iffalse

Although there are some exceptions, an external API containing a pointer
to a lock is very often a misdesigned API\@.
Handing an internal lock to some other software component is after all
the antithesis of information hiding, which is in turn a key design
principle.

\fi

\QuickQuiz{
	락으로의 포인터가 함수에 넘겨지는 흔한 경우 하나를 이야기해 보시죠.

	\iffalse

	Name one common situation where a pointer to a lock is passed
	into a function.

	\fi

}\QuickQuizAnswer{
	물론 락킹 기능들이죠!

	\iffalse

	Locking primitives, of course!

	\fi

}\QuickQuizEnd

한가지 예외는 어떤 것들을 넘겨주는 함수인데 이 호출자의 락이 이 넘겨주기가
완료될 때까지는 잡혀져 있어야 하지만 이 락이 이 함수의 리턴 전에 해제되어야
하는 경우입니다.
그런 함수의 예 중 하나로는 POSIX \co{pthread_cond_wait()} 함수가 있겠는데,
여기선 잃어버린 깨우기 때문에 멈춰있는 경우를 막기 위해 \co{pthread_mutex_t}
로의 포인터가 넘겨집니다.

\iffalse

One exception is functions that hand off some entity,
where the caller's lock must be held until the handoff is complete,
but where the lock must be released before the function returns.
One example of such a function is the POSIX \co{pthread_cond_wait()}
function, where passing an pointer to a \co{pthread_mutex_t}
prevents hangs due to lost wakeups.

\fi

\QuickQuiz{
	\co{pthread_cond_wait()} 이 먼저 이 뮤텍스를 해제하고 다시 그 뮤텍스를
	획득한다는 사실은 데드락의 가능성을 제거하지 않나요?

	\iffalse

	Doesn't the fact that \co{pthread_cond_wait()} first releases the
	mutex and then re-acquires it eliminate the possibility of deadlock?

	\fi

}\QuickQuizAnswer{
	결코 아닙니다!

	\co{mutex_a} 를 획득하고 이어서 \co{mutex_b} 를 획득하는, 그리고 나서는
	\co{mutex_a} 를 \co{pthread_cond_wait()} 에 넘기는 프로그램을 생각해
	봅시다.
	이제, \co{pthread_cond_wait()} 은 \co{mutex_a} 를 해제하지만 리턴하기
	전에 이를 다시 획득할 겁니다.
	만약 어떤 다른 쓰레드가 그 사이에 \co{mutex_a} 를 획득하고 \co{mutex_b}
	를 기다리며 블락된다면, 이 프로그램은 데드락에 걸립니다.

	\iffalse

	Absolutely not!

	Consider a program that acquires \co{mutex_a}, and then
	\co{mutex_b}, in that order, and then passes \co{mutex_a}
	to \co{pthread_cond_wait()}.
	Now, \co{pthread_cond_wait()} will release \co{mutex_a}, but
	will re-acquire it before returning.
	If some other thread acquires \co{mutex_a} in the meantime
	and then blocks on \co{mutex_b}, the program will deadlock.

	\fi

}\QuickQuizEnd

요약하자면, 여러분이 외부로의 API 에 락으로의 포인터를 인자로 또는 리턴 값으로
노출하고 있다면, 여러분의 API 설계를 다시 고민해 보세요.
그게 맞는 행동일 수도 있습니다만, 경험은 그렇지 않을 가능성이 높다고 이야기
합니다.

\iffalse

In short, if you find yourself exporting an API with a pointer to a
lock as an argument or the as the return value, do yourself a favor and
carefully reconsider your API design.
It might well be the right thing to do, but experience indicates that
this is unlikely.

\fi

\subsubsection{Conditional Locking}
\label{sec:locking:Conditional Locking}

하지만 합리적인 락킹 계층이 존재하지 않는다고 생각해 봅시다.
이는 실제 삶에서 일어날 수 있는데, 예를 들어, 패킷이 양방향으로 떠다니는 어떤
종류의 레이어화 된 네트워크 프로토콜 스택, 또는 예를 들어 분산된 락 관리자
구현이 있겠습니다.
네트워킹의 경우, 패킷을 한 레이어에서 다른 레이어로 넘길 때 양 레이어로부터
락을 잡아야 할수도 있습니다.
패킷은 프로토콜 스택의 위로도 아래로도 갈 수 있다는 점을 생각하면, 이는
Listing~\ref{lst:locking:Protocol Layering and Deadlock}
에 그린 것처럼 데드락을 일으키기 위한 훌륭한 방법입니다.
\begin{fcvref}[ln:locking:Protocol Layering and Deadlock]
여기서, 통신선을 향해 스택의 아래 방향으로 움직이는 패킷은 다름 레이어의 락을
순서 반대로 잡아야만 합니다.
통신선으로부터 멀어지는, 스택 위쪽으로 움직이는 패킷은 락을 순서대로 잡는다는
점을 놓고 보면, 라인~\lnref{acq} 에서의 락 획득은 데드락을 일으킬 수 있습니다.
\end{fcvref}

\iffalse

But suppose that there is no reasonable locking hierarchy.
This can happen in real life, for example, in some types of layered
network protocol stacks where packets flow in both directions, for
example, in implementations of distributed lock managers.
In the networking case, it might be necessary to hold the locks from
both layers when passing a packet from one layer to another.
Given that packets travel both up and down the protocol stack, this
is an excellent recipe for deadlock, as illustrated in
Listing~\ref{lst:locking:Protocol Layering and Deadlock}.
\begin{fcvref}[ln:locking:Protocol Layering and Deadlock]
Here, a packet moving down the stack towards the wire must acquire
the next layer's lock out of order.
Given that packets moving up the stack away from the wire are acquiring
the locks in order, the lock acquisition in line~\lnref{acq} of the listing
can result in deadlock.
\end{fcvref}

\fi

\begin{listing}[tbp]
\begin{fcvlabel}[ln:locking:Protocol Layering and Deadlock]
\begin{VerbatimL}[commandchars=\\\{\}]
spin_lock(&lock2);
layer_2_processing(pkt);
nextlayer = layer_1(pkt);
spin_lock(&nextlayer->lock1);	\lnlbl{acq}
layer_1_processing(pkt);
spin_unlock(&lock2);
spin_unlock(&nextlayer->lock1);
\end{VerbatimL}
\end{fcvlabel}
\caption{Protocol Layering and Deadlock}
\label{lst:locking:Protocol Layering and Deadlock}
\end{listing}

이 경우에 데드락을 방지하는 한가지 방법은 락킹 계층을 암시하지만 락을 반대
순서로 잡아야 할 경우에는
Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
에 보인 것처럼 그것을 조건적으로 잡는 것입니다.
\begin{fcvref}[ln:locking:Avoiding Deadlock Via Conditional Locking]
무조건적으로 layer-1 락을 잡는 대신, 라인~\lnref{trylock} 은
\co{spin_trylock()} 기능을 사용해 조건적으로 락을 잡습니다.
이 기능은 이 락이 잡을 수 있다면 바로 잡지만 (0이 아닌 값을 리턴합니다), 그렇지
않다면 락을 잡지 않은 채 0 을 리턴합니다.

\iffalse

One way to avoid deadlocks in this case is to impose a locking hierarchy,
but when it is necessary to acquire a lock out of order, acquire it
conditionally, as shown in
Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}.
\begin{fcvref}[ln:locking:Avoiding Deadlock Via Conditional Locking]
Instead of unconditionally acquiring the layer-1 lock, line~\lnref{trylock}
conditionally acquires the lock using the \co{spin_trylock()} primitive.
This primitive acquires the lock immediately if the lock is available
(returning non-zero), and otherwise returns zero without acquiring the lock.

\fi

\begin{listing}[tbp]
\begin{fcvlabel}[ln:locking:Avoiding Deadlock Via Conditional Locking]
\begin{VerbatimL}[commandchars=\\\[\]]
retry:
	spin_lock(&lock2);
	layer_2_processing(pkt);
	nextlayer = layer_1(pkt);
	if (!spin_trylock(&nextlayer->lock1)) {	\lnlbl[trylock]
		spin_unlock(&lock2);		\lnlbl[rel2]
		spin_lock(&nextlayer->lock1);	\lnlbl[acq1]
		spin_lock(&lock2);		\lnlbl[acq2]
		if (layer_1(pkt) != nextlayer) {\lnlbl[recheck]
			spin_unlock(&nextlayer->lock1);
			spin_unlock(&lock2);
			goto retry;
		}
	}
	layer_1_processing(pkt);		\lnlbl[l1_proc]
	spin_unlock(&lock2);
	spin_unlock(&nextlayer->lock1);
\end{VerbatimL}
\end{fcvlabel}
\caption{Avoiding Deadlock Via Conditional Locking}
\label{lst:locking:Avoiding Deadlock Via Conditional Locking}
\end{listing}

이 \co{spin_trylock()} 이 성공했다면, 라인~\lnref{l1_proc} 은 필요한 layer-1
처리를 진행합니다.
그렇지 않다면, 라인~\lnref{rel2} 는 락을 내려놓고, 라인~\lnref{acq1}
과~\lnref{acq2} 는 그것들을 옳은 순서로 획득합니다.
불행히도, 시스템에는 여러 네트워킹 디바이스가 있을 수 있으며 (예를 들면,
이더넷과 와이파이), 따라서 \co{layer_1()} 함수는 라우팅 결정을 내려야만 합니다.
이 결정은 언제든 바뀔 수 있는데, 특히나 이 시스템이 모바일이라면
그렇습니다.\footnote{
	그리고, 1900년대와 달리, 모바일 환경은 흔합니다.}
따라서, 라인~\lnref{recheck} 은 이 결정을 다시 검사하고, 그게 변경되었다면, 이
락을 내려놓고 재시작 해야 합니다.
\end{fcvref}

\iffalse

If \co{spin_trylock()} was successful, line~\lnref{l1_proc} does the needed
layer-1 processing.
Otherwise, line~\lnref{rel2} releases the lock, and
lines~\lnref{acq1} and~\lnref{acq2} acquire them in
the correct order.
Unfortunately, there might be multiple networking devices on
the system (e.g., Ethernet and WiFi), so that the \co{layer_1()}
function must make a routing decision.
This decision might change at any time, especially if the system
is mobile.\footnote{
	And, in contrast to the 1900s, mobility is the common case.}
Therefore, line~\lnref{recheck} must recheck the decision, and if it has changed,
must release the locks and start over.
\end{fcvref}

\fi

\QuickQuizSeries{%
\QuickQuizB{
	Listing~\ref{lst:locking:Protocol Layering and Deadlock} 에서
	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	로의 변환은 어디서든 적용될 수 있나요?

	\iffalse

	Can the transformation from
	Listing~\ref{lst:locking:Protocol Layering and Deadlock} to
	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	be applied universally?

	\fi

}\QuickQuizAnswerB{
	결코 아닙니다!

	이 변환은 \co{layer_2_processing()} 함수가 멱등하다고 가정하는데,
	\co{layer_1()} 라우팅 결정이 변화될 때 같은 함수에 대해서도 여러번
	실행될 수 있다는 사실 때문입니다.
	따라서, 실제 삶에서는, 이 변환이 무척 복잡할 수 있습니다.

	\iffalse

	Absolutely not!

	This transformation assumes that the
	\co{layer_2_processing()} function is idempotent, given that
	it might be executed multiple times on the same packet when
	the \co{layer_1()} routing decision changes.
	Therefore, in real life, this transformation can become
	arbitrarily complex.

	\fi

}\QuickQuizEndB
%
\QuickQuizE{
	하지만
	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	의 복잡도는 그게 데드락을 방지한다는 점을 생각하면 가치있죠, 그렇죠?

	\iffalse

	But the complexity in
	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	is well worthwhile given that it avoids deadlock, right?

	\fi

}\QuickQuizAnswerE{
	아마도요.

	\co{layer_1()} 에서의 라우팅 결정이 충분히 자주 바뀐다면, 이 코드는
	항상 재시도를 하고 결코 진행을 못낼 겁니다.
	이는 어떤 쓰레드도 진행을 못낸다면 ``livelock'' 이라고, 그렇지 않고
	일부 쓰레드는 진행을 내지만 나머지는 그러지 못한다면 ``starvation''
	이라고 불립니다
	(Section~\ref{sec:locking:Livelock and Starvation} 을 참고하세요).

	\iffalse

	Maybe.

	If the routing decision in \co{layer_1()} changes often enough,
	the code will always retry, never making forward progress.
	This is termed ``livelock'' if no thread makes any forward progress or
	``starvation''
	if some threads make forward progress but others do not
	(see Section~\ref{sec:locking:Livelock and Starvation}).

	\fi

}\QuickQuizEndE
}

\subsubsection{Acquire Needed Locks First}
\label{sec:locking:Acquire Needed Locks First}

조건적 락킹의 중요한 특수 경우에는 모든 필요한 락들이 어떤 처리가 이어지기 전에
획득됩니다.
이 경우, 처리는 멱등적일 필요가 없습니다: 이미 획득되어 있는 락을 먼저 내려놓지
않고는 해당 락을 획득할 수 없다면, 모든 락을 내려놓고 다시 시도합니다.
모든 락들이 획득된 다음에야 어떤 처리든 이어질 수 있습니다.

하지만, 이 방법은
Section~\ref{sec:locking:Livelock and Starvation} 에서도 이야기 될
\emph{livelock} 을 초래할 수 있습니다.

\iffalse

In an important special case of conditional locking, all needed
locks are acquired before any processing is carried out.
In this case, processing need not be idempotent: if it turns out
to be impossible to acquire a given lock without first releasing
one that was already acquired, just release all the locks and
try again.
Only once all needed locks are held will any processing be carried out.

However, this procedure can result in \emph{livelock}, which will
be discussed in
Section~\ref{sec:locking:Livelock and Starvation}.

\fi

\QuickQuiz{
	Section~\ref{sec:locking:Acquire Needed Locks First} 에서 설명된
	``필요한 락들을 먼저 획득하기'' 접근법을 사용할 때 livelock 은 어떻게
	회피할 수 있나요?

	\iffalse

	When using the ``acquire needed locks first'' approach described in
	Section~\ref{sec:locking:Acquire Needed Locks First},
	how can livelock be avoided?

	\fi

}\QuickQuizAnswer{
	추가적인 전역 락을 제공합니다.
	어떤 쓰레드가 반복적으로 필요한 락을 획득하는 걸 시도하고 실패한다면,
	해당 쓰레드가 조건 없이 새로운 전역 락을 획득하고, 이어서 조건적으로
	모든 필요한 락들을 획득합니다. (Doug Lea 에 의해 제안되었습니다.)

	\iffalse

	Provide an additional global lock.
	If a given thread has repeatedly tried and failed to acquire the needed
	locks, then have that thread unconditionally acquire the new
	global lock, and then unconditionally acquire any needed locks.
	(Suggested by Doug Lea.)

	\fi

}\QuickQuizEnd

연관된 접근법인 two-phase locking~\cite{PhilipABernstein1987} 은 트랜잭셔널
데이터베이스 시스템에서 오랫동안 제품군에서 사용되었습니다.
Two-phase locking 의 첫번째 단계에서는 락들이 획득되기만 하고 해제되지
않습니다.
일단 모든 필요한 락들이 획득되면, 트랜잭션은 두번째 단계로 들어가서, 락들은
해제되기만 하지 획득되지 않습니다.
이 락킹 접근법은 데이터베이스가 트랜잭션에 serializability 보장을 제공하게
하는데, 달리 말하자면 트랜잭션에 의해 보이고 만들어지는 모든 값들이 모든
트랜잭션 사이의 어떤 전역적 순서와 함께 일관적일 것을 보장하기 위함입니다.
그런 많은 시스템이 트랜잭션을 중단할 수 있는 기능에 의존하는데, 이게 모든
락들이 획득되기 전까지 공유된 데이터에 어떤 변경도 만드는 걸 회피함으로써
단순화 될 수 있긴 합니다.
Livelock 과 deadlock 은 그런 시스템에서 문제가 됩니다만, 실용적인 해결책은 여러
데이터베이스 교재에서 찾을 수 있을 겁니다.

\iffalse

A related approach, two-phase locking~\cite{PhilipABernstein1987},
has seen long production use in transactional database systems.
In the first phase of a two-phase locking transaction, locks are
acquired but not released.
Once all needed locks have been acquired, the transaction enters the
second phase, where locks are released, but not acquired.
This locking approach allows databases to provide serializability
guarantees for their transactions, in other words, to guarantee
that all values seen and produced by the transactions are consistent
with some global ordering of all the transactions.
Many such systems rely on the ability to abort transactions, although
this can be simplified by avoiding making any changes to shared data
until all needed locks are acquired.
Livelock and deadlock are issues in such systems, but practical
solutions may be found in any of a number of database textbooks.

\fi

\subsubsection{Single-Lock-at-a-Time Designs}
\label{sec:locking:Single-Lock-at-a-Time Designs}

어떤 경우에는 락을 중첩하는 것을 막을 수 있어서 deadlock 을 회피할 수도
있습니다.
예를 들어, 문제가 완전히 분할 가능하다면, 각 분할된 조각별로 하나의 락을 할당할
수 있습니다.
그러면 해당 조각에서 일하는 쓰레드는 연관된 락 하나만 잡으면 됩니다.
어떤 쓰레드도 한번에 두개 이상의 락을 잡지 않으므로, deadlock 은 불가능합니다.

하지만, 어떤 락도 잡히지 않은 상태에서도 필요한 데이터 구조가 존재함을 보장하기
위한 어떤 메커니즘이 있어야만 합니다.
그런 메커니즘 중 하나가
Section~\ref{sec:locking:Lock-Based Existence Guarantees}
에서 설명되며 다른 것들 몇가지가
Chapter~\ref{chp:Deferred Processing}
에서 설명됩니다.

\iffalse

In some cases, it is possible to avoid nesting locks, thus avoiding
deadlock.
For example, if a problem is perfectly partitionable, a single
lock may be assigned to each partition.
Then a thread working on a given partition need only acquire the one
corresponding lock.
Because no thread ever holds more than one lock at a time,
deadlock is impossible.

However, there must be some mechanism to ensure that the needed data
structures remain in existence during the time that neither lock is
held.
One such mechanism is discussed in
Section~\ref{sec:locking:Lock-Based Existence Guarantees}
and several others are presented in
Chapter~\ref{chp:Deferred Processing}.

\fi

\subsubsection{Signal/Interrupt Handlers}
\label{sec:locking:Signal/Interrupt Handlers}

시그널 핸들러가 연관되는 deadlock 은 시그널 핸들러 내에서
\co{pthread_mutex_lock()} 을 호출하는 것이 합법적이지 않음을 알리는 것으로 종종
금방 사라지기도 합니다.
하지만, 시그널 핸들러에서 호출될 수 \emph{있는} 손으로 만든 락킹 기능을
사용하는 것도 (많은 경우 현명치 못한 일이지만) 가능합니다.
거의 모든 운영체제 커널이 시그널 핸들러와 비슷한 인터럽트 핸들러 내에서 락을
잡을 수 있게 합니다.

여기서의 트릭은 시그널 (또는 인터럽트) 핸들러 내에서 잡힐 수 있는 어떤 락을
잡을 때마다 시그널을 블록 (또는 경우에 따라 인터럽트를 불능화) 시키는 것입니다.
더 나아가서, 그런 락을 잡을 때에는 시그널을 블록 시키지 않고서 시그널 핸들러의
바깥에서 잡힌 어떤 락도 잡으려 시도해선 안됩니다.

\iffalse

Deadlocks involving signal handlers are often quickly dismissed by
noting that it is not legal to invoke \co{pthread_mutex_lock()} from
within a signal handler~\cite{OpenGroup1997pthreads}.
However, it is possible (though often unwise) to hand-craft
locking primitives that \emph{can} be invoked from signal handlers.
Besides which, almost all operating-system kernels permit locks to
be acquired from within interrupt handlers, which are analogous
to signal handlers.

The trick is to block signals (or disable interrupts, as the case may be)
when acquiring any lock that might be acquired within a signal
(or an interrupt) handler.
Furthermore, if holding such a lock, it is illegal to attempt to
acquire any lock that is ever acquired
outside of a signal handler without blocking signals.

\fi

\QuickQuiz{
	락~A 가 어떤 시그널 핸들러의 내부에선 결코 잡히지 않았지만, 락~B 는
	쓰레드 컨텍스트에서도 시그널 핸들러에서도 잡혔다고 해봅시다.
	나아가서 락~A 는 가끔 시그널이 블록되지 않은 상태에서도 잡힌다고
	해봅시다.
	락~B 를 잡고 있는 사이에 락~A 를 획득하는 건 왜 불법인가요?

	\iffalse

	Suppose Lock~A is never acquired within a signal handler,
	but Lock~B is acquired both from thread context and by signal
	handlers.
	Suppose further that Lock~A is sometimes acquired with signals
	unblocked.
	Why is it illegal to acquire Lock~A holding Lock~B?

	\fi

}\QuickQuizAnswer{
	이는 데드락을 초래할 테니까요.
	락~A 는 가끔 시그널 핸들러 바깥에서 시그널을 블록하지 않은 상태에서
	잡힌다는 걸 놓고 보면, 이 락을 잡는 사이 시그널이 처리될 수 있습니다.
	여기 연관된 시그널 핸들러는 락~B 를 잡을 수도 있고, 따라서 락~B 는 락~A
	를 쥔 상태에서 획득될 수 있습니다.
	따라서, 우리가 락~B 를 잡은 상태에서 락~A 도 잡으면, 우린 deadlock
	사이클을 갖게 됩니다.
	이 문제는 락~B 를 잡고 있는 사이 시그널이 블록되어 있다 해도 존재함을
	알아두시기 바랍니다.

	이는 인터럽트나 시그널 핸들러 내에서 획득되는 락에 무척 조심스러워야
	하는 또다른 이유입니다.
	하지만 리눅스 커널의 락 의존성 검사기는 이 상황을 포함해 많은 걸 알고
	있으니, 부디 그걸 항상 사용하십시오!

	\iffalse

	Because this would lead to deadlock.
	Given that Lock~A is sometimes held outside of a signal
	handler without blocking signals, a signal might be handled while
	holding this lock.
	The corresponding signal handler might then acquire
	Lock~B, so that Lock~B is acquired while holding Lock~A.
	Therefore, if we also acquire Lock~A while holding Lock~B,
	we will have a deadlock cycle.
	Note that this problem exists even if signals are blocked while
	holding Lock~B.

	This is another reason to be very careful with locks that are
	acquired within interrupt or signal handlers.
	But the Linux kernel's lock dependency checker knows about this
	situation and many others as well, so please do make full use
	of it!

	\fi

}\QuickQuizEnd

어떤 락이 여러 시그널을 위한 핸들러에 의해 획득된다면, 해당 락이 하나의 시그널
핸들러에 의해서 획득될 때 조차도 해당 락이 획득될 때마다 이 시그널들은 모두
블록되어야만 할 겁니다.

\iffalse

If a lock is acquired by the handlers for several signals, then each
and every one of these signals must be blocked whenever that lock is
acquired, even when that
lock is acquired within a signal handler.

\fi

\QuickQuiz{
	시그널 핸들러 내에서 어떻게 시그널들을 합법적으로 블록할 수 있죠?

	\iffalse

	How can you legally block signals within a signal handler?

	\fi

}\QuickQuizAnswer{
	그렇게 할 수 있는 가장 간단하고 빠른 방법은 해당 시그널을 설정할 때
	\co{sigaction()} 에 넘기는 \co{struct sigaction} 의 \co{sa_mask} 필드를
	사용하는 겁니다.

	\iffalse

	One of the simplest and fastest ways to do so is to use
	the \co{sa_mask} field of the \co{struct sigaction} that
	you pass to \co{sigaction()} when setting up the signal.

	\fi

}\QuickQuizEnd

불행히도, 시그널을 블록하고 블록 해제하는 것은 리눅스를 포함한 일부
운영체제에선 값비싼 행위이므로, 성능에 대한 염려는 시그널 핸들러 내에서
획득되는 락들은 시그널 핸들러에서만 획득되며, 어플리케이션 코드와 시그널 핸들러
사이에서 통신하는 데에는 lockless 동기화 메커니즘이 사용되어야 함을 의미합니다.

또는 시그널 핸들러는 치명적 에러를 처리하는 경우를 제외하고는 완전히 사용되지
않을 수도 있겠습니다.

\iffalse

Unfortunately, blocking and unblocking signals can be expensive in
some operating systems, notably including Linux, so performance
concerns often mean that locks acquired in signal handlers are only
acquired in signal handlers, and that lockless synchronization
mechanisms are used to communicate between application code and
signal handlers.

Or that signal handlers are avoided completely except for handling
fatal errors.

\fi

\QuickQuiz{
	시그널 핸들러 내에서 락을 잡는게 그렇게 나쁜 생각이라면, 그걸 안전하게
	하는 방법을 이야기 하는 이유는 뭐죠?

	\iffalse

	If acquiring locks in signal handlers is such a bad idea, why
	even discuss ways of making it safe?

	\fi

}\QuickQuizAnswer{
	똑같은 규칙이 운영체제 커널과 일부 임베디드 어플리케이션의 인터럽트
	핸들러에도 적용되기 때문입니다.

	많은 어플리케이션 환경에서, 시그널 핸들러 내에서 락을 잡는 건 나쁜
	행위로 알려져 있습니다~\cite{OpenGroup1997pthreads}.
	하지만, 이는 영리한 개발자들이 (아마도 현명치 못하게도) 어토믹
	오퍼레이션을 가지고 집에서 만든 락 기능을 사용하는 걸 막지는 못합니다.
	그리고 어토믹 오퍼레이션들은 많은 경우 시근러 핸들러 내에서 사용이
	합법입니다.

	\iffalse

	Because these same rules apply to the interrupt handlers used in
	operating-system kernels and in some embedded applications.

	In many application environments, acquiring locks in signal
	handlers is frowned upon~\cite{OpenGroup1997pthreads}.
	However, that does not stop clever developers from (perhaps
	unwisely) fashioning home-brew locks out of atomic operations.
	And atomic operations are in many cases perfectly legal in
	signal handlers.

	\fi

}\QuickQuizEnd

\subsubsection{Discussion}
\label{sec:locking:Locking Hierarchy Discussion}

공유메모리 병렬 프로그래머가 사용할 수 있는 굉장히 많은 deadlock 회피 기법들이
존재합니다만, 그것들 중 어느 것도 잘 들어맞지 않는 순차적 프로그램들이
있습니다.
이는 전문가 프로그래머들이 그들의 도구상자에 여러개의 도구를 가지고 다니는
이유입니다: 락킹은 강력한 동시성 도구이지만, 다른 도구들로 처리되는게 나은
일들도 있습니다.

\iffalse

There are a large number of deadlock-avoidance strategies available to
the shared-memory parallel programmer, but there are sequential
programs for which none of them is a good fit.
This is one of the reasons that expert programmers have more than
one tool in their toolbox: locking is a powerful concurrency
tool, but there are jobs better addressed with other tools.

\fi

\QuickQuiz{
	간단한 락킹 계층, 레이어나 다른 것들이 존재 하지 않게끔 제어를 객체
	그룹 간에 자유로이 넘겨대는 객체 지향 어플리케이션에서\footnote{
		``객체 지향 스파게티 코드'' 라고도 알려져 있습니다.}
	는 이 어플리케이션을 어떻게 병렬화 시킬 수 있을까요?

	\iffalse

	Given an object-oriented application that passes control freely
	among a group of objects such that there is no straightforward
	locking hierarchy,\footnote{
		Also known as ``object-oriented spaghetti code.''}
	layered or otherwise, how can this
	application be parallelized?

	\fi

}\QuickQuizAnswer{
	여러 방법들이 있습니다:
	\begin{enumerate}
	\item	많은 수의 시뮬레이션이 (예를 들면) 기계적이나 전자적 기기의
		좋은 설계로 수렴되게 하게끔 하는 시뮬레이션을 통한 패러메트릭
		탐색의 경우, 시뮬레이션을 싱글쓰레드고 두되, 시뮬레이션의 각
		인스턴스를 병렬로 돌립니다.
		이는 객체 지향 설계를 유지하며, 높은 단계에서의 병렬성을 얻게
		하며, 또한 deadklock 과 동기화 오버헤드를 회피합니다.
	\item	객체를 한번에 하나의 그룹 이상의 객체들과는 작업하지 않게끔
		여러 그룹으로 나눕니다.
		그러고 나면 각 그룹에 락 하나씩을 연관짓습니다.
		이는
		Section~\ref{sec:locking:Single-Lock-at-a-Time Designs}
		에서 이야기 된 한번에 하나의 락 설계의 예입니다.
	\item	객체를 어떤 그룹 기반 순서대로 각 그룹의 객체들과 작업하게끔
		그룹짓습니다.
		그리고 각 그룹별로 락을 연관짓고, 그룹들 간의 락킹 계층을
		넣습니다.
	\item	임의로 선택된 계층을 락들에 넣고, 락을 역순서로 획득해야 하면
		Section~\ref{sec:locking:Conditional Locking} 에서 이야기한
		조건적 락킹을 사용합니다.

	\iffalse

	There are a number of approaches:
	\begin{enumerate}
	\item	In the case of parametric search via simulation,
		where a large number of simulations will be run
		in order to converge on (for example) a good design
		for a mechanical or electrical device, leave the
		simulation single-threaded, but run many instances
		of the simulation in parallel.
		This retains the object-oriented design, and gains
		parallelism at a higher level, and likely also avoids
		both deadlocks and synchronization overhead.
	\item	Partition the objects into groups such that there
		is no need to operate on objects in
		more than one group at a given time.
		Then associate a lock with each group.
		This is an example of a single-lock-at-a-time
		design, which discussed in
		Section~\ref{sec:locking:Single-Lock-at-a-Time Designs}.
	\item	Partition the objects into groups such that threads
		can all operate on objects in the groups in some
		groupwise ordering.
		Then associate a lock with each group, and impose a
		locking hierarchy over the groups.
	\item	Impose an arbitrarily selected hierarchy on the locks,
		and then use conditional locking if it is necessary
		to acquire a lock out of order, as was discussed in
		Section~\ref{sec:locking:Conditional Locking}.

	\fi

	\item	주어진 오퍼레이션 그룹을 이어가기 전에, 어떤 락이 획득될지
		예측하고, 실제로 어떤 업데이트를 하기 전에 그것들을 획득하려
		합니다.
		만약 이 예측이 올바르지 않았음이 드러난다면, 모든 락을 내려놓고
		경험의 이익을 포함하는 업데이트 된 예측을 가지고 재시도 합니다.
		이 방법은
		Section~\ref{sec:locking:Acquire Needed Locks First}
		에서 이야기 되었습니다.
	\item	트랜잭셔널 메모리를 사용합니다.
		이 방법은
		\crefrange{sec:future:Transactional Memory}{sec:future:Hardware Transactional Memory}
		에서 이야기 될 여러 장단점을 가지고 있습니다.
	\item	어플리케이션을 더욱 동시성 친화적으로 리팩토링 합니다.
		이는 싱글쓰레드로 돌아갈 때 조차도 더 빨리 수행되도록
		어플리케이션을 만드는 부작용 또한 가질 확률이 높으나,
		어플리케이션을 수정하기 더 어렵게 할수도 있습니다.
	\item	락킹에 추가적으로 뒤 챕터들에서 다룰 기법들을 사용합니다.
	\end{enumerate}

	\iffalse


	\item	Before carrying out a given group of operations, predict
		which locks will be acquired, and attempt to acquire them
		before actually carrying out any updates.
		If the prediction turns out to be incorrect, drop
		all the locks and retry with an updated prediction
		that includes the benefit of experience.
		This approach was discussed in
		Section~\ref{sec:locking:Acquire Needed Locks First}.
	\item	Use transactional memory.
		This approach has a number of advantages and disadvantages
		which will be discussed in
		\crefrange{sec:future:Transactional Memory}{sec:future:Hardware Transactional Memory}.
	\item	Refactor the application to be more concurrency-friendly.
		This would likely also have the side effect of making
		the application run faster even when single-threaded, but might
		also make it more difficult to modify the application.
	\item	Use techniques from later chapters in addition to locking.
	\end{enumerate}

	\fi

}\QuickQuizEnd

그러나, 이 섹션에서 이야기된 전법들은 많은 상황에서 유용함으로 증명되었습니다.

\iffalse

Nevertheless, the strategies described in this section have proven
quite useful in many settings.

\fi

\subsection{Livelock and Starvation}
\label{sec:locking:Livelock and Starvation}

조건적 락킹이 효과적인 deadlock 회피 메커니즘이 될 수 있지만, 오용될 수
있습니다.
예를 들어
Listing~\ref{lst:locking:Abusing Conditional Locking} 에 보인 아름답도록
대칭적인 예를 예로 들어 봅시다.
이 예의 아름다움은 추악한 livelock 을 감춥니다.
이를 보기 위해, 아래와 같은 이벤트들의 순서를 생각해 봅시다:

\iffalse

Although conditional locking can be an effective deadlock-avoidance
mechanism, it can be abused.
Consider for example the beautifully symmetric example shown in
Listing~\ref{lst:locking:Abusing Conditional Locking}.
This example's beauty hides an ugly livelock.
To see this, consider the following sequence of events:

\fi

\begin{listing}[tbp]
\begin{fcvlabel}[ln:locking:Abusing Conditional Locking]
\begin{VerbatimL}[commandchars=\\\[\]]
void thread1(void)
{
retry:					\lnlbl[thr1:retry]
	spin_lock(&lock1);		\lnlbl[thr1:acq1]
	do_one_thing();
	if (!spin_trylock(&lock2)) {	\lnlbl[thr1:try2]
		spin_unlock(&lock1);    \lnlbl[thr1:rel1]
		goto retry;
	}
	do_another_thing();
	spin_unlock(&lock2);
	spin_unlock(&lock1);
}

void thread2(void)
{
retry:					\lnlbl[thr2:retry]
	spin_lock(&lock2);		\lnlbl[thr2:acq2]
	do_a_third_thing();
	if (!spin_trylock(&lock1)) {	\lnlbl[thr2:try1]
		spin_unlock(&lock2);	\lnlbl[thr2:rel2]
		goto retry;
	}
	do_a_fourth_thing();
	spin_unlock(&lock1);
	spin_unlock(&lock2);
}
\end{VerbatimL}
\end{fcvlabel}
\caption{Abusing Conditional Locking}
\label{lst:locking:Abusing Conditional Locking}
\end{listing}

\begin{fcvref}[ln:locking:Abusing Conditional Locking]
\begin{enumerate}
\item	쓰레드~1 이 라인~\lnref{thr1:acq1} 에서 \co{lock1} 을 잡고,
	\co{do_one_thing()} 을 호출합니다.
\item	쓰레드~2 가 라인~\lnref{thr2:acq2} 에서 \co{lock2} 를 잡고
	\co{do_a_third_thing()} 을 호출합니다.
\item	쓰레드~1 이 라인~\lnref{thr1:try2} 에서 \co{lock2} 를 잡으려 하지만,
	쓰레드~2 가 이를 잡고 있으므로 실패합니다.
\item	쓰레드~2 가 라인~\lnref{thr2:try1} 에서 \co{lock1} 을 잡으려 하지만,
	쓰레드~1 이 이를 잡고 있으므로 실패합니다.
\item	쓰레드~1 이 라인~\lnref{thr1:rel1} 에서 \co{lock1} 을 놓고,
	라인~\lnref{thr1:retry} 의 \co{retry} 로 점프합니다.
\item	쓰레드~2 가 라인~\lnref{thr2:rel2} 에서 \co{lock2} 를 내려놓고,
	라인~\lnref{thr2:retry} 의 \co{retry} 로 점프합니다.
\item	이 livelock 이 처음부터 반복됩니다.

\iffalse

\item	Thread~1 acquires \co{lock1} on line~\lnref{thr1:acq1}, then invokes
	\co{do_one_thing()}.
\item	Thread~2 acquires \co{lock2} on line~\lnref{thr2:acq2}, then invokes
	\co{do_a_third_thing()}.
\item	Thread~1 attempts to acquire \co{lock2} on line~\lnref{thr1:try2},
	but fails because Thread~2 holds it.
\item	Thread~2 attempts to acquire \co{lock1} on line~\lnref{thr2:try1},
	but fails because Thread~1 holds it.
\item	Thread~1 releases \co{lock1} on line~\lnref{thr1:rel1},
	then jumps to \co{retry} at line~\lnref{thr1:retry}.
\item	Thread~2 releases \co{lock2} on line~\lnref{thr2:rel2},
	and jumps to \co{retry} at line~\lnref{thr2:retry}.
\item	The livelock dance repeats from the beginning.

\fi

\end{enumerate}
\end{fcvref}

\QuickQuiz{
	Listing~\ref{lst:locking:Abusing Conditional Locking}
	에 보인 livelock 은 어떻게 회피될 수 있나요?

	\iffalse

	How can the livelock shown in
	Listing~\ref{lst:locking:Abusing Conditional Locking}
	be avoided?

	\fi

}\QuickQuizAnswer{
	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	은 몇가지 좋은 힌트를 제공합니다.
	많은 경우, livelock 은 여러분이 락킹 설계를 다시 고려해야 한다는
	힌트입니다.
	또는 여러분의 락킹 설계가 ``막 자랐다면'' 처음으로 방문해야 하는
	곳입니다.

	그러나, Doug Lea 가 제안한 충분히 좋은 한가지 방법은
	Section~\ref{sec:locking:Conditional Locking} 에 이야기된 것과 같이
	조건적 락킹을 사용하되 이를
	Section~\ref{sec:locking:Acquire Needed Locks First}
	에서 이야기한 것처럼 공유된 데이터를 수정하기 전에 모든 필요한 락들을
	먼저 잡는 것입니다.
	특정 크리티컬 섹션이 너무 여러번 재시도 된다면, 무조건적으로 전역 락을
	잡고, 무조건적으로 모든 필요한 락들을 잡습니다.
	이는 deadlock 과 livelock 을 모두 회피하며, 이 전역 락은 너무 자주
	잡히지 않는다는 가정 하에 합리적 수준으로 확장합니다.

	\iffalse

	Listing~\ref{lst:locking:Avoiding Deadlock Via Conditional Locking}
	provides some good hints.
	In many cases, livelocks are a hint that you should revisit your
	locking design.
	Or visit it in the first place if your locking design
	``just grew''.

	That said, one good-and-sufficient approach due to Doug Lea
	is to use conditional locking as described in
	Section~\ref{sec:locking:Conditional Locking}, but combine this
	with acquiring all needed locks first, before modifying shared
	data, as described in
	Section~\ref{sec:locking:Acquire Needed Locks First}.
	If a given critical section retries too many times,
	unconditionally acquire
	a global lock, then unconditionally acquire all the needed locks.
	This avoids both deadlock and livelock, and scales reasonably
	assuming that the global lock need not be acquired too often.

	\fi

}\QuickQuizEnd

Livelock 은 한 그룹의 쓰레드 중 하나가 아니라 전체가 기아에 빠지는 극단적
형태의 starvation 이라 할 수 있습니다.\footnote{
	Livelock, starvation, 그리고 unfairness 같은 용어들의 정확한 정의에
	너무 매달려 있지는 마세요.
	어떤 그룹의 쓰레드들이 적절한 진행을 내는 것을 막는 모든 것은 고쳐져야
	하는 버그이며, 논쟁적인 이름들은 버그를 고치지 않습니다.}

\iffalse

Livelock can be thought of as an extreme form of starvation where
a group of threads starves, rather than just one of them.\footnote{
	Try not to get too hung up on the exact definitions of terms
	like livelock, starvation, and unfairness.
	Anything that causes a group of threads to fail to make adequate
	forward progress is a bug that needs to be fixed, and debating
	names doesn't fix bugs.}

\fi

\begin{listing}[tbp]
\begin{fcvlabel}[ln:locking:Conditional Locking and Exponential Backoff]
\begin{VerbatimL}[commandchars=\\\[\]]
void thread1(void)
{
	unsigned int wait = 1;
retry:
	spin_lock(&lock1);
	do_one_thing();
	if (!spin_trylock(&lock2)) {
		spin_unlock(&lock1);
		sleep(wait);
		wait = wait << 1;
		goto retry;
	}
	do_another_thing();
	spin_unlock(&lock2);
	spin_unlock(&lock1);
}

void thread2(void)
{
	unsigned int wait = 1;
retry:
	spin_lock(&lock2);
	do_a_third_thing();
	if (!spin_trylock(&lock1)) {
		spin_unlock(&lock2);
		sleep(wait);
		wait = wait << 1;
		goto retry;
	}
	do_a_fourth_thing();
	spin_unlock(&lock1);
	spin_unlock(&lock2);
}
\end{VerbatimL}
\end{fcvlabel}
\caption{Conditional Locking and Exponential Backoff}
\label{lst:locking:Conditional Locking and Exponential Backoff}
\end{listing}

Livelock 과 starvation 은 소프트웨어 트랜잭셔널 메모리 구현의 심각한 문제이며,
이 문제를 캡슐화 하기 위해 \emph{컨텐션 매니저} 의 개념이 도입되었습니다.
락킹의 경우, 간단한 exponential backoff 는 livelock 과 starvation 을 종종
해결할 수 있습니다.
아이디어는
Listing~\ref{lst:locking:Conditional Locking and Exponential Backoff}
에 보인 것과 같이 각 재시도 사이의 딜레이를 지수적으로 증가시키는 겁니다.

\iffalse

Livelock and starvation are serious issues in software transactional
memory implementations, and so the concept of \emph{contention
manager} has been introduced to encapsulate these issues.
In the case of locking, simple exponential backoff can often address
livelock and starvation.
The idea is to introduce exponentially increasing delays before each
retry, as shown in
Listing~\ref{lst:locking:Conditional Locking and Exponential Backoff}.

\fi

\QuickQuiz{
	Listing~\ref{lst:locking:Conditional Locking and Exponential Backoff}
	의 코드에는 어떤 문제들이 있나요?

	\iffalse

	What problems can you spot in the code in
	Listing~\ref{lst:locking:Conditional Locking and Exponential Backoff}?

	\fi

}\QuickQuizAnswer{
	여기 두가지 문제가 있습니다:
	\begin{enumerate}
	\item	1초의 대기는 대부분의 경우 너무 깁니다.
		대기 기간은 대략 이 크리티컬 섹션을 수행하는데 걸리는 시간으로
		시작되어야 하는데, 이는 보통 마리크로세컨드나 밀리세컨드 범위가
		될 겁니다.
	\item	이 코드는 오버플로우를 검사하지 않습니다.
		한편, 이 버그는 앞의 버그에 의해 제거될 수 있습니다: 32 비트로
		초를 저장하면 50년도 넘는 시간을 표현할 수 있습니다.
	\end{enumerate}

	\iffalse

	Here are a couple:
	\begin{enumerate}
	\item	A one-second wait is way too long for most uses.
		Wait intervals should begin with roughly the time
		required to execute the critical section, which will
		normally be in the microsecond or millisecond range.
	\item	The code does not check for overflow.
		On the other hand, this bug is nullified
		by the previous bug: 32 bits worth of seconds is
		more than 50 years.
	\end{enumerate}

	\fi

}\QuickQuizEnd

더 나은 결과를 위해, backoff 는 제한되어야 하며, 이보다도 나은 높은
컨텐션에서의 결과는
Section~\ref{sec:locking:Other Exclusive-Locking Implementations}
에서 더 이야기 되는 queued locking~\cite{Anderson90} 을 통해 얻어질 수
있습니다.
물론, 최고의 방법은 낮은 락 컨텐션을 유지함으로써 이런 문제들을 방지하는 좋은
병렬 설계를 사용하는 것입니다.

\iffalse

For better results, backoffs should be bounded, and
even better high-contention results are obtained via queued
locking~\cite{Anderson90}, which is discussed more in
Section~\ref{sec:locking:Other Exclusive-Locking Implementations}.
Of course, best of all is to use a good parallel design that avoids
these problems by maintaining low lock contention.

\fi

\subsection{Unfairness}
\label{sec:locking:Unfairness}

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{cpu/SystemArch}}
\caption{System Architecture and Lock Unfairness}
\label{fig:locking:System Architecture and Lock Unfairness}
\end{figure}

Unfairness 는 쓰레드들 중 특정 락을 가지고 경쟁하는 일부가 획득의 특권을 갖는,
덜 심각한 형태의 starvation 으로 생각될 수 있습니다.
이는 공유된 캐쉬나 NUMA 특성을 갖는 기계에서 일어날 수 있는데, 예를 들면
Figure~\ref{fig:locking:System Architecture and Lock Unfairness} 와 같습니다.
CPU~0 이 모든 다른 CPU 들이 획득하고자 하는 락을 놓으면, CPU~0 과~1 사이에
공유된 연결부는 CPU~1 이 CPU~2--7 보다 이득을 얻을 것을 의미합니다.
따라서 CPU~1 은 락을 획득할 가능성이 높습니다.
CPU~0 이 이 락을 다시 잡으려 할만큼 충분히 CPU~1 이 이 락을 길게 소유한다면,
그리고 그 반대의 경우에도, 이 락은 CPU~2--7 을 제끼고 CPU~0 과~1 사이에서
돌아다닐 겁니다.

\iffalse

Unfairness can be thought of as a less-severe form of starvation,
where a subset of threads contending for a given lock are granted
the lion's share of the acquisitions.
This can happen on machines with shared caches or NUMA characteristics,
for example, as shown in
Figure~\ref{fig:locking:System Architecture and Lock Unfairness}.
If CPU~0 releases a lock that all the other CPUs are attempting
to acquire, the interconnect shared between CPUs~0 and~1 means that
CPU~1 will have an advantage over CPUs~2--7.
Therefore CPU~1 will likely acquire the lock.
If CPU~1 holds the lock long enough for CPU~0 to be requesting the
lock by the time CPU~1 releases it and vice versa, the lock can
shuttle between CPUs~0 and~1, bypassing CPUs~2--7.

\fi

\QuickQuiz{
	Unfairness 를 회피할 정도로 락 경쟁을 낮게 유지하는 좋은 병렬 설계를
	사용하는게 낫지 않을까요?

	\iffalse

	Wouldn't it be better just to use a good parallel design
	so that lock contention was low enough to avoid unfairness?

	\fi
}\QuickQuizAnswer{
	어떤 면에선 그게 낫겠습니다만, 때로는 높은 락 경쟁을 초래하는 설계를
	사용하는게 적절한 상황이 있습니다.

	예를 들어, 드문 에러 조건을 갖는 시스템을 상상해 보세요.
	그런 드문 에러 조건 중 하나가 발동되었을 때에만 도움이 되는 복잡하고
	디버깅 하기 어려운 설계보다는 이 드문 에러 상황의 기간 동안만 낮은
	성능과 확장성을 갖는 간단한 에러 처리 설계가 최고일 수도 있습니다.

	그러나, 예를 들면 문제 자체를 쪼개는 것과 같은, 간단하면서도 에러 조건
	하에서도 효율적인 설계를 시도하는 노력을 할 가치는 있습니다.

	\iffalse

	It would be better in some sense, but there are situations
	where it can be appropriate to use
	designs that sometimes result in high lock contentions.

	For example, imagine a system that is subject to a rare error
	condition.
	It might well be best to have a simple error-handling design
	that has poor performance and scalability for the duration of
	the rare error condition, as opposed to a complex and
	difficult-to-debug design that is helpful only when one of
	those rare error conditions is in effect.

	That said, it is usually worth putting some effort into
	attempting to produce a design that both simple as well as
	efficient during error conditions, for example by partitioning
	the problem.

	\fi

}\QuickQuizEnd

\subsection{Inefficiency}
\label{sec:locking:Inefficiency}

락은 어토믹 인스트럭션과 메모리 배리어를 사용해 구현되며 종종 캐쉬 미스를
일으킵니다.
Chapter~\ref{chp:Hardware and its Habits} 에서 보았듯, 이 인스트럭션들은 상당히
미용이 높은데, 간단한 인스트럭션들보다 대략 수백배 높은 오버헤드를 갖습니다.
이는 락킹에 있어 중요한 문제가 될 수 있습니다: 여러분이 하나의 인스트럭션을
락으로 보호한다면, 여러분은 이 오버헤드를 백배 가량 증가시키게 됩니다.
완벽한 확장성을 가정한다 해도, 같은 코드를 락킹 없이 수행하는 하나의 CPU 의
성능을 내려면 \emph{백개의} CPU 가 필요할 겁니다.

이 상황은 Section~\ref{sec:SMPdesign:Synchronization Granularity}, 특히
Figure~\ref{fig:SMPdesign:Synchronization Efficiency} 에서 설명한
동기화\-/granularity 트레이드오프의 중요성을 강조합니다:
너무 coarse 한 granularity 는 확장성을 제한하는 반면, 너무 fine 한 granularity
는 지나친 동기화 오버헤드를 초래합니다.

락을 잡는 건 높은 비용을 초래하지만, 일단 잡고 나면, 이 CPU 의 캐쉬는 적어도 큰
크리티컬 섹션에 대해서는 효과적인 성능 부스터가 됩니다.
또한, 일단 락을 잡고 나면, 이 락에 의해 보호되는 데이터는 다른 쓰레드로부터의
간섭 없이 이 락을 잡은 쓰레드에 의해 접근될 수 있습니다.

\iffalse

Locks are implemented using atomic instructions and memory barriers,
and often involve cache misses.
As we saw in Chapter~\ref{chp:Hardware and its Habits},
these instructions are quite expensive, roughly two
orders of magnitude greater overhead than simple instructions.
This can be a serious problem for locking: If you protect a single
instruction with a lock, you will increase the overhead by a factor
of one hundred.
Even assuming perfect scalability, \emph{one hundred} CPUs would
be required to keep up with a single CPU executing the same code
without locking.

This situation underscores the synchronization\-/granularity
tradeoff discussed in Section~\ref{sec:SMPdesign:Synchronization Granularity},
especially Figure~\ref{fig:SMPdesign:Synchronization Efficiency}:
Too coarse a granularity will limit scalability, while too fine a
granularity will result in excessive synchronization overhead.

Acquiring a lock might be expensive, but once held, the CPU's caches
are an effective performance booster, at least for large critical sections.
In addition, once a lock is held, the data protected by that lock can
be accessed by the lock holder without interference from other threads.

\fi

\QuickQuiz{
	락을 잡은 쓰레드는 어떻게 간섭받을 수 있나요?

	\iffalse

	How might the lock holder be interfered with?

	\fi

}\QuickQuizAnswer{
	이 락에 의해 보호되는 데이터가 락 자체와 같은 캐쉬라인에 존재한다면
	다른 CPU 들의 이 락을 획득하려는 시도들은 이 락을 쥐고 있는 CPU 에게
	비싼 캐쉬 미스를 일으킬 겁니다.
	이는 거짓 공유 (false sharing) 의 특수한 경우로, 다른 락들로 보호되는
	한쌍의 변수가 같은 캐쉬 라인에 있을 때에도 일어날 수 있습니다.
	반대로, 이 락이 자신이 보호하는 데이터와 다른 캐쉬 라인에 있다면, 이
	락을 쥐고 있는 CPU 는 해당 변수로의 첫번째 액세스 때에만 캐쉬 미스를 낼
	겁니다.

	물론, 락과 데이터를 별도의 캐쉬 라인에 두는 것의 단점은 경쟁되지 않은
	경우에 단 하나가 아니라 두개의 캐쉬 미스를 일으킬 것이라는 겁니다.
	항상 그렇듯, 현명한 선택을 하세요!

	\iffalse

	If the data protected by the lock is in the same cache line
	as the lock itself, then attempts by other CPUs to acquire
	the lock will result in expensive cache misses on the part
	of the CPU holding the lock.
	This is a special case of false sharing, which can also occur
	if a pair of variables protected by different locks happen
	to share a cache line.
	In contrast, if the lock is in a different cache line than
	the data that it protects, the CPU holding the lock will
	usually suffer a cache miss only on first access to a given
	variable.

	Of course, the downside of placing the lock and data into separate
	cache lines is that the code will incur two cache misses rather
	than only one in the uncontended case.
	As always, choose wisely!

	\fi

}\QuickQuizEnd

\section{Types of Locks}
\label{sec:locking:Types of Locks}
%
\epigraph{Only locks in life are what you think you know, but don't.
	  Accept your ignorance and try something new.}
	 {\emph{Dennis Vickers}}

놀랄만큼 많은 수의 락 타입이 존재하는데, 이 짧은 챕터가 담을 수 있는 정도보다
큽니다.
다음 섹션들은 배타적 락 (Section~\ref{sec:locking:Exclusive Locks}),
reader-writer 락 (Section~\ref{sec:locking:Reader-Writer Locks}),
multi-role 락 (Section~\ref{sec:locking:Beyond Reader-Writer Locks}),
그리고 scoped 락킹 (Section~\ref{sec:locking:Scoped Locking}) 을 이야기 합니다.

\iffalse

There are a surprising number of types of locks, more than this
short chapter can possibly do justice to.
The following sections discuss
exclusive locks (Section~\ref{sec:locking:Exclusive Locks}),
reader-writer locks (Section~\ref{sec:locking:Reader-Writer Locks}),
multi-role locks (Section~\ref{sec:locking:Beyond Reader-Writer Locks}),
and scoped locking (Section~\ref{sec:locking:Scoped Locking}).

\fi

\subsection{Exclusive Locks}
\label{sec:locking:Exclusive Locks}

배타적 락은 말 그대로입니다: 한번에 하나의 쓰레드만이 이 락을 잡을 수 있습니다.
따라서, 이런 락을 잡은 쓰레드는 해당 락에 의해 보호되는 모든 데이터로의 배타적
접근 권한을 가지며, 그런 이유로 그런 이름이 붙었습니다.

물론, 이는 이 락이 이 락에 의해 보호되는 데이터에 대한 모든 접근 시에 잡힌다는
것을 가정하고 있습니다.
도움을 줄 수 있는 도구가 일부 있기는 하지만 (예를 들어
Section~\ref{sec:formal:Axiomatic Approaches and Locking} 을 참고하세요), 이
락이 항상 획득된다는 것을 보장하는 궁극적 책임은 개발자에게 있습니다.

\iffalse

Exclusive locks are what they say they are: only one thread may hold
the lock at a time.
The holder of such a lock thus has exclusive access to all data protected
by that lock, hence the name.

Of course, this all assumes that this lock is held across all accesses
to data purportedly protected by the lock.
Although there are some tools that can help (see for example
Section~\ref{sec:formal:Axiomatic Approaches and Locking}),
the ultimate responsibility for ensuring that the lock is always acquired
when needed rests with the developer.

\fi

\QuickQuiz{
	배타적 락을 잡자마자 곧바로 놔버리는, 즉 텅 빈 크리티컬 섹션 같은 것을
	갖는 것은 말이 될까요?

	\iffalse

	Does it ever make sense to have an exclusive lock acquisition
	immediately followed by a release of that same lock, that is,
	an empty critical section?

	\fi

}\QuickQuizAnswer{
	텅 빈 락 기반 크리티컬 섹션은 드물게만 사용되지만, 쓰임새가 있습니다.
	핵심은 배타적 락의 의미는 두개의 컴포넌트를 갖는다는 것입니다:
	(1)~친숙한 데이터 보호 의미 그리고 (2)~어떤 락을 해제한다는 것이 같은
	락의 획득을 기다리는 쓰레드에게 보내는 메세지의 의미.
	텅 빈 크리티컬 섹션은 데이터 보호 컴포넌트 없이 이 메세지 컴포넌트를
	사용합니다.

	이 답의 나머지 부분은 텅 빈 크리티컬 섹션의 사용 예를 제공합니다만, 이
	예들은 ``회색 마법'' 으로 여겨져야 합니다.\footnote{
		이 설명을 제공한 Alexey Roytman 에게 감사를 표합니다.}
	그처럼, 텅 빈 크리티컬 섹션은 실제 환경에서는 거의 사용되지 않습니다.
	그러나, 이 회색 영역으로 들어가 봅시다 \ldots

	텅 빈 크리티컬 섹션의 역사적 사용 중 하나는 2.4 리눅스 커널의 네트워킹
	스택에 ``big reader lock'' 의 약자로 \co{brlock} 이라 불린, 읽기
	쓰레드쪽 확장성 있는 reader-writer 락에 의해 등장했습니다.
	이 사용 예는
	Section~\ref{sec:defer:Read-Copy Update (RCU)} 에서 설명되는 read-copy
	update (RCU) 의 의미를 간략화 하는 방법이었습니다.
	그리고 실제로 이 리눅스 커널의 사용 예는 RCU 로 대체되었습니다.

	\iffalse

	Empty lock-based critical sections are rarely used, but they
	do have their uses.
	The point is that the semantics of exclusive locks have two
	components: (1)~the familiar data-protection semantic and
	(2)~a messaging semantic, where releasing a given lock notifies
	a waiting acquisition of that same lock.
	An empty critical section uses the messaging component without
	the data-protection component.

	The rest of this answer provides some example uses of empty
	critical sections, however, these examples should be considered
	``gray magic.''\footnote{
		Thanks to Alexey Roytman for this description.}
	As such, empty critical sections are almost never used in practice.
	Nevertheless, pressing on into this gray area \ldots

	One historical use of empty critical sections appeared in the
	networking stack of the 2.4 Linux kernel through use of a
	read-side-scalable reader-writer lock called \co{brlock}
	for ``big reader lock''.
	This use case is a way of approximating the semantics of read-copy
	update (RCU), which is discussed in
	Section~\ref{sec:defer:Read-Copy Update (RCU)}.
	And in fact this Linux-kernel use case has been replaced
	with RCU\@.

	\fi

	이 텅 빈 락 기반 크리티컬 섹션 사용법은 일부 상황에서 락 컨텐션을
	줄이기 위해 사용될 수도 있습니다.
	예를 들어, 각 쓰레드가 쓰레드별 리스트에 관리되는 일의 단위를 처리하며,
	쓰레드들은 각자의 리스트를 만지는 게
	금지된~\cite{PaulEMcKenney2012EmptyLocks} 멀티쓰레드 기반 유저 스페이스
	어플리케이션을 생각해 봅시다.
	앞서 스케쥴된 모든 일 단위들이 업데이트가 진행되기 전에 완료되어야 함을
	필요로 하는 업데이트도 있을 수 있습니다.
	이를 처리하는 한가지 방법은 각 쓰레드에 하나의 일 단위를 스케쥴 해서,
	이 모든 일 단위들이 완료되었을 때, 업데이트가 진행될 수 있게 하는
	것입니다.

	\iffalse

	The empty-lock-critical-section idiom can also be used to
	reduce lock contention in some situations.
	For example, consider a multithreaded user-space application where
	each thread processes units of work maintained in a per-thread
	list, where threads are prohibited from touching each others'
	lists~\cite{PaulEMcKenney2012EmptyLocks}.
	There could also be updates that require that all previously
	scheduled units of work have completed before the update can
	progress.
	One way to handle this is to schedule a unit of work on each
	thread, so that when all of these units of work complete, the
	update may proceed.

	\fi

	일부 어플리케이션에서, 쓰레드는 오고 갈 수 있습니다.
	예를 들어, 각 쓰레드는 어플리케이션의 한 사용자에 연관될 수도 있고,
	따라서 해당 유저가 로그아웃 하거나 연결이 끊겼을 때 제거될 수 있습니다.
	많은 어플리케이션에서, 쓰레드는 원자적으로 떠날 수 없습니다: 그대신
	명시적으로 스스로를 특정 순서의 액션들을 사용해 어플리케이션의 다양한
	부분으로부터 제거해야 합니다.
	그런 특정 액션 하나는 다른 쓰레드로부터 더이상 요청을 받는 것을
	거부하는 것, 그리고 다른 특정 액션은 자신의 리스트의 남아있는 일
	단위들을 폐기하는 것, 예를 들어 이 일 단위들을 전역 일 항목 폐기
	리스트에 넣어 남아있는 쓰레드 중 하나가 이를 취하도록 하는 것일 겁니다.
	(왜 그 일들을 직접 함으로써 이 쓰레드의 일 항목 리스트를 비우지
	않을까요?
	주어진 일 항목이 더 많은 일 항목을 생성해서 이 리스트가 빠른 시간 내에
	비워지지 않을 수 있기 때문입니다.)

	\iffalse

	In some applications, threads can come and go.
	For example, each thread might correspond to one user of the
	application, and thus be removed when that user logs out or
	otherwise disconnects.
	In many applications, threads cannot depart atomically: They must
	instead explicitly unravel themselves from various portions of
	the application using a specific sequence of actions.
	One specific action will be refusing to accept further requests
	from other threads, and another specific action will be disposing
	of any remaining units of work on its list, for example, by
	placing these units of work in a global work-item-disposal list
	to be taken by one of the remaining threads.
	(Why not just drain the thread's work-item list by executing
	each item?
	Because a given work item might generate more work items, so
	that the list could not be drained in a timely fashion.)

	\fi

	이 어플리케이션이 성능도 좋고 확장성도 좋아야 한다면, 좋은 락킹 설계가
	필요합니다.
	한가지 흔한 해결책은 쓰레드 제거 전체 프로세스를 위한 하나의 전역 락을
	(\co{G} 라 부릅시다) 개별 제거 오퍼레이션을 보호하는 잔 규모의 락과
	함께 사용하는 것입니다.

	이제, 제거되는 쓰레드는 자신의 리스트의 일을 폐기하기 전에 이어지는
	요청들을 받는 것을 분명히 거부해야 하는데, 그러지 않는다면 이 폐기 액션
	후에 추가적인 일이 도착할 수 있으며, 이는 이 폐기 액션이 효과적이지
	못하게 할 것이기 때문입니다.
	따라서 제거되는 쓰레드를 위한 간략화된 슈도코드는 다음과 같을 겁니다:

	\iffalse

	If the application is to perform and scale well, a good locking
	design is required.
	One common solution is to have a global lock (call it \co{G})
	protecting the entire
	process of departing (and perhaps other things as well),
	with finer-grained locks protecting the
	individual unraveling operations.

	Now, a departing thread must clearly refuse to accept further
	requests before disposing of the work on its list, because
	otherwise additional work might arrive after the disposal action,
	which would render that disposal action ineffective.
	So simplified pseudocode for a departing thread might be as follows:

	\fi

	\begin{enumerate}
	\item	\co{G} 락을 획득합니다.
	\item	통신을 보호하는 락을 잡습니다.
	\item	다른 쓰레드로부터의 더이상의 통신을 거부합니다.
	\item	통신을 보호하는 락을 해제합니다.
	\item	전역 일 항목 폐기 리스트를 보호하는 락을 잡습니다.
	\item	모든 보류 중인 일 항목을 전역 일 항목 폐기 리스트로
		이동시킵니다.
	\item	전역 일 항목 폐기 리스트를 보호하는 락을 해제합니다.
	\item	\co{G} 락을 해제합니다.

	\iffalse

	\item	Acquire lock \co{G}.
	\item	Acquire the lock guarding communications.
	\item	Refuse further communications from other threads.
	\item	Release the lock guarding communications.
	\item	Acquire the lock guarding the global work-item-disposal list.
	\item	Move all pending work items to the global
		work-item-disposal list.
	\item	Release the lock guarding the global work-item-disposal list.
	\item	Release lock \co{G}.

	\fi

	\end{enumerate}

	물론, 모든 미리 존재하던 일 항목을 기다려야 하는 쓰레드는 떠나는
	쓰레드를 신경써야 합니다.
	이를 자세히 알아보기 위해, 어떤 제거되는 쓰레드가 다른 쓰레드로부터의
	더이상의 통신을 거부한 직후에 모든 미리 존재한 일 항목을 기다리는
	쓰레드가 시작되었다고 생각해 봅시다.
	쓰레드들은 서로의 일 항목 리스트에 접근할 수 없는데 이 쓰레드는 이
	제거되는 쓰레드의 일 항목이 완료되기를 기다릴 수 있을까요?

	한가지 간단한 방법은 이 쓰레드가 \co{G} 를, 그리고 전역 일 항목 폐기
	리스트를 보호하는 락을 잡고, 이 일 항목들을 자신의 리스트로 옮기는
	것입니다.
	이 쓰레드는 이후에 두 락을 해제하고, 일 항목 중 하나를 자신의 리스트의
	끝에 위치시키고, 각 쓰레드의 리스트 (자신의 것 포함) 에 있는 일
	항목들이 모두 완료되기를 기다립니다.

	이 방법은 많은 경우에 잘 동작합니다만, 각 일 항목이 전역 일 항목 폐기
	리스트에서 가져와 지는 만큼 특별한 처리가 필요하다면, \co{G} 로의
	지나친 컨텐션이 초래될 수 있습니다.
	이 컨텐션을 방지하는 한가지 방법은 \co{G} 를 획득하고는 곧바로 해제하는
	것입니다.
	그러면 앞의 모든 일 항목이 완료되기를 기다리는 프로세스는 다음과 같이
	됩니다:

	\iffalse

	Of course, a thread that needs to wait for all pre-existing work
	items will need to take departing threads into account.
	To see this, suppose that this thread starts waiting for all
	pre-existing work items just after a departing thread has refused
	further communications from other threads.
	How can this thread wait for the departing thread's work items
	to complete, keeping in mind that threads are not allowed to
	access each others' lists of work items?

	One straightforward approach is for this thread to acquire \co{G}
	and then the lock guarding the global work-item-disposal list, then
	move the work items to its own list.
	The thread then release both locks,
	places a work item on the end of its own list,
	and then wait for all of the work items that it placed on each thread's
	list (including its own) to complete.

	This approach does work well in many cases, but if special
	processing is required for each work item as it is pulled in
	from the global work-item-disposal list, the result could be
	excessive contention on \co{G}.
	One way to avoid that contention is to acquire \co{G} and then
	immediately release it.
	Then the process of waiting for all prior work items look
	something like the following:

	\fi

	\begin{enumerate}
	\item	한 전역 카운터를 1로 값 설정하고 한 조건 변수를 0으로 초기화
		합니다.
	\item	모든 쓰레드에게 그것들이 원자적으로 이 전역 카운터의 값을
		증가시키도록, 그리고 나서 일 항목을 넣도록 메세지를 보냅니다.
		이 일 항목은 원자적으로 이 전역 카운터의 값을 감소시키며, 그
		결과 그 값이 0이 되면, 이는 한 조건 변수의 값을 1로 설정합니다.
	\item	모든 현재 제거되는 중인 쓰레드가 제거되기를 끝낼 때까지
		기다리게끔 \co{G} 를 획득합니다.
		한번에 하나의 쓰레드만 제거될 것이므로, 모든 남아있는 쓰레드는
		이미 앞의 단계에서 보낸 메세지를 받았을 겁니다.
	\item	\co{G} 를 해제함.
	\item	전역 일 항목 폐기 리스트를 보호하는 락을 획득합니다.
	\item	모든 일 항목을 전역 일 항목 폐기 리스트에서 이 쓰레드의
		리스트로 이동시키고, 그것들을 필요한 대로 처리합니다.
	\item	전역 일 항목 폐기 리스트를 보호하는 락을 해제합니다.
	\item	추가적인 일 항목을 이 쓰레드의 리스트에 넣습니다.
		(앞에서와 마찬가지로, 이 일 항목은 원자적으로 전역 카운터의
		값을 감소시키고, 그 결과 값이 0이 되면 조건 변수를 1로
		설정합니다.)
	\item	이 조건 변수가 값 1이 되기를 기다립니다.

	\iffalse

	\item	Set a global counter to one and initialize a condition
		variable to zero.
	\item	Send a message to all threads to cause them to atomically
		increment the global counter, and then to enqueue a
		work item.
		The work item will atomically decrement the global
		counter, and if the result is zero, it will set a
		condition variable to one.
	\item	Acquire \co{G}, which will wait on any currently departing
		thread to finish departing.
		Because only one thread may depart at a time, all the
		remaining threads will have already received the message
		sent in the preceding step.
	\item	Release \co{G}.
	\item	Acquire the lock guarding the global work-item-disposal list.
	\item	Move all work items from the global work-item-disposal list
		to this thread's list, processing them as needed along the way.
	\item	Release the lock guarding the global work-item-disposal list.
	\item	Enqueue an additional work item onto this thread's list.
		(As before, this work item will atomically decrement
		the global counter, and if the result is zero, it will
		set a condition variable to one.)
	\item	Wait for the condition variable to take on the value one.

	\fi

	\end{enumerate}

	이 프로세스가 완료되면, 모든 앞서서부터 존재한 일 항목들은 완료되었을
	것이 보장됩니다.
	텅 빈 크리티컬 섹션은 락킹을 메세징은 물론이고 데이터의 보호를 위해서도
	사용됩니다.

	\iffalse

	Once this procedure completes, all pre-existing work items are
	guaranteed to have completed.
	The empty critical sections are using locking for messaging as
	well as for protection of data.

	\fi

}\QuickQuizEnd

\QuickQuizLabel{\QlockingQemptycriticalsection}

무조건적으로 배타적 락을 획득하는 것이 두가지 영향을 끼침을 알아두는 것이
중요합니다:
(1)~해당 락을 앞서 잡은 모든 쓰레드가 그것을 놓기를 기다림, 그리고
(2)~이 락이 해제되기 전까지는 모든 이 락을 잡으려는 시도가 블록됨.
그 결과, 락 획득 시점에서, 모든 동시의 해당 락에 대한 획득은 앞서 락을 잡고
있던 쓰레드와 뒤따라 락을 잡는 쓰레드로 분리됩니다.
다른 종류의 배타적 락은 다른 분리 전략을
사용하는데~\cite{BjoernBrandenburgPhD,Guerraoui:2019:LPA:3319851.3301501}, 예를
들면:

\iffalse

It is important to note that unconditionally acquiring an exclusive lock
has two effects:
(1)~Waiting for all prior holders of that lock to release it, and
(2)~Blocking any other acquisition attempts until the lock is released.
As a result, at lock acquisition time, any concurrent acquisitions of
that lock must be partitioned into prior holders and subsequent
holders.
Different types of exclusive locks use different partitioning
strategies~\cite{BjoernBrandenburgPhD,Guerraoui:2019:LPA:3319851.3301501},
for example:

\fi

\begin{enumerate}
\item	락을 먼저 획득하고자 한 쓰레드가 먼저 락을 획득하는 엄격한 FIFO.
\item	충분히 먼저 락을 획득하고자 한 쓰레드가 먼저 락을 획득하는 대략적 FIFO.
\item	비슷한 시점에 락을 획득하려 시도하는 어떤 낮은 우선순위 쓰레드보다 더
	높은 우선순위를 갖고 일찍 락을 획득하고자 한 쓰레드가 먼저 락을
	획득하게 되는, 하지만 같은 우선순위의 쓰레드 사이에는 약간의 FIFO 순서
	규칙이 적용되는 우선순위 단계 기반 FIFO.
\item	새로운 락 획득 쓰레드는 타이밍에 관계 없이 해당 락을 획득하려는 쓰레드
	중 하나가 무작위로 뽑혀지는 무작위 방식.
\item	특정 락 획득 시도가 결코 락을 획득하지 못하게 될수도 있는 unfair 방식
	(see \cref{sec:locking:Unfairness}).

\iffalse

\item	Strict FIFO, with acquisitions starting earlier acquiring
	the lock earlier.
\item	Approximate FIFO, with acquisitions starting sufficiently
	earlier acquiring the lock earlier.
\item	FIFO within priority level, with higher-priority threads
	acquiring the lock earlier than any lower-priority threads
	attempting to acquire the lock at about the same time, but so
	that some FIFO ordering applies for threads of the same priority.
\item	Random, so that the new lock holder is chosen randomly from
	all threads attempting acquisition, regardless of timing.
\item	
	Unfair, so that a given acquisition might never acquire the lock
	(see \cref{sec:locking:Unfairness}).

\fi

\end{enumerate}

불행히도, 더 강력한 보장을 제공하는 락킹 구현은 일반적으로 더 높은 오버헤드를
일으켜서, 제품들에서 사용되는 다양한 락킹 구현의 모티베이션이 되었습니다.
예를 들어, 리얼타임 시스템은 종종 다른 것들보다도 우선순위 단계를 갖는
어느정도의 FIFO 순서 규칙을 필요로 하는 반면
(\cref{sec:advsync:Event-Driven Real-Time Support} 를 참고하세요),
높은 컨텐션을 갖는 비 리얼타임 시스템은 starvation 을 피하기 충분한
순서규칙만을 필요로 할 수 있으며, 마지막으로, 컨텐션을 방지하게끔 설계된 비
리얼타임 시스템은 fairness 자체가 필요하지 않을 수도 있습니다.

\iffalse

Unfortunately, locking implementations with stronger guarantees
typically incur higher overhead, motivating the wide variety of locking
implementations in production use.
For example, real-time systems often require some degree of FIFO
ordering within priority level, and much else besides
(see \cref{sec:advsync:Event-Driven Real-Time Support}),
while non-realtime systems subject to high contention might require
only enough ordering to avoid starvation, and finally, non-realtime
systems designed to avoid contention might not need fairness at all.

\fi

\subsection{Reader-Writer Locks}
\label{sec:locking:Reader-Writer Locks}

Reader-writer locks~\cite{Courtois71}
permit any number of readers to hold the lock
concurrently on the one hand or a single writer to hold the lock
on the other.
In theory, then, reader-writer locks should allow excellent scalability
for data that is read often and written rarely.
In practice, the scalability will depend on the reader-writer lock
implementation.

The classic reader-writer lock implementation involves a set of
counters and flags that are manipulated atomically.
This type of implementation suffers from the same problem as does
exclusive locking for short critical sections: The overhead of acquiring
and releasing the lock
is about two orders of magnitude greater than the overhead
of a simple instruction.
Of course, if the critical section is long enough, the overhead of
acquiring and releasing the lock becomes negligible.
However, because only
one thread at a time can be manipulating the lock, the required
critical-section size increases with the number of CPUs.

It is possible to design a reader-writer lock that is much more
favorable to readers through use of per-thread exclusive
locks~\cite{WilsonCHsieh92a}.
To read, a thread acquires only its own lock.
To write, a thread acquires all locks.
In the absence of writers, each reader incurs only atomic-instruction
and memory-barrier overhead, with no cache misses, which is quite
good for a locking primitive.
Unfortunately, writers must incur cache misses as well as atomic-instruction
and memory-barrier overhead---multiplied by the number of threads.

In short, reader-writer locks can be quite useful in a number of
situations, but each type of implementation does have its drawbacks.
The canonical use case for reader-writer locking involves very long
read-side critical sections, preferably measured in hundreds of microseconds
or even milliseconds.

As with exclusive locks, a reader-writer lock acquisition cannot complete
until all prior conflicting holders of that lock have released it.
If a lock is read-held, then read acquisitions can complete immediately,
but write acquisitions must wait until there are no longer any readers
holding the lock.
If a lock is write-held, then all acquisitions must wait until the writer
releases the lock.
Again as with exclusive locks, different reader-writer lock implementations
provide different degrees of FIFO ordering to readers on the one hand
and to writers on the other.

But suppose a large number of readers hold the lock and a writer is waiting
to acquire the lock.
Should readers be allowed to continue to acquire the lock, possibly
starving the writer?
Similarly, suppose that a writer holds the lock and that a large number
of both readers and writers are waiting to acquire the lock.
When the current writer release the lock, should it be given to a reader
or to another writer?
If it is given to a reader, how many readers should be allowed to acquire
the lock before the next writer is permitted to do so?

There are many possible answers to these questions, with different
levels of complexity, overhead, and fairness.
Different implementations might have different costs, for example,
some types of reader-writer locks incur extremely large latencies
when switching from read-holder to write-holder mode.
Here are a few possible approaches:

\begin{enumerate}
\item	
	Reader-preference implementations unconditionally favor readers
	over writers, possibly allowing write acquisitions to be
	indefinitely blocked.
\item	Batch-fair implementations ensure that when both readers and writers
	are acquiring the lock, both have reasonable access via batching.
	For example, the lock might admit five readers per CPU, then two
	writers, then five more readers per CPU, and so on.
\item	Writer-preference implementations unconditionally favor
	writers over readers, possibly allowing read acquisitions to be
	indefinitely blocked.
\end{enumerate}

Of course, these distinctions matter only under conditions of high
lock contention.

Please keep the waiting/blocking dual nature of locks firmly in mind.
This will be revisited in \cref{chp:Deferred Processing}'s discussion
of scalable high-performance special-purpose alternatives to locking.

\subsection{Beyond Reader-Writer Locks}
\label{sec:locking:Beyond Reader-Writer Locks}

\begin{table}
\renewcommand*{\arraystretch}{1.2}
\newcommand{\x}{\textcolor{gray!20}{\rule{7pt}{7pt}}}
\newcommand{\rothead}[1]{\begin{picture}(6,65)(0,0)\rotatebox{90}{#1}\end{picture}}
\small
\centering
\begin{tabular}{lcccccc}
	\toprule
	& \rothead{Null (Not Held)}
	& \rothead{Concurrent Read}
	& \rothead{Concurrent Write}
	& \rothead{Protected Read}
	& \rothead{Protected Write}
	& \rothead{Exclusive}
	\\
%				  NL   CR   CW     PR   PW   EX
	\cmidrule(r){1-1} \cmidrule{2-7}
	Null (Not Held)		& \x & \x & \x   & \x & \x & \x \\
	Concurrent Read		& \x & \x & \x   & \x & \x &  X \\
	Concurrent Write	& \x & \x & \x   &  X &  X &  X \\
	Protected Read		& \x & \x &  X   & \x &  X &  X \\
	Protected Write		& \x & \x &  X   &  X &  X &  X \\
	Exclusive		& \x &  X &  X   &  X &  X &  X \\
	\bottomrule
\end{tabular}
\caption{VAX/VMS Distributed Lock Manager Policy}
\label{tab:locking:VAX/VMS Distributed Lock Manager Policy}
\end{table}

Reader-writer locks and exclusive locks differ in their admission
policy: exclusive locks allow at most one holder, while reader-writer
locks permit an arbitrary number of read-holders (but only one write-holder).
There is a very large number of possible admission policies, one of
which is that of the VAX/VMS distributed lock
manager (DLM)~\cite{Snaman87}, which is shown in
Table~\ref{tab:locking:VAX/VMS Distributed Lock Manager Policy}.
Blank cells indicate compatible modes, while cells containing ``X''
indicate incompatible modes.

The VAX/VMS DLM uses six modes.
For purposes of comparison, exclusive
locks use two modes (not held and held), while reader-writer locks
use three modes (not held, read held, and write held).

The first mode is null, or not held.
This mode is compatible with all other modes, which is to be expected:
If a thread is not holding a lock, it should not prevent any
other thread from acquiring that lock.

The second mode is concurrent read, which is compatible with every other
mode except for exclusive.
The concurrent-read mode might be used to accumulate approximate
statistics on a data structure, while permitting updates to proceed
concurrently.

The third mode is concurrent write, which is compatible with null,
concurrent read, and concurrent write.
The concurrent-write mode might be used to update approximate statistics,
while still permitting reads and concurrent updates to proceed
concurrently.

The fourth mode is protected read, which is compatible with null,
concurrent read, and protected read.
The protected-read mode might be used to obtain a consistent snapshot
of the data structure, while permitting reads but not updates to
proceed concurrently.

The fifth mode is protected write, which is compatible with null and
concurrent read.
The protected-write mode might be used to carry out updates to a data
structure that could interfere with protected readers but which could
be tolerated by concurrent readers.

The sixth and final mode is exclusive, which is compatible only with null.
The exclusive mode is used when it is necessary to exclude all other accesses.

It is interesting to note that exclusive locks and reader-writer locks
can be emulated by the VAX/VMS DLM\@.
Exclusive locks would use only the null and exclusive modes, while
reader-writer locks might use the null, protected-read, and
protected-write modes.

\QuickQuiz{
	Is there any other way for the VAX/VMS DLM to emulate
	a reader-writer lock?
}\QuickQuizAnswer{
	There are in fact several.
	One way would be to use the null, protected-read, and exclusive
	modes.
	Another way would be to use the null, protected-read, and
	concurrent-write modes.
	A third way would be to use the null, concurrent-read, and
	exclusive modes.
}\QuickQuizEnd

Although the VAX/VMS DLM policy has seen widespread production use
for distributed databases, it does not appear to be used much in
shared-memory applications.
One possible reason for this is that the greater communication overheads
of distributed databases can hide the greater overhead of the
VAX/VMS DLM's more-complex admission policy.

Nevertheless, the VAX/VMS DLM is an interesting illustration of just
how flexible the concepts behind locking can be.
It also serves as a very simple introduction to the locking schemes
used by modern DBMSes, which can have more than thirty locking modes,
compared to VAX/VMS's six.

\subsection{Scoped Locking}
\label{sec:locking:Scoped Locking}

The locking primitives discussed thus far require explicit acquisition and
release primitives, for example, \co{spin_lock()} and \co{spin_unlock()},
respectively.
Another approach is to use the object-oriented ``resource allocation
is initialization'' (RAII) pattern~\cite{MargaretAEllis1990Cplusplus}.\footnote{
	Though more clearly expressed at
	\url{https://www.stroustrup.com/bs_faq2.html\#finally}.}
This pattern is often applied to auto variables in languages like C++,
where the corresponding \emph{constructor} is invoked upon entry to
the object's scope, and the corresponding \emph{destructor} is invoked
upon exit from that scope.
This can be applied to locking by having the constructor acquire the
lock and the destructor free it.

This approach can be quite useful, in fact in 1990 I was convinced that it
was the only type of locking that was needed.\footnote{
	My later work with parallelism at Sequent Computer Systems very
	quickly disabused me of this misguided notion.}
One very nice property of RAII locking is that you don't need to carefully
release the lock on each and every code path that exits that scope,
a property that can eliminate a troublesome set of bugs.

However, RAII locking also has a dark side.
RAII makes it quite difficult to encapsulate lock acquisition and release,
for example, in iterators.
In many iterator implementations, you would like to acquire the lock in
the iterator's ``start'' function and release it in the iterator's ``stop''
function.
RAII locking instead requires that the lock acquisition and release take
place in the same level of scoping, making such encapsulation difficult or
even impossible.

Strict RAII locking also prohibits overlapping critical sections, due
to the fact that scopes must nest.
This prohibition makes it difficult or impossible to express a number of
useful constructs, for example, locking trees
that mediate between multiple concurrent attempts to assert an event.
Of an arbitrarily large group of concurrent attempts, only one need succeed,
and the best strategy for the remaining attempts is for them to fail as
quickly and painlessly as possible.
Otherwise, lock contention becomes pathological on large systems
(where ``large'' is many hundreds of CPUs).
Therefore, C++17~\cite{RichardSmith2019N4800} has escapes from strict RAII
in its \co{unique_lock} class, which allows the scope of the critical
section to be controlled to roughly the same extent as can be achieved
with explicit lock acquisition and release primitives.

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{locking/rnplock}}
\caption{Locking Hierarchy}
\label{fig:locking:Locking Hierarchy}
\end{figure}

Example strict-RAII-unfriendly data structures from Linux-kernel RCU
are shown in
Figure~\ref{fig:locking:Locking Hierarchy}.
Here, each CPU is assigned a leaf \co{rcu_node} structure, and each
\co{rcu_node} structure has a pointer to its parent (named, oddly
enough, \co{->parent}), up to the root \co{rcu_node} structure,
which has a \co{NULL} \co{->parent} pointer.
The number of child \co{rcu_node} structures per parent can vary,
but is typically 32 or 64.
Each \co{rcu_node} structure also contains a lock named \co{->fqslock}.

The general approach is a \emph{tournament}, where
a given CPU conditionally acquires its
leaf \co{rcu_node} structure's \co{->fqslock}, and, if successful,
attempt to acquire that of the parent, then release that of the child.
In addition, at each level, the CPU checks a global \co{gp_flags}
variable, and if this variable indicates that some other CPU has
asserted the event, the first CPU drops out of the competition.
This acquire-then-release sequence continues until either the
\co{gp_flags} variable indicates that someone else won the tournament,
one of the attempts to acquire an \co{->fqslock} fails, or
the root \co{rcu_node} structure's \co{->fqslock} has been acquired.
If the root \co{rcu_node} structure's \co{->fqslock} is acquired,
a function named \co{do_force_quiescent_state()} is invoked.

\begin{listing}[tbp]
\begin{fcvlabel}[ln:locking:Conditional Locking to Reduce Contention]
\begin{VerbatimL}[commandchars=\\\[\]]
void force_quiescent_state(struct rcu_node *rnp_leaf)
{
	int ret;
	struct rcu_node *rnp = rnp_leaf;
	struct rcu_node *rnp_old = NULL;

	for (; rnp != NULL; rnp = rnp->parent) {	\lnlbl[loop:b]
		ret = (READ_ONCE(gp_flags)) ||		\lnlbl[flag_set]
		       !raw_spin_trylock(&rnp->fqslock);\lnlbl[trylock]
		if (rnp_old != NULL)			\lnlbl[non_NULL]
			raw_spin_unlock(&rnp_old->fqslock);\lnlbl[rel1]
		if (ret)				\lnlbl[giveup]
			return;				\lnlbl[return]
		rnp_old = rnp;				\lnlbl[save]
	}						\lnlbl[loop:e]
	if (!READ_ONCE(gp_flags)) {			\lnlbl[flag_not_set]
		WRITE_ONCE(gp_flags, 1);		\lnlbl[set_flag]
		do_force_quiescent_state();		\lnlbl[invoke]
		WRITE_ONCE(gp_flags, 0);		\lnlbl[clr_flag]
	}
	raw_spin_unlock(&rnp_old->fqslock);		\lnlbl[rel2]
}
\end{VerbatimL}
\end{fcvlabel}
\caption{Conditional Locking to Reduce Contention}
\label{lst:locking:Conditional Locking to Reduce Contention}
\end{listing}

Simplified code to implement this is shown in
Listing~\ref{lst:locking:Conditional Locking to Reduce Contention}.
The purpose of this function is to mediate between CPUs who have concurrently
detected a need to invoke the \co{do_force_quiescent_state()} function.
At any given time, it only makes sense for one instance of
\co{do_force_quiescent_state()} to be active, so if there are multiple
concurrent callers, we need at most one of them to actually invoke
\co{do_force_quiescent_state()}, and we need the rest to (as quickly and
painlessly as possible) give up and leave.

\begin{fcvref}[ln:locking:Conditional Locking to Reduce Contention]
To this end, each pass through the loop spanning \clnrefrange{loop:b}{loop:e} attempts
to advance up one level in the \co{rcu_node} hierarchy.
If the \co{gp_flags} variable is already set (line~\lnref{flag_set}) or if the attempt
to acquire the current \co{rcu_node} structure's \co{->fqslock} is
unsuccessful (line~\lnref{trylock}), then local variable \co{ret} is set to 1.
If line~\lnref{non_NULL} sees that local variable \co{rnp_old} is non-\co{NULL},
meaning that we hold \co{rnp_old}'s \co{->fqs_lock},
line~\lnref{rel1} releases this lock (but only after the attempt has been made
to acquire the parent \co{rcu_node} structure's \co{->fqslock}).
If line~\lnref{giveup} sees that either line~\lnref{flag_set} or~\lnref{trylock}
saw a reason to give up,
line~\lnref{return} returns to the caller.
Otherwise, we must have acquired the current \co{rcu_node} structure's
\co{->fqslock}, so line~\lnref{save} saves a pointer to this structure in local
variable \co{rnp_old} in preparation for the next pass through the loop.

If control reaches line~\lnref{flag_not_set}, we won the tournament, and now holds the
root \co{rcu_node} structure's \co{->fqslock}.
If line~\lnref{flag_not_set} still sees that the global variable \co{gp_flags} is zero,
line~\lnref{set_flag} sets \co{gp_flags} to one, line~\lnref{invoke} invokes
\co{do_force_quiescent_state()},
and line~\lnref{clr_flag} resets \co{gp_flags} back to zero.
Either way, line~\lnref{rel2} releases the root \co{rcu_node} structure's
\co{->fqslock}.
\end{fcvref}

\QuickQuizSeries{%
\QuickQuizB{
	The code in
	Listing~\ref{lst:locking:Conditional Locking to Reduce Contention}
	is ridiculously complicated!
	Why not conditionally acquire a single global lock?
}\QuickQuizAnswerB{
	Conditionally acquiring a single global lock does work very well,
	but only for relatively small numbers of CPUs.
	To see why it is problematic in systems with many hundreds of
	CPUs, look at
	Figure~\ref{fig:count:Atomic Increment Scalability on x86}.
}\QuickQuizEndB
%
\QuickQuizE{
	\begin{fcvref}[ln:locking:Conditional Locking to Reduce Contention]
	Wait a minute!
	If we ``win'' the tournament on line~\lnref{flag_not_set} of
	Listing~\ref{lst:locking:Conditional Locking to Reduce Contention},
	we get to do all the work of \co{do_force_quiescent_state()}.
	Exactly how is that a win, really?
        \end{fcvref}
}\QuickQuizAnswerE{
	How indeed?
	This just shows that in concurrency, just as in life, one
	should take care to learn exactly what winning entails before
	playing the game.
}\QuickQuizEndE
}

This function illustrates the not-uncommon pattern of hierarchical
locking.
This pattern is difficult to implement using strict RAII locking,\footnote{
	Which is why many RAII locking implementations provide a way
	to leak the lock out of the scope that it was acquired and into
	the scope in which it is to be released.
	However, some object must mediate the scope leaking, which can
	add complexity compared to non-RAII explicit locking primitives.}
just like the iterator encapsulation noted earlier, and so explicit
lock/unlock primitives (or C++17-style \co{unique_lock} escapes) will
be required for the foreseeable future.

\section{Locking Implementation Issues}
\label{sec:locking:Locking Implementation Issues}
%
\epigraph{When you translate a dream into reality, it's never a full
	  implementation.  It is easier to dream than to do.}
	 {\emph{Shai Agassi}}

Developers are almost always best-served by using whatever locking
primitives are provided by the system, for example, the POSIX
pthread mutex locks~\cite{OpenGroup1997pthreads,Butenhof1997pthreads}.
Nevertheless, studying sample implementations can be helpful,
as can considering the challenges posed by extreme workloads and
environments.

\subsection{Sample Exclusive-Locking Implementation Based on Atomic Exchange}
\label{sec:locking:Sample Exclusive-Locking Implementation Based on Atomic Exchange}

\begin{fcvref}[ln:locking:xchglock:lock_unlock]
This section reviews the implementation shown in
listing~\ref{lst:locking:Sample Lock Based on Atomic Exchange}.
The data structure for this lock is just an \co{int}, as shown on
line~\lnref{typedef}, but could be any integral type.
The initial value of this lock is zero, meaning ``unlocked'',
as shown on line~\lnref{initval}.
\end{fcvref}

\begin{listing}[tbp]
\input{CodeSamples/locking/xchglock@lock_unlock.fcv}
\caption{Sample Lock Based on Atomic Exchange}
\label{lst:locking:Sample Lock Based on Atomic Exchange}
\end{listing}

\QuickQuiz{
	\begin{fcvref}[ln:locking:xchglock:lock_unlock]
	Why not rely on the C language's default initialization of
	zero instead of using the explicit initializer shown on
	line~\lnref{initval} of
	Listing~\ref{lst:locking:Sample Lock Based on Atomic Exchange}?
	\end{fcvref}
}\QuickQuizAnswer{
	Because this default initialization does not apply to locks
	allocated as auto variables within the scope of a function.
}\QuickQuizEnd

\begin{fcvref}[ln:locking:xchglock:lock_unlock:lock]
Lock acquisition is carried out by the \co{xchg_lock()} function
shown on \clnrefrange{b}{e}.
This function uses a nested loop, with the outer loop repeatedly
atomically exchanging the value of the lock with the value one
(meaning ``locked'').
If the old value was already the value one (in other words, someone
else already holds the lock), then the inner loop (\clnrefrange{inner:b}{inner:e})
spins until the lock is available, at which point the outer loop
makes another attempt to acquire the lock.
\end{fcvref}

\QuickQuiz{
	\begin{fcvref}[ln:locking:xchglock:lock_unlock:lock]
	Why bother with the inner loop on \clnrefrange{inner:b}{inner:e} of
	Listing~\ref{lst:locking:Sample Lock Based on Atomic Exchange}?
	Why not simply repeatedly do the atomic exchange operation
	on line~\lnref{atmxchg}?
	\end{fcvref}
}\QuickQuizAnswer{
	\begin{fcvref}[ln:locking:xchglock:lock_unlock:lock]
	Suppose that the lock is held and that several threads
	are attempting to acquire the lock.
	In this situation, if these threads all loop on the atomic
	exchange operation, they will ping-pong the cache line
	containing the lock among themselves, imposing load
	on the interconnect.
	In contrast, if these threads are spinning in the inner
	loop on \clnrefrange{inner:b}{inner:e},
        they will each spin within their own
	caches, placing negligible load on the interconnect.
	\end{fcvref}
}\QuickQuizEnd

\begin{fcvref}[ln:locking:xchglock:lock_unlock:unlock]
Lock release is carried out by the \co{xchg_unlock()} function
shown on \clnrefrange{b}{e}.
Line~\lnref{atmxchg} atomically exchanges the value zero (``unlocked'') into
the lock, thus marking it as having been released.
\end{fcvref}

\QuickQuiz{
	\begin{fcvref}[ln:locking:xchglock:lock_unlock:unlock]
	Why not simply store zero into the lock word on line~\lnref{atmxchg} of
	Listing~\ref{lst:locking:Sample Lock Based on Atomic Exchange}?
	\end{fcvref}
}\QuickQuizAnswer{
	This can be a legitimate implementation, but only if
	this store is preceded by a memory barrier and makes use
	of \co{WRITE_ONCE()}.
	The memory barrier is not required when the \co{xchg()}
	operation is used because this operation implies a
	full memory barrier due to the fact that it returns
	a value.
}\QuickQuizEnd

This lock is a simple example of a test-and-set lock~\cite{Segall84},
but very similar
mechanisms have been used extensively as pure spinlocks in production.

\subsection{Other Exclusive-Locking Implementations}
\label{sec:locking:Other Exclusive-Locking Implementations}

There are a great many other possible implementations of locking based
on atomic instructions, many of which are reviewed in the classic paper
by Mellor-Crummey and Scott~\cite{MellorCrummey91a}.
These implementations represent different points in a multi-dimensional
design tradeoff~\cite{Guerraoui:2019:LPA:3319851.3301501,HugoGuirouxPhD,McKenney96a}.
For example,
the atomic-exchange-based test-and-set lock presented in the previous
section works well when contention is low and has the advantage
of small memory footprint.
It avoids giving the lock to threads that cannot use it, but as
a result can suffer from unfairness or even starvation at high
contention levels.

In contrast, ticket lock~\cite{MellorCrummey91a}, which was once used
in the Linux kernel, avoids unfairness at high contention levels.
However, as a consequence of its strict FIFO discipline, it can grant
the lock to a thread that is currently unable to use it, perhaps due
to that thread being preempted or interrupted.
On the other hand, it is important to avoid getting too worried about the
possibility of preemption and interruption.
After all, in many cases, this preemption and interruption could just
as well happen just after the lock was
acquired.\footnote{
	Besides, the best way of handling high lock contention is to avoid
	it in the first place!
	There are nevertheless some situations where high lock contention
	is the lesser of the available evils, and in any case, studying
	schemes that deal with high levels of contention is a good mental
	exercise.}

All locking implementations where waiters spin on a single memory
location, including both test-and-set locks and ticket locks,
suffer from performance problems at high contention levels.
The problem is that the thread releasing the lock must update the
value of the corresponding memory location.
At low contention, this is not a problem: The corresponding cache line
is very likely still local to and writeable by the thread holding
the lock.
In contrast, at high levels of contention, each thread attempting to
acquire the lock will have a read-only copy of the cache line, and
the lock holder will need to invalidate all such copies before it
can carry out the update that releases the lock.
In general, the more CPUs and threads there are, the greater the
overhead incurred when releasing the lock under conditions of
high contention.

This negative scalability has motivated a number of different
queued-lock
implementations~\cite{Anderson90,Graunke90,MellorCrummey91a,Wisniewski94,Craig93,Magnusson94,Takada93},
some of which are used in recent versions of the Linux
kernel~\cite{JonathanCorbet2014qspinlocks}.
Queued locks avoid high cache-invalidation overhead by assigning each
thread a queue element.
These queue elements are linked together into a queue that governs the
order that the lock will be granted to the waiting threads.
The key point is that each thread spins on its own queue element,
so that the lock holder need only invalidate the first element from
the next thread's CPU's cache.
This arrangement greatly reduces the overhead of lock handoff at high
levels of contention.

More recent queued-lock implementations also take the system's architecture
into account, preferentially granting locks locally, while also taking
steps to avoid
starvation~\cite{McKenney02e,radovic03hierarchical,radovic02efficient,BenJackson02,McKenney02d}.
Many of these can be thought of as analogous to the elevator algorithms
traditionally used in scheduling disk I/O.

Unfortunately, the same scheduling logic that improves the efficiency
of queued locks at high contention also increases their overhead at
low contention.
Beng-Hong Lim and Anant Agarwal therefore combined a simple test-and-set
lock with a queued lock, using the test-and-set lock at low levels of
contention and switching to the queued lock at high levels of
contention~\cite{BengHongLim94}, thus getting low overhead at low levels
of contention and getting fairness and high throughput at high levels
of contention.
Browning et al.\ took a similar approach, but avoided the use of a separate
flag, so that the test-and-set fast path uses the same sequence of
instructions that would be used in a simple test-and-set
lock~\cite{LukeBrowning2005SimpleLockNUMAAware}.
This approach has been used in production.

Another issue that arises at high levels of contention is when the
lock holder is delayed, especially when the delay is due to
preemption, which can result in \emph{priority inversion},
where a low-priority thread holds a lock, but is preempted
by a medium priority CPU-bound thread, which results in
a high-priority process blocking while attempting to acquire the
lock.
The result is that the CPU-bound medium-priority process is preventing the
high-priority process from running.
One solution is \emph{priority inheritance}~\cite{Lampson1980Mesa},
which has been widely used for real-time
computing~\cite{LuiSha1990PriorityInheritance,JonathanCorbet2006PriorityInheritance},
despite some lingering controversy over this
practice~\cite{Yodaiken2004FSM,DougLocke2002a}.

Another way to avoid priority inversion is to prevent preemption
while a lock is held.
Because preventing preemption while locks are held also improves throughput,
most proprietary UNIX kernels offer some form of scheduler-conscious
synchronization mechanism~\cite{Kontothanassis97a},
largely due to the efforts of a certain sizable database vendor.
These mechanisms usually take the form of a hint that preemption
should be avoided in a given region of code, with this hint typically
being placed in a machine register.
These hints frequently take the form of a bit set in a particular
machine register, which enables extremely low per-lock-acquisition overhead
for these mechanisms.
In contrast, Linux avoids these hints, instead getting
similar results from a mechanism called
\emph{futexes}~\cite{HubertusFrancke2002Futex,IngoMolnar2006RobustFutexes,StevenRostedt2006piFutexes,UlrichDrepper2011Futexes}.

Interestingly enough, atomic instructions are not strictly needed to
implement locks~\cite{Dijkstra65a,Lamport74a}.
An excellent exposition of the issues surrounding locking implementations
based on simple loads and stores may be found in Herlihy's and
Shavit's textbook~\cite{HerlihyShavit2008Textbook,HerlihyShavit2020Textbook}.
The main point echoed here is that such implementations currently
have little practical application, although a careful study of
them can be both entertaining and enlightening.
Nevertheless, with one exception described below, such study is left
as an exercise for the reader.

Gamsa et al.~\cite[Section 5.3]{Gamsa99} describe a token-based
mechanism in which a token circulates among the CPUs.
When the token reaches a given CPU, it has exclusive
access to anything protected by that token.
There are any number of schemes that may be used to implement
the token-based mechanism, for example:

\begin{enumerate}
\item	Maintain a per-CPU flag, which is initially
	zero for all but one CPU\@.
	When a CPU's flag is non-zero, it holds the token.
	When it finishes with the token, it zeroes its flag and
	sets the flag of the next CPU to one (or to any other
	non-zero value).
\item	Maintain a per-CPU counter, which is initially set
	to the corresponding CPU's number, which we assume
	to range from zero to $N-1$, where $N$ is the number
	of CPUs in the system.
	When a CPU's counter is greater than that of the next
	CPU (taking counter wrap into account), the first CPU holds the token.
	When it is finished with the token, it sets the next
	CPU's counter to a value one greater than its own counter.
\end{enumerate}

\QuickQuizSeries{%
\QuickQuizB{
	How can you tell if one counter is greater than another,
	while accounting for counter wrap?
}\QuickQuizAnswerB{
	In the C language, the following macro correctly handles this:

\begin{VerbatimU}
#define ULONG_CMP_LT(a, b) \
        (ULONG_MAX / 2 < (a) - (b))
\end{VerbatimU}

	Although it is tempting to simply subtract two signed integers,
	this should be avoided because signed overflow is undefined
	in the C language.
	For example, if the compiler knows that one of the values is
	positive and the other negative, it is within its rights to
	simply assume that the positive number is greater than the
	negative number, even though subtracting the negative number
	from the positive number might well result in overflow and
	thus a negative number.

	How could the compiler know the signs of the two numbers?
	It might be able to deduce it based on prior assignments
	and comparisons.
	In this case, if the per-CPU counters were signed, the compiler
	could deduce that they were always increasing in value, and
	then might assume that they would never go negative.
	This assumption could well lead the compiler to generate
	unfortunate code~\cite{PaulEMcKenney2012SignedOverflow,JohnRegehr2010UndefinedBehavior}.
}\QuickQuizEndB
%
\QuickQuizE{
	Which is better, the counter approach or the flag approach?
}\QuickQuizAnswerE{
	The flag approach will normally suffer fewer cache misses,
	but a better answer is to try both and see which works best
	for your particular workload.
}\QuickQuizEndE
}

This lock is unusual in that a given CPU cannot necessarily acquire it
immediately, even if no other CPU is using it at the moment.
Instead, the CPU must wait until the token comes around to it.
This is useful in cases where CPUs need periodic access to the critical
section, but can tolerate variances in token-circulation rate.
Gamsa et al.~\cite{Gamsa99} used it to implement a variant of
read-copy update (see Section~\ref{sec:defer:Read-Copy Update (RCU)}),
but it could also be used to protect periodic per-CPU operations such
as flushing per-CPU caches used by memory allocators~\cite{McKenney93},
garbage-collecting per-CPU data structures, or flushing per-CPU
data to shared storage (or to mass storage, for that matter).

The Linux kernel now uses queued spinlocks~\cite{JonathanCorbet2014qspinlocks},
but because of the complexity of implementations that provide good
performance across the range of contention levels, the path has not
always been smooth~\cite{CatalinMarinas2018qspinlockTLA,WillDeacon2018qspinlock}.
As increasing numbers of people gain familiarity with parallel hardware
and parallelize increasing amounts of code, we can continue to expect more
special-purpose locking primitives to appear, see for example Guerraoui et
al.~\cite{Guerraoui:2019:LPA:3319851.3301501,HugoGuirouxPhD}.
Nevertheless, you should carefully consider this important safety tip:
Use the standard synchronization primitives whenever humanly possible.
The big advantage of the standard synchronization primitives over
roll-your-own efforts is that the standard primitives are typically
\emph{much} less bug-prone.\footnote{
	And yes, I have done at least my share of roll-your-own
	synchronization primitives.
	However, you will notice that my hair is much greyer than
	it was before I started doing that sort of work.
	Coincidence?
	Maybe.
	But are you \emph{really} willing to risk your own hair turning
	prematurely grey?}

\input{locking/locking-existence}

\section{Locking: Hero or Villain?}
\label{sec:locking:Locking: Hero or Villain?}
%
\epigraph{You either die a hero or live long enough to become the villain.}
	 {\emph{Aaron Eckhart}}

As is often the case in real life, locking can be either hero or villain,
depending on how it is used and on the problem at hand.
In my experience, those writing whole applications are happy with
locking, those writing parallel libraries are less happy, and those
parallelizing existing sequential libraries are extremely unhappy.
The following sections discuss some reasons for these differences in
viewpoints.

\subsection{Locking For Applications: Hero!}
\label{sec:locking:Locking For Applications: Hero!}

When writing an entire application (or entire kernel), developers have
full control of the design, including the synchronization design.
Assuming that the design makes good use of partitioning, as discussed in
Chapter~\ref{cha:Partitioning and Synchronization Design}, locking
can be an extremely effective synchronization mechanism, as demonstrated
by the heavy use of locking in production-quality parallel software.

Nevertheless, although such software usually bases most of its
synchronization design on locking, such software also almost always
makes use of other synchronization mechanisms, including
special counting algorithms (\cref{chp:Counting}),
data ownership (\cref{chp:Data Ownership}),
reference counting (\cref{sec:defer:Reference Counting}),
hazard pointers (\cref{sec:defer:Hazard Pointers}),
sequence locking (\cref{sec:defer:Sequence Locks}), and
read-copy update (\cref{sec:defer:Read-Copy Update (RCU)}).
In addition, practitioners use tools for deadlock
detection~\cite{JonathanCorbet2006lockdep},
lock acquisition/release balancing~\cite{JonathanCorbet2004sparse},
cache-miss analysis~\cite{ValgrindHomePage},
hardware-counter-based profiling~\cite{LinuxKernelPerfWiki,OProfileHomePage},
and many more besides.

Given careful design, use of a good combination of synchronization
mechanisms, and good tooling, locking works quite well for applications
and kernels.

\subsection{Locking For Parallel Libraries: Just Another Tool}
\label{sec:locking:Locking For Parallel Libraries: Just Another Tool}

Unlike applications and kernels, the designer of a library cannot
know the locking design of the code that the library will be interacting
with.
In fact, that code might not be written for years to come.
Library designers therefore have less control and must exercise more
care when laying out their synchronization design.

Deadlock is of course of particular concern, and the techniques discussed
in Section~\ref{sec:locking:Deadlock} need to be applied.
One popular deadlock-avoidance strategy is therefore to ensure that
the library's locks are independent subtrees of the enclosing program's
locking hierarchy.
However, this can be harder than it looks.

One complication was discussed in
Section~\ref{sec:locking:Local Locking Hierarchies}, namely
when library functions call into application code, with \co{qsort()}'s
comparison-function argument being a case in point.
Another complication is the interaction with signal handlers.
If an application signal handler is invoked from a signal received within
the library function, deadlock can ensue just as surely as
if the library function had called the signal handler directly.
A final complication occurs for those library functions that can be used
between a \co{fork()}/\co{exec()} pair, for example, due to use of
the \co{system()} function.
In this case, if your library function was holding a lock at the time of
the \co{fork()}, then the child process will begin life with that lock held.
Because the thread that will release the lock is running in the parent
but not the child, if the child calls your library function, deadlock
will ensue.

The following strategies may be used to avoid deadlock problems in these cases:

\begin{enumerate}
\item	Don't use either callbacks or signals.
\item	Don't acquire locks from within callbacks or signal handlers.
\item	Let the caller control synchronization.
\item	Parameterize the library API to delegate locking to caller.
\item	Explicitly avoid callback deadlocks.
\item	Explicitly avoid signal-handler deadlocks.
\item	Avoid invoking \co{fork()}.
\end{enumerate}

Each of these strategies is discussed in one of the following sections.

\subsubsection{Use Neither Callbacks Nor Signals}
\label{sec:locking:Use Neither Callbacks Nor Signals}

If a library function avoids callbacks and the application as a whole
avoids signals, then any locks acquired by that library function will
be leaves of the locking-hierarchy tree.
This arrangement avoids deadlock, as discussed in
Section~\ref{sec:locking:Locking Hierarchies}.
Although this strategy works extremely well where it applies,
there are some applications that must use signal handlers,
and there are some library functions (such as the \co{qsort()} function
discussed in
Section~\ref{sec:locking:Local Locking Hierarchies})
that require callbacks.

The strategy described in the next section can often be used in these cases.

\subsubsection{Avoid Locking in Callbacks and Signal Handlers}
\label{sec:locking:Avoid Locking in Callbacks and Signal Handlers}

If neither callbacks nor signal handlers acquire locks, then they
cannot be involved in deadlock cycles, which allows straightforward
locking hierarchies to once again consider library functions to
be leaves on the locking-hierarchy tree.
This strategy works very well for most uses of \co{qsort}, whose
callbacks usually simply compare the two values passed in to them.
This strategy also works wonderfully for many signal handlers,
especially given that acquiring locks from within signal handlers
is generally frowned upon~\cite{TheOpenGroup1997SUS},\footnote{
	But the standard's words do not stop clever coders from creating
	their own home-brew locking primitives from atomic operations.}
but can fail if the application needs to manipulate complex data structures
from a signal handler.

Here are some ways to avoid acquiring locks in signal handlers even
if complex data structures must be manipulated:

\begin{enumerate}
\item	Use simple data structures based on non-blocking synchronization,
	as will be discussed in
	Section~\ref{sec:advsync:Simple NBS}.
\item	If the data structures are too complex for reasonable use of
	non-blocking synchronization, create a queue that allows
	non-blocking enqueue operations.
	In the signal handler, instead of manipulating the complex
	data structure, add an element to the queue describing the
	required change.
	A separate thread can then remove elements from the queue and
	carry out the required changes using normal locking.
	There are a number of readily available implementations of
	concurrent
	queues~\cite{ChristophMKirsch2012FIFOisntTR,MathieuDesnoyers2009URCU,MichaelScott96}.
\end{enumerate}

This strategy should be enforced with occasional manual or (preferably)
automated inspections of callbacks and signal handlers.
When carrying out these inspections, be wary of clever coders who
might have (unwisely) created home-brew locks from atomic operations.

\subsubsection{Caller Controls Synchronization}
\label{sec:locking:Caller Controls Synchronization}

Letting the caller control synchronization
works extremely well when the library functions are operating
on independent caller-visible instances of a data structure, each
of which may be synchronized separately.
For example, if the library functions operate on a search tree,
and if the application needs a large number of independent search
trees, then the application can associate a lock with each tree.
The application then acquires and releases locks as needed, so
that the library need not be aware of parallelism at all.
Instead, the application controls the parallelism, so that locking
can work very well, as was discussed in
Section~\ref{sec:locking:Locking For Applications: Hero!}.

However, this strategy fails if the
library implements a data structure that requires internal
concurrency, for example, a hash table or a parallel sort.
In this case, the library absolutely must control its own
synchronization.

\subsubsection{Parameterize Library Synchronization}
\label{sec:locking:Parameterize Library Synchronization}

The idea here is to add arguments to the library's API to specify
which locks to acquire, how to acquire and release them, or both.
This strategy allows the application to take on the global task of
avoiding deadlock by specifying which locks to acquire (by passing in
pointers to the locks in question) and how to
acquire them (by passing in pointers to lock acquisition and release
functions),
but also allows a given library function to control its own concurrency
by deciding where the locks should be acquired and released.

In particular, this strategy allows the lock acquisition and release
functions to block signals as needed without the library code needing to
be concerned with which signals need to be blocked by which locks.
The separation of concerns used by this strategy can be quite effective,
but in some cases the strategies laid out in the following sections
can work better.

That said, passing explicit pointers to locks to external APIs must
be very carefully considered, as discussed in
Section~\ref{sec:locking:Locking Hierarchies and Pointers to Locks}.
Although this practice is sometimes the right thing to do, you should do
yourself a favor by looking into alternative designs first.

\subsubsection{Explicitly Avoid Callback Deadlocks}
\label{sec:locking:Explicitly Avoid Callback Deadlocks}

The basic rule behind this strategy was discussed in
Section~\ref{sec:locking:Local Locking Hierarchies}: ``Release all
locks before invoking unknown code.''
This is usually the best approach because it allows the application to
ignore the library's locking hierarchy: the library remains a leaf or
isolated subtree of the application's overall locking hierarchy.

In cases where it is not possible to release all locks before invoking
unknown code, the layered locking hierarchies described in
Section~\ref{sec:locking:Layered Locking Hierarchies} can work well.
For example, if the unknown code is a signal handler, this implies that
the library function block signals across all lock acquisitions, which
can be complex and slow.
Therefore, in cases where signal handlers (probably unwisely) acquire
locks, the strategies in the next section may prove helpful.

\subsubsection{Explicitly Avoid Signal-Handler Deadlocks}
\label{sec:locking:Explicitly Avoid Signal-Handler Deadlocks}

Suppose that a given library function is known to acquire locks,
but does not block signals.
Suppose further that it is necessary to invoke that function both from
within and outside of a signal handler, and that it is not permissible
to modify this library function.
Of course, if no special action is taken, then if a signal arrives
while that library function is holding its lock, deadlock can occur
when the signal handler invokes that same library function,
which in turn attempts to re-acquire that same lock.

Such deadlocks can be avoided as follows:

\begin{enumerate}
\item	If the application invokes the library function from
	within a signal handler, then that signal must be blocked
	every time that the library function is invoked from outside
	of a signal handler.
\item	If the application invokes the library function
	while holding a lock acquired within a given signal
	handler, then that signal must be blocked every time that the
	library function is called outside of a signal handler.
\end{enumerate}

These rules can be enforced by using tools similar to
the Linux kernel's lockdep lock dependency
checker~\cite{JonathanCorbet2006lockdep}.
One of the great strengths of lockdep is that it is not fooled by
human intuition~\cite{StevenRostedt2011locdepCryptic}.

\subsubsection{Library Functions Used Between \tco{fork()} and \tco{exec()}}
\label{sec:locking:Library Functions Used Between fork() and exec()}

As noted earlier, if a thread executing a library function is holding
a lock at the time that some other thread invokes \co{fork()}, the
fact that the parent's memory is copied to create the child means that
this lock will be born held in the child's context.
The thread that will release this lock is running in the parent, but not
in the child, which means that the child's copy of this lock will never
be released.
Therefore, any attempt on the part of the child to invoke that same
library function will result in deadlock.

A pragmatic and straightforward way of solving this problem is
to \co{fork()} a child process while the process is still single-threaded,
and have this child process remain single-threaded.
Requests to create further child processes can then be communicated
to this initial child process, which can safely carry out any
needed \co{fork()} and \co{exec()} system calls on behalf of its
multi-threaded parent process.

Another rather less pragmatic and straightforward solution to this problem
is to have the library function check to see if the owner of the lock
is still running, and if not, ``breaking'' the lock by re-initializing
and then acquiring it.
However, this approach has a couple of vulnerabilities:

\begin{enumerate}
\item	The data structures protected by that lock are likely to
	be in some intermediate state, so that naively breaking the lock
	might result in arbitrary memory corruption.
\item	If the child creates additional threads, two threads might
	break the lock concurrently, with the result that both
	threads believe they own the lock.
	This could again result in arbitrary memory corruption.
\end{enumerate}

The \co{pthread_atfork()} function is provided to help deal with these situations.
The idea is to register a triplet of functions, one to be called by the
parent before the \co{fork()}, one to be called by the parent after the
\co{fork()}, and one to be called by the child after the \co{fork()}.
Appropriate cleanups can then be carried out at these three points.

Be warned, however, that coding of \co{pthread_atfork()} handlers is quite subtle
in general.
The cases where \co{pthread_atfork()} works best are cases where the data structure
in question can simply be re-initialized by the child.

\subsubsection{Parallel Libraries: Discussion}
\label{sec:locking:Parallel Libraries: Discussion}

Regardless of the strategy used, the description of the library's API
must include a clear description of that strategy and how the caller
should interact with that strategy.
In short, constructing parallel libraries using locking is possible,
but not as easy as constructing a parallel application.

\subsection{Locking For Parallelizing Sequential Libraries: Villain!}
\label{sec:locking:Locking For Parallelizing Sequential Libraries: Villain!}

With the advent of readily available low-cost multicore systems,
a common task is parallelizing an existing library that was designed
with only single-threaded use in mind.
This all-too-common disregard for parallelism can result in a library
API that is severely flawed from a parallel-programming viewpoint.
Candidate flaws include:

\begin{enumerate}
\item	Implicit prohibition of partitioning.
\item	Callback functions requiring locking.
\item	Object-oriented spaghetti code.
\end{enumerate}

These flaws and the consequences for locking are discussed in the following
sections.

\subsubsection{Partitioning Prohibited}
\label{sec:locking:Partitioning Prohibited}

Suppose that you were writing a single-threaded hash-table implementation.
It is easy and fast to maintain an exact count of the total number of items
in the hash table, and also easy and fast to return this exact count on each
addition and deletion operation.
So why not?

One reason is that exact counters do not perform or scale well on
multicore systems, as was
seen in Chapter~\ref{chp:Counting}.
As a result, the parallelized implementation of the hash table will not
perform or scale well.

So what can be done about this?
One approach is to return an approximate count, using one of the algorithms
from Chapter~\ref{chp:Counting}.
Another approach is to drop the element count altogether.

Either way, it will be necessary to inspect uses of the hash table to see
why the addition and deletion operations need the exact count.
Here are a few possibilities:

\begin{enumerate}
\item	Determining when to resize the hash table.
	In this case, an approximate count should work quite well.
	It might also be useful to trigger the resizing operation from
	the length of the longest chain, which can be computed and
	maintained in a nicely partitioned per-chain manner.
\item	Producing an estimate of the time required to traverse the
	entire hash table.
	An approximate count works well in this case, also.
\item	For diagnostic purposes, for example, to check for items being
	lost when transferring them to and from the hash table.
	This clearly requires an exact count.
	However, given that this usage is diagnostic in nature, it might
	suffice to maintain the lengths of the hash chains, then to
	infrequently sum them
	up while locking out addition and deletion operations.
\end{enumerate}

It turns out that there is now a strong theoretical basis for some of the
constraints that performance and scalability place on a parallel library's
APIs~\cite{HagitAttiya2011LawsOfOrder,Attiya:2011:LOE:1925844.1926442,PaulEMcKenney2011SNC}.
Anyone designing a parallel library needs to pay close attention to
those constraints.

Although it is all too easy to blame locking for what are really problems
due to a concurrency-unfriendly API, doing so is not helpful.
On the other hand, one has little choice but to sympathize with the
hapless developer who made this choice in (say) 1985.
It would have been a rare and courageous developer to anticipate the
need for parallelism at that time, and it would have required an
even more rare combination of brilliance and luck to actually arrive
at a good parallel-friendly API\@.

Times change, and code must change with them.
That said, there might be a huge number of users of a popular library,
in which case an incompatible change to the API would be quite foolish.
Adding a parallel-friendly API to complement the existing heavily used
sequential-only API is usually the best course of action.

Nevertheless, human nature being what it is, we can expect our hapless
developer to be more likely to complain about locking than about his
or her own poor (though understandable) API design choices.

\subsubsection{Deadlock-Prone Callbacks}
\label{sec:locking:Deadlock-Prone Callbacks}

Sections~\ref{sec:locking:Local Locking Hierarchies},
\ref{sec:locking:Layered Locking Hierarchies},
and~\ref{sec:locking:Locking For Parallel Libraries: Just Another Tool}
described how undisciplined use of callbacks can result in locking
woes.
These sections also described how to design your library function to
avoid these problems, but it is unrealistic to expect a 1990s programmer
with no experience in parallel programming to have followed such a design.
Therefore, someone attempting to parallelize an existing callback-heavy
single-threaded library will likely have many opportunities to curse
locking's villainy.

If there are a very large number of uses of a callback-heavy library,
it may be wise to again add a parallel-friendly API to the library in
order to allow existing users to convert their code incrementally.
Alternatively, some advocate use of transactional memory in these cases.
While the jury is still out on transactional memory,
Section~\ref{sec:future:Transactional Memory} discusses its strengths and
weaknesses.
It is important to note that hardware transactional memory
(discussed in
Section~\ref{sec:future:Hardware Transactional Memory})
cannot help here unless the hardware transactional memory implementation
provides forward-progress guarantees, which few do.
Other alternatives that appear to be quite practical (if less heavily
hyped) include the methods discussed in
Sections~\ref{sec:locking:Conditional Locking},
and~\ref{sec:locking:Acquire Needed Locks First},
as well as those that will be discussed in
Chapters~\ref{chp:Data Ownership}
and~\ref{chp:Deferred Processing}.

\subsubsection{Object-Oriented Spaghetti Code}
\label{sec:locking:Object-Oriented Spaghetti Code}

Object-oriented programming went mainstream sometime in the 1980s or
1990s, and as a result there is a huge amount of single-threaded
object-oriented code in production.
Although object orientation can be a valuable software technique,
undisciplined use of objects can easily result in object-oriented
spaghetti code.
In object-oriented spaghetti code, control flits from object to object
in an essentially random manner, making the code hard to understand
and even harder, and perhaps impossible, to accommodate a locking hierarchy.

Although many might argue that such code should be cleaned up in any
case, such things are much easier to say than to do.
If you are tasked with parallelizing such a beast, you can reduce the
number of opportunities to curse locking by using the techniques
described in
Sections~\ref{sec:locking:Conditional Locking},
and~\ref{sec:locking:Acquire Needed Locks First},
as well as those that will be discussed in
Chapters~\ref{chp:Data Ownership}
and~\ref{chp:Deferred Processing}.
This situation appears to be the use case that inspired transactional
memory, so it might be worth a try as well.
That said, the choice of synchronization mechanism should be made in
light of the hardware habits discussed in
Chapter~\ref{chp:Hardware and its Habits}.
After all, if the overhead of the synchronization mechanism is orders of
magnitude more than that of the operations being protected, the results
are not going to be pretty.

And that leads to a question well worth asking in these situations:
Should the code remain sequential?
For example, perhaps parallelism should be introduced at the process level
rather than the thread level.
In general, if a task is proving extremely hard, it is worth some time
spent thinking about not only alternative ways to accomplish that
particular task, but also alternative tasks that might better solve
the problem at hand.

\section{Summary}
\label{sec:locking:Summary}
%
\epigraph{Achievement unlocked.}{\emph{Unknown}}

Locking is perhaps the most widely used and most generally useful
synchronization tool.
However, it works best when designed into an application
or library from the beginning.
Given the large quantity of pre-existing single-threaded code that might
need to one day run in parallel, locking should therefore not be the
only tool in your parallel-programming toolbox.
The next few chapters will discuss other tools, and how they can best
be used in concert with locking and with each other.

\QuickQuizAnswersChp{qqzlocking}
