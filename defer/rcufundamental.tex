% defer/rcufundamental.tex

\subsection{RCU Fundamentals}
\label{sec:defer:RCU Fundamentals}
\OriginallyPublished{Section}{sec:defer:RCU Fundamentals}{RCU Fundamentals}{Linux Weekly News}{PaulEMcKenney2007WhatIsRCUFundamentally}

RCU is made up of three fundamental mechanisms, the first being
used for insertion, the second being used for deletion, and the third
being used to allow readers to tolerate concurrent insertions and deletions.
Section~\ref{sec:defer:Publish-Subscribe Mechanism}
describes the publish-subscribe mechanism used for insertion,
Section~\ref{sec:defer:Wait For Pre-Existing RCU Readers}
describes how waiting for pre-existing RCU readers enabled deletion,
and
Section~\ref{sec:defer:Maintain Multiple Versions of Recently Updated Objects}
discusses how maintaining multiple versions of recently updated objects
permits concurrent insertions and deletions.
Finally,
Section~\ref{sec:defer:Summary of RCU Fundamentals}
summarizes RCU fundamentals.

\subsubsection{Publish-Subscribe Mechanism}
\label{sec:defer:Publish-Subscribe Mechanism}

Because RCU readers are not excluded by RCU updaters, an RCU-protected
data structure might change while a reader accesses it.
The accessed data item might be moved, removed, or replaced.
Because the data structure does not ``hold still'' for the reader,
each reader's access can be thought of as subscribing to the current
version of the RCU-protected data item.
For their part, updaters can be thought of as publishing new versions.

% @@@ Merge usage section into "Which to Choose?" and suggest choices.
% @@@ Should "RCU Exercises" move to "RCU Rescues"?

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{defer/pubsub}}
\caption{Publication/Subscription Constraints}
\label{fig:defer:Publication/Subscription Constraints}
\end{figure}

Unfortunately, as laid out in
Section~\ref{sec:toolsoftrade:Shared-Variable Shenanigans}
and reiterated in
Section~\ref{sec:defer:Minimal Insertion and Deletion},
it is unwise to use plain accesses for these publication and subscription
operations.
It is instead necessary to inform both the compiler and the CPU
of the need for care, as can be seen from
Figure~\ref{fig:defer:Publication/Subscription Constraints},
which illustrates interactions between concurrent executions of
\co{ins_route()} (and its caller) and \co{read_gptr()} from
Listing~\ref{lst:defer:Insertion and Deletion With Concurrent Readers}.

The \co{ins_route()} column from
Figure~\ref{fig:defer:Publication/Subscription Constraints}
shows \co{ins_route()}'s caller allocating a new \co{route} structure,
which then contains pre-initialization garbage.
The caller then initializes the newly allocated structure, and then
invokes \co{ins_route()} to publish a pointer to the new \co{route}
structure.
Publication does not affect the contents of the structure, which
therefore remain valid after publication.

The \co{access_route()} column from this same figure shows
the pointer being subscribed to and dereferenced.
This dereference operation absolutely must see a valid \co{route}
structure rather than pre-initialization garbage because referencing
garbage could result in memory corruption, crashes, and hangs.
As noted earlier, avoiding such garbage means that the publish and
subscribe operations must inform both the compiler and the CPU of the
need to maintain the needed ordering.

Publication is carried out by \co{rcu_assign_pointer()}, which ensures that
\co{ins_route()}'s callers initialization is ordered before the actual
publication operation's store of the pointer.
In addition, \co{rcu_assign_pointer()} must be atomic in the sense that 
concurrent readers see either the old value of the pointer or the new
value of the pointer, but not some mash-up of these two values.
These requirements are met by the C11 store-release operation,
and in fact in the Linux kernel, \co{rcu_assign_pointer()} is defined
in terms of \co{smp_store_release()}, which is roughly similar to
C11 store-release.

Note that if concurrent updates are required, some sort of synchronization
mechanism will be required to mediate among multiple concurrent
\co{rcu_assign_pointer()} calls on the same pointer.
In the Linux kernel, locking is the mechanism of choice, but pretty
much any synchronization mechanism may be use.
An example of a particularly lightweight synchronization mechanism is
Chapter~\ref{chp:Data Ownership}'s data ownership: If each pointer is
owned by a particular thread, then that thread may execute
\co{rcu_assign_pointer()} on that pointer with no additional
synchronization overhead.

\QuickQuiz{}
	Wouldn't use of data ownership for RCU updaters mean that
	the updates could use exactly the same sequence of instructions
	as would the corresponding single-threaded code?
\QuickQuizAnswer{
	Sometimes, for example, on TSO systems such as x86 or the IBM
	mainframe where a store-release operation emits a single store
	instruction.
	However, weakly ordered systems must also emit a memory barrier
	or perhaps a store-release instruction.
	In addition, removing data requires quite a bit of additional
	work because it is necessary to wait for pre-existing readers
	before freeing the removed data.
} \QuickQuizEnd

Subscription is carried out by \co{rcu_dereference()}, which ensures that
the subscription operation's load from the pointer is ordered before the
dereference.
Similar to \co{rcu_assign_pointer()}, \co{rcu_dereference()} must be
atomic in the sense that the value loaded must be that from a single
store, for example, the compiler must not tear the load.\footnote{
	That is, the compiler must not break the load into multiple
	smaller loads, as described under ``load tearing'' in
	Section~\ref{sec:toolsoftrade:Shared-Variable Shenanigans}.}
Unfortunately, compiler support for \co{rcu_dereference()} is at best
a work in progress~\cite{PaulEMcKennneyConsumeP0190R4,PaulEMcKenney2017markconsumeP0462R1}. @@@ Add JF Bastien's consume paper. @@@
In the meantime, the Linux kernel relies on volatile loads, the details of
the various CPU architectures, coding
restrictions~\cite{PaulEMcKenney2014rcu-dereference},
and, on DEC Alpha~\cite{ALPHA2002}, a memory-barrier instruction.
However, on other architectures, \co{rcu_dereference()} typically
emits a single load instruction, just as would the equivalent single-threaded
code.
The coding restrictions are described in more detail in
Section~\ref{sec:memorder:Address- and Data-Dependency Difficulties},
however, the common case of field selection (\qco{->}) works quite well.
Software that does not require the ultimate in read-side performance
can instead use C11 acquire loads, which provide the needed ordering and
more, albeit at a cost.
It is hoped that lighter-weight compiler support for \co{rcu_dereference()}
will appear sooner rather than later.

In short, use of \co{rcu_assign_pointer()} for publishing pointers and
use of \co{rcu_dereference()} for subscribing to them successfully avoids the
``Not OK'' garbage loads depicted in
Figure~\ref{fig:defer:Publication/Subscription Constraints}.
These two primitives can therefore be used to add new data to linked
structures without disrupting concurrent readers.

\QuickQuiz{}
	But suppose that updaters are adding and removing multiple data
	items from a linked list while a reader is iterating over that
	same list.
	Specifically, suppose that a list initially contains elements
	A, B, and C, and that an updater removes element A and then
	adds a new element D at the end of the list.
	The reader might well see \{A, B, C, D\}, when that sequence of
	elements never actually ever existed!
	In what alternate universe would that qualify as ``not disrupting
	concurrent readers''???
\QuickQuizAnswer{
	In the universe where an iterating reader is only required to
	traverse elements that were present throughout the full duration
	of the iteration.
	In the example, that would be elements B and C.
	Because elements A and D were each present for only part of the
	iteration, the reader is permitted to iterate over them, but not
	obliged to.

	If stronger consistency is required, then higher-cost
	synchronization mechanisms are required, for example, sequence
	locking or reader-writer locking.
	But if stronger consistency is \emph{not} required (and it very often
	is not), then why pay the higher cost?
} \QuickQuizEnd

Adding data to a linked structure without disrupting readers is a good thing,
as are the cases where this can be done with no added read-side cost compared
to single-threaded readers.
However, in most cases it is also necessary to remove data, and this is the
subject of the next section.

\subsubsection{Wait For Pre-Existing RCU Readers}
\label{sec:defer:Wait For Pre-Existing RCU Readers}

가장 기본적인 형태에서, RCU 는 일들이 끝나기를 기다리는 방법입니다.
물론, RCU 외에도 일들이 끝나길 기다리는 훌륭한 방법들이 여럿 있는데, 레퍼런스
카운팅, reader-writer lock, 이벤트 등등이 포함됩니다.
RCU 의 커다란 장점은 (대략) 20,000 개의 서로 다른 일들을 명시적으로 그 모든
것들을 각각 정보를 쫓아가지 않으면서 성능 하락, 확장성 제한, 복잡한 데드락
시나리오, 그리고 명시적으로 정보를 쫓는 방법에서는 필연적인 메모리 누수 문제를
걱정할 필요 없이 기다릴 수 있다는 겁니다.
\iffalse

In its most basic form, RCU is a way of waiting for things to finish.
Of course, there are a great many other ways of waiting for things to
finish, including reference counts, reader-writer locks, events, and so on.
The great advantage of RCU is that it can wait for each of
(say) 20,000 different things without having to explicitly
track each and every one of them, and without having to worry about
the performance degradation, scalability limitations, complex deadlock
scenarios, and memory-leak hazards that are inherent in schemes
using explicit tracking.
\fi

RCU 의 경우에, 기다려지고 있는 일들은 \emph{RCU read-side 크리티컬 섹션} 이라
불립니다.
Section~\ref{sec:defer:Toy Implementation}
에서 힌트를 준 것처럼, RCU read-side 크리티컬 섹션은 \co{rcu_read_lock()}
함수로 시작되고, 그에 연관되는 \co{rcu_read_unlock()} 함수로 종료됩니다.
RCU read-side 크리티컬 섹션들은 중첩될 수 있고, 어떤 코드든 그 코드가
quiescent state 를 갖지 않는 한 그 안에 들어갈 수 있는데, 예를 들어 리눅스
커널에서는 컨텍스트 스위치가 quiescent state 중 하나이기 때문에 RCU read-side
크리티컬 섹션 내에서 sleep 을 하는건 불법입니다.\footnote{
	하지만, SRCU~\cite{PaulEMcKenney2006c} 라고 하는 특수한 형태의 RCU 는
	SRCU read-side 크리티컬 섹션 내에서의 sleep 을 허용합니다.}
여러분이 이 규약을 준수한다면, 여러분은 \emph{모든} 이전부터 존재한 RCU
read-side 크리티컬 섹션이 완료되길 기다리는데 RCU 를 사용할 수 있으며,
\co{synchronize_rcu()} 가 실제 이 기다림을 행합니다.

불리는 특별한 형태의 RCU 가 SRCU read-side 크리티컬 섹션 내에서의 일반적인
잠들기를 가능하게 하긴 하지만), 그 안에 얼마든지 들어갈 수 있습니다.
이런 규칙에 동의한다면, 코드의 \emph{모든} 원하는 부분이 완료되기를
기다리는데에 RCU 를 사용할 수 있습니다.
\iffalse

In RCU's case, each of the things waited on is called an
\emph{RCU read-side critical section}.
As hinted at in
Section~\ref{sec:defer:Toy Implementation}, an RCU read-side critical
section starts with an \co{rcu_read_lock()} primitive, and ends with a
corresponding \co{rcu_read_unlock()} primitive.
RCU read-side critical sections can be nested, and may contain pretty
much any code, as long as that code does not contain a quiescent state,
for example, within the Linux kernel, it is illegal to sleep within
an RCU read-side critical section because a context switch is a quiescent
state.\footnote{
	However, a special form of RCU called SRCU~\cite{PaulEMcKenney2006c}
	does permit general sleeping in SRCU read-side critical sections.}
If you abide by these conventions, you can use RCU to wait for \emph{any}
pre-existing RCU read-side critical section to complete, and
\co{synchronize_rcu()} does the actual waiting.
\fi

% @@@ citations? @@@ RCU accomplishes this feat by indirectly determining
% when these other things have finished~\cite{PaulEMcKenney2007whatisRCU,
% PaulEMcKenney2007PreemptibleRCU}.

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{defer/RCUGuaranteeFwd}}
\caption{RCU Reader and Later Grace Period}
\label{fig:defer:RCU Reader and Later Grace Period}
\end{figure}

RCU read-side 크리티컬 섹션과 뒤의 RCU grace period 사이의 관계는
Figure~\ref{fig:defer:RCU Reader and Later Grace Period} 에 그린 것처럼 if-then
관계입니다.
특정 크리티컬 섹션의 어떤 부분이 특정 grace period 의 시작을 앞선다면, RCU 는
이 크리티컬 섹션 전체가 이 grace period 의 종료를 앞서게 될 것을 보장합니다.
이 그림에서, \co{P0()} 의 \co{y} 접근이 \co{P1()} 의 해당 변수로의 접근을
앞서므로, \co{P0()} 의 \co{x} 로의 접근은 \co{P1()} 의 액세스를 앞설 겁니다.
이 경우, \co{y} 의 마지막 값이 2라면, \co{x} 의 마지막 값 역시 2가 될 것이
보장됩니다.
\iffalse

The relationship between an RCU read-side critical section and a later
RCU grace period is an if-then relationship, as illustrated by
Figure~\ref{fig:defer:RCU Reader and Later Grace Period}.
If any portion of a given critical section precedes the beginning of
a given grace period, then RCU guarantees that all of that critical
section will precede the end of that grace period.
In the figure, because \co{P0()}'s access to \co{y} precedes
\co{P1()}'s access to this same variable, it is guaranteed that
\co{P0()}'s access to \co{x} will precede \co{P1()}'s access.
In this case, if \co{y}'s final value is 2, then \co{x}'s
final value is guaranteed to also be 2.
\fi

\QuickQuiz{}
	Figure~\ref{fig:defer:RCU Reader and Later Grace Period}
	에서 \co{x} 와 \co{y} 의 마지막 값은 다른 어떤 값이 될수도 있을까요?
	\iffalse

	What other final values of \co{x} and \co{y} are possible in
	Figure~\ref{fig:defer:RCU Reader and Later Grace Period}?
	\fi
\QuickQuizAnswer{
	\co{x == 2 && y == 2} 일 가능성은 앞의 글에서 설명되었습니다.
	\co{y == 2} 가 \co{x == 2} 일 것을 의미하므로, \co{x == 1 && y == 2} 는
	불가능함을 알고 있습니다.
	뒤의 토의는 \co{x == 1 && y == 1} 과 \co{x == 2 && y == 1} 이 가능함을
	보입니다.
	\iffalse

	The \co{x == 2 && y == 2} possibility was called out in the text.
	Given that \co{y == 2} implies \co{x == 2}, we know that
	\co{x == 1 && y == 2} is forbidden.
	The following discussion will show that both
	\co{x == 1 && y == 1} and \co{x == 2 && y == 1} are possible.
	\fi
} \QuickQuizEnd

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{defer/RCUGuaranteeRev}}
\caption{RCU Reader and Earlier Grace Period}
\label{fig:defer:RCU Reader and Earlier Grace Period}
\end{figure}

RCU read-side 크리티컬 섹션과 그 전의 RCU grace period 사이의 관계 역시 if-then
관계이며,
Figure~\ref{fig:defer:RCU Reader and Earlier Grace Period} 에 그려져 있습니다.
특정 크리티컬 섹션의 어떤 부분이 특정 grace period 의 종료를 뒤따른다면, RCU 는
이 크리티컬 섹션의 모든 부분이 이 grace period 의 시작을 뒤따를 것을
보장합니다.
이 그림에서, \co{P0()} 의 \co{x} 로의 접근이 \co{P1()} 의 같은 변수로의 접근을
뒤따르므로, \co{P0()} 의 \co{y} 로의 접근은 \co{P1()} 의 접근을 앞설 것이
보장됩니다.
이 경우, 만약 \co{x} 의 마지막 값이 1이라면, \co{y} 의 마지막 값 역시 1일 것이
보장됩니다.
\iffalse

The relationship between an RCU read-side critical section and an earlier
RCU grace period is also an if-then relationship, as illustrated by
Figure~\ref{fig:defer:RCU Reader and Earlier Grace Period}.
If any portion of a given critical section follows the end of
a given grace period, then RCU guarantees that all of that critical
section will follow the beginning of that grace period.
In the figure, because \co{P0()}'s access to \co{x} follows
\co{P1()}'s access to this same variable, it is guaranteed that
\co{P0()}'s access to \co{y} will precede \co{P1()}'s access.
In this case, if \co{x}'s final value is 1, then \co{y}'s
final value is guaranteed to also be 1.
\fi

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{defer/RCUGuaranteeMid}}
\caption{RCU Reader Within Grace Period}
\label{fig:defer:RCU Reader Within Grace Period}
\end{figure}

마지막으로,
Figure~\ref{fig:defer:RCU Reader Within Grace Period} 에 보인 것처럼, RCU
read-side 크리티컬 섹션은 RCU grace period 에 의해 완전히 중복될 수 있습니다.
여기서 \co{x} 의 마지막 값은 1이고 \co{y} 의 마지막 값은 2 입니다.
\iffalse

Finally, as shown in
Figure~\ref{fig:defer:RCU Reader Within Grace Period},
an RCU read-side critical section can be completely overlapped by
an RCU grace period.
In this case, \co{x}'s final value is 1 and \co{y}'s final value is 2.
\fi

하지만, \co{x} 의 마지막 값이 2 이고 \co{y} 의 마지막 값이 1일 수는 없습니다.
이는 RCU read-side 크리티컬 섹션은 하나의 grace period 내에 완전히 파묻혀서,
감춰져 버릴 수 있음을 의미합니다.
RCU 의 wait-for-readers 보장은 따라서 두개의 부분으로 이루어집니다:
(1)~만약 특정 RCU read-side 크리티컬 섹션의 어느 부분이 특정 grace period 의
시작을 앞선다면, 이 크리티컬 섹션의 모든 부분이 이 grace period 의 끝을 앞선다.
(2)~만약 특정 RCU read-side 크리티컬 섹션의 어느 부분이 특정 grace period 의
끝을 뒤따른다면, 이 크리티컬 섹션의 모든 부분이 이 grace period 의 시작을
뒤따른다.
이 정의는 거의 모든 RCU 기반 알고리즘에 충분합니다만, 더 많은 걸 원하는
사람들을 위해, RCU 의 간단한 수행에 대한 정형적 모델이 리눅스 커널 v4.17 과 그
뒤 버전들의 부분으로 사용 가능하며,
Section~\ref{sec:formal:Axiomatic Approaches and RCU} 에 설명되어 있습니다.
추가로, RCU 의 순서 특성은 Section~\ref{sec:memorder:RCU} 에 훨씬 자세히
설명되어 있습니다.
\iffalse

However, it cannot be the case that \co{x}'s final value is 2 and \co{y}'s
final value is 1.
This would mean that an RCU read-side critical section had completely
overlapped a grace period, which is forbidden.
RCU's wait-for-readers guarantee therefore has two parts:
(1)~If any part of a given RCU read-side critical section precedes
the beginning of a given grace period, then the entirety of that
critical section precedes the end of that grace period.
(2)~If any part of a given RCU read-side critical section follows
the end of a given grace period, then the entirety of that
critical section follows the beginning of that grace period.
This definition is sufficient for almost all RCU-based algorithms, but
for those wanting more,
simple executable formal models of RCU are available
as part of Linux kernel v4.17 and later, as discussed in
Section~\ref{sec:formal:Axiomatic Approaches and RCU}.
In addition, RCU's ordering properties are examined in much
greater detail in Section~\ref{sec:memorder:RCU}.
\fi

RCU 의 wait-for-readers 기능이
Figure~\ref{fig:defer:RCU Reader and Later Grace Period}-\ref{fig:defer:RCU Reader Within Grace Period}
에서 보인 것처럼 변수로의 값 할당 순서를 위해 가끔은 사용되지만,
Section~\ref{sec:defer:Introduction to RCU} 에서 한것처럼 연결된 구조체에서
제거된 데이터 원소를 안전히 해제하는데에 더 자주 사용됩니다.
일반화된 프로세스는 다음 pseudocode 로 설명됩니다:
\iffalse

Although RCU's wait-for-readers capability really is sometimes used to
order the assignment of values to variables as shown in
Figures~\ref{fig:defer:RCU Reader and Later Grace Period}-\ref{fig:defer:RCU Reader Within Grace Period},
it is more frequently used to safely free data elements removed from
a linked structure, as was done in
Section~\ref{sec:defer:Introduction to RCU}.
The general process is illustrated by the following pseudocode:
\fi

\begin{enumerate}
\item	변경을 만드는데, 예를 들어 링크드 리스트에서 원소를 제거합니다.
\item	모든 이전부터 존재한 RCU read-side 크리티컬 섹션이 완전히 종료되길
	기다립니다 (예를 들면, \co{synchronize_rcu()} 를 사용해서).
\item	해제를 진행합니다, 예를 들어 앞서 교체된 원소를 메모리에서
	해제시킵니다.
\iffalse

\item	Make a change, for example, remove an element from a linked list.
\item	Wait for all pre-existing RCU read-side critical sections to
	completely finish (for example, by using
	\co{synchronize_rcu()}).
\item	Clean up, for example, free the element that was replaced above.
\fi
\end{enumerate}

RCU 읽기 쓰레드는 업데이트가 진행중인 상황에서 진행될 수 있다는 점을 놓고 보면,
다른 읽기 쓰레드는 데이터 구조의 상태에 대해 다른 의견을 가질 수 있는데, 이는
다음 섹션에서 다루어지는 주제입니다.
\iffalse

Given that RCU readers can make forward progress while this update
is in progress, different readers might disagree about the state
of the data structure, a topic taken up by the next section.
\fi

\subsubsection{Maintain Multiple Versions of Recently Updated Objects}
\label{sec:defer:Maintain Multiple Versions of Recently Updated Objects}

This section discusses how RCU accommodates synchronization-free readers
by maintaining multiple versions of data.
This discussion builds on the introduction of multiple versions by
Figure~\ref{fig:defer:Deletion With Concurrent Readers}
in
Section~\ref{sec:defer:Minimal Insertion and Deletion},
in which readers running concurrently with \co{del_route()}
(see Listing~\ref{lst:defer:Insertion and Deletion With Concurrent Readers})
might see the old \co{route} structure or an empty list, but either
way get a valid result.
Of course, a closer look at
Figure~\ref{fig:defer:Insertion With Concurrent Readers}
shows that calls to \co{ins_route()} can also result in concurrent
readers seeing different versions: Either the initial empty list
or the newly inserted \co{route} structure.
Note that both reference counting
(Section~\ref{sec:defer:Reference Counting})
and hazard pointers
(Section~\ref{sec:defer:Hazard Pointers})
can also cause concurrent readers to see different versions, but
RCU's extremely light-weight readers increase the probability of
readers doing so.

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{defer/multver}}
\caption{Multiple RCU Data-Structure Versions}
\label{fig:defer:Multiple RCU Data-Structure Versions}
\end{figure}

However, maintaining multiple versions can be even more surprising.
For example, consider
Figure~\ref{fig:defer:Multiple RCU Data-Structure Versions},
in which a reader is traversing a linked list that is concurrently
updated.\footnote{
	RCU linked-list APIs may be found in
	Section~\ref{sec:defer:RCU Linux-Kernel API}.}
In the first row of the figure, the reader is referencing data item~A,
and in the second row, it advances to~B, having thus far seen A followed by~B.
In the third row, an updater removes element~A and in the fourth row
an updater adds element~E to the end of the list.
In the fifth and final row, the reader completes its traversal, having
seeing elements A through E.

Except that there was no time at which such a list existed.
This situation might be even more surprising than that shown in
Figure~\ref{fig:defer:Deletion With Concurrent Readers},
in which different concurrent readers see different versions.
In contrast, in
Figure~\ref{fig:defer:Multiple RCU Data-Structure Versions}
the reader sees a version that never actually existed!

One way to resolve this strange situation is via weaker semanitics.
A reader traversal must encounter any data item that was present
during the full traversal (B, C, and~D), and might or might not
encounter data items that were present for only part of the
traversal (A and~E).
Therefore, in this particular case, it is perfectly legitimate for
the reader traveral to encounter all five elements.
If this outcome is problematic, another way to resolve this situation is
through use of stronger synchronization mechanisms, such as reader-writer
locking.
Of course, stronger mechanisms will be more expensive, but life is
about choices and tradeoffs.

Strange though this situation might seem, it is entirely consistent with
the real world.
As we saw in
Section~\ref{sec:cpu:Overheads},
the finite speed of light cannot be ignored within a computer system,
but it also cannot be ignored outside of the system.
Which in turn means that any data within the system representing
state in the real world outside of the system is outdated, and thus
inconsistent with the real world.
As a result, algorithms operating on real-world data must account for
that data being inconsistent with the real world.
In many cases, such algorithms are also perfectly capable of dealing
with inconsistencies within the system.

The pre-BSD packet routing example laid out in
Section~\ref{sec:defer:Running Example}
is a case in point.
The contents of a routing list is set by routing protocols, and these
protocols feature significant delays (seconds or even minutes) to avoid
various instabilities.
These delays mean that once a routing update reaches a given system,
it might well have been sending packets the wrong way for quite some time.
Sending a few more packets the wrong way for the few microseconds during
which the update is in flight is clearly not a problem because the same
higher-level protocol actions that deal with the delayed routing updates
will also deal with the inconsistencies.

Nor is Internet routing the only situation tolerating inconsistencies.
To repeat, any algorithm in which data within a system tracks
outside-of-system state must tolerate inconsistencies, which includes
security policies (often set by committees of humans), storage configuration,
and WiFi access points, to say nothing of removable hardware such as
microphones, headsets, cameras, mice, printers, and much else besides.
Furthermore, the large number of Linux-kernel RCU API uses shown in
Figure~\ref{fig:defer:RCU Usage in the Linux Kernel},
combined with the Linux kernel's heavy use of reference counting
and with increasing use of hazard pointers in other projects, demonstrates
that tolerance for such inconsistencies is more common than one might
imagine.
This is especially the case given that single-item lookups are much more
common than traversals:  After all, concurrent updates are less likely
to affect a single-item lookup than they are a full traversal.

From a more theoretical viewpoint, there are even some special cases where
RCU readers can be considered to be fully ordered with updaters, despite
the fact that these readers might be executing the exact same sequence of
machine instructions that would be executed by a single-threaded program.
For example, referring back to
Listing~\ref{lst:defer:Insertion and Deletion With Concurrent Readers}
on page~\pageref{lst:defer:Insertion and Deletion With Concurrent Readers},
suppose that each reader thread invokes \co{access_route()} exactly
once during its lifetime, and that there is no other communication among
reader and updater threads.
Then each invocation of \co{access_route()} can be ordered after the
\co{ins_route()} invocation that produced the \co{route} structure
accessed by line~9 of \co{access_route()} and ordered before any subsequent
\co{ins_route()} or \co{del_route()} invocation.

In summary, maintaining multiple versions is exactly what enables the
extremely low overheads of RCU readers, and as noted earlier, many
algorithms are unfazed by multiple versions.
However, there are algorithms that absolutely cannot handle multiple versions.
There are techniques for adapting such algorithms to
RCU~\cite{PaulEdwardMcKenneyPhD},
but these are beyond the scope of this section.

\paragraph{Discussion}
\label{sec:defer:Discussion}

이 예제들은 모든 업데이트 오퍼레이션이 뮤텍스를 잡고 있다고 가정을 하고 있는데,
이 말은 리스트의 버전은 한 순간에 최대 두개까지만 존재할 수 있음을 의미합니다.
\iffalse

These examples assumed that a mutex was held across the entire
update operation, which would mean that there could be at most two
versions of the list active at a given time.
\fi

\QuickQuiz{}
	리스트의 버전이 두개보다 많을 수 있도록 하기 위해서는 삭제 예제를
	어떻게 수정해야 할까요?
	\iffalse

	How would you modify the deletion example to permit more than two
	versions of the list to be active?
	\fi
\QuickQuizAnswer{
	이를 가능하게 하는 한가지 방법은
	Listing~\ref{lst:defer:Concurrent RCU Deletion} 에 보인 것처럼 하는
	것입니다.
	\iffalse

	One way of accomplishing this is as shown in
	Listing~\ref{lst:defer:Concurrent RCU Deletion}.
	\fi

\begin{listing}[htbp]
\begin{VerbatimL}
spin_lock(&mylock);
p = search(head, key);
if (p == NULL)
	spin_unlock(&mylock);
else {
	list_del_rcu(&p->list);
	spin_unlock(&mylock);
	synchronize_rcu();
	kfree(p);
}
\end{VerbatimL}
\caption{Concurrent RCU Deletion}
\label{lst:defer:Concurrent RCU Deletion}
\end{listing}

	이 코드는 여러개의 동시에 수행되는 삭제 작업들이 \co{synchronize_rcu()}
	에서 기다리게 될수도 있음을 의미합니다.
	\iffalse

	Note that this means that multiple concurrent deletions might be
	waiting in \co{synchronize_rcu()}.
	\fi
} \QuickQuizEnd

\QuickQuiz{}
	하나의 리스트는 한 순간에 RCU 버전들을 몇개까지 가질 수 있을까요?
	\iffalse

	How many RCU versions of a given list can be
	active at any given time?
	\fi
\QuickQuizAnswer{
	동기화 설계에 따라 달라집니다.
	업데이트를 보호하는 세마포어가 grace period 에 걸쳐 잡혀 있다면, 옛날
	버전과 새로운 버전, 최대 두개의 버전이 있을 수 있을 겁니다.

	하지만, 검색, 업데이트, 그리고 \co{list_replace_rcu()} 만이 락으로
	보호되고 있어서
	Listing~\ref{lst:defer:Concurrent RCU Deletion} 보인 코드에서처럼
	\co{synchronize_rcu()} 가 락의 바깥에 있다고 생각해 봅시다.
	더 나아가서 수많은 쓰레드들이 거의 동시에 RCU 교체 기능을 수행했고, 이
	읽기 쓰레드들은 또한 데이터 구조를 계속해서 지나다니고 있다고 생각해
	봅시다.
	\iffalse

	That depends on the synchronization design.
	If a semaphore protecting the update is held across the grace period,
	then there can be at most two versions, the old and the new.

	However, suppose that only the search, the update, and the
	\co{list_replace_rcu()} were protected by a lock, so that
	the \co{synchronize_rcu()} was outside of that lock, similar
	to the code shown in
	Listing~\ref{lst:defer:Concurrent RCU Deletion}.
	Suppose further that a large number of threads undertook an
	RCU replacement at about the same time, and that readers
	are also constantly traversing the data structure.
	\fi

	그러면 다음과 같은 일련의 이벤트들이
	Figure~\ref{fig:defer:RCU Replacement in Linked List} 의 마지막
	상태로부터 발생할 수 있습니다.
	\iffalse

	Then the following sequence of events could occur, starting from
	the end state of
	Figure~\ref{fig:defer:RCU Replacement in Linked List}:
	\fi

	\begin{enumerate}
	\item	쓰레드~A 가 리스트를 횡단하면서 5,2,3 원소로의 레퍼런스를
		얻습니다.
	\item	쓰레드~B 가 5,2,3 원소를 새로운 5,2,4 원소로 교체하고, 자신의
		\co{synchronize_rcu()} 호출이 리턴하길 기다립니다.
	\item	쓰레드~C 가 리스트를 횡단하면서 5,2,4 원소로의 레퍼런스를
		얻습니다.
	\item	쓰레드~D 가 5,2,4 원소를 새로운 5,2,5 원소로 교체하고, 자신의
		\co{synchronize_rcu()} 호출이 리턴하길 기다립니다.
	\item	쓰레드~E 가 리스트를 횡단하면서 5,2,5 원소로의 레퍼런스를
		얻습니다.
	\item	쓰레드~F 가 5,2,5 원소를 새로운 5,2,6 원소로 교체하고, 자신의
		\co{synchronize_rcu()} 호출이 리턴하길 기다립니다.
	\item	쓰레드~G 가 리스트를 횡단하면서 5,2,6 원소로의 레퍼런스를
		얻습니다.
	\item	그리고 앞의 두 스텝들이 빠르게 계속해서 반복되어서 모든
		쓰레드들이 \co{synchronize_rcu()} 호출이 리턴하길 기다립니다.
	\end{enumerate}
	\iffalse

	\item	Thread~A traverses the list, obtaining a reference to
		the 5,2,3 element.
	\item	Thread~B replaces the 5,2,3 element with a new
		5,2,4 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~C traverses the list, obtaining a reference to
		the 5,2,4 element.
	\item	Thread~D replaces the 5,2,4 element with a new
		5,2,5 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~E traverses the list, obtaining a reference to
		the 5,2,5 element.
	\item	Thread~F replaces the 5,2,5 element with a new
		5,2,6 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~G traverses the list, obtaining a reference to
		the 5,2,6 element.
	\item	And the previous two steps repeat quickly, so that all
		of them happen before any of the \co{synchronize_rcu()}
		calls return.
	\end{enumerate}
	\fi

	따라서, 얼마든지 많은 버전들이 존재할 수 있습니다만, 메모리 크기와
	얼마나 많은 업데이트들이 하나의 grace period 안에 완료될 수 있느냐에 그
	수가 제한됩니다.
	하지만 그렇게 자주 업데이트 되는 데이터 구조체들은 RCU 를 사용하기에
	적합한 후보가 아닐 수 있음을 알아두시기 바랍니다.
	그렇다곤 해도, RCU 는 필요할 때에는 높은 비율의 업데이트를 처리할 수
	있습니다.
	\iffalse

	Thus, there can be an arbitrary number of versions active,
	limited only by memory and by how many updates could be completed
	within a grace period.
	But please note that data structures that are updated so frequently
	probably are not good candidates for RCU.
	That said, RCU can handle high update rates when necessary.
	\fi
} \QuickQuizEnd

\subsubsection{Summary of RCU Fundamentals}
\label{sec:defer:Summary of RCU Fundamentals}

이 섹션에서는 RCU 기반 알고리즘의 세가지 기본 컴포넌트를 설명했습니다:
\iffalse

This section has described the three fundamental components of RCU-based
algorithms:
\fi

\begin{enumerate}
\item	새로운 데이터의 추가를 위한 공개-구독 메커니즘,

\item	전부터 존재했던 RCU 읽기 쓰레드들이 종료되기를 기다리는 방법 (자세한
	내용은 Section~\ref{sec:memorder:RCU} 을 참고하세요), 그리고

\item	동시에 수행중인 RCU 읽기 쓰레드들에 피해를 주거나 너무 오래 기다리게
	하지 않고 변화를 가할 수 있도록 여러 버전을 관리하는 방법.
\iffalse

\item	a publish-subscribe mechanism for adding new data,

\item	a way of waiting for pre-existing RCU readers to finish
	(see Section~\ref{sec:memorder:RCU} for more detail),
	and

\item	a discipline of maintaining multiple versions to permit
	change without harming or unduly delaying concurrent RCU readers.
\fi
\end{enumerate}

\QuickQuiz{}
	\co{rcu_read_lock()} 과 \co{rcu_read_unlock()} 함수는 스핀하지도
	블락하지도 않는데 어떻게 RCU 업데이트 쓰레드들이 RCU 읽기 쓰레드들을
	대기시킬 수가 있나요?
	\iffalse

	How can RCU updaters possibly delay RCU readers, given that the
	\co{rcu_read_lock()} and \co{rcu_read_unlock()}
	primitives neither spin nor block?
	\fi
\QuickQuizAnswer{
	특정 RCU 업데이트 쓰레드에 의해 가해진 수정사항은 연관된 CPU 가 해당
	데이터를 포함하는 캐시 라인들을 무효화 하도록 만들 것이고, 동시에
	수행중인 RCU 읽기 쓰레드들을 수행중인 해당 CPU 들이 비싼 캐시 미스를
	마주하게 만들 겁니다.
	(동시에 수행중인 읽기 쓰레드에게 비싼 캐시 미스를 만들지 않고 데이터
	구조에 변경을 가할 수 있는 알고리즘을 설계할 수 있을까요?
	또는 뒤따르는 읽기 쓰레드들에게?)
	\iffalse

	The modifications undertaken by a given RCU updater will cause the
	corresponding CPU to invalidate cache lines containing the data,
	forcing the CPUs running concurrent RCU readers to incur expensive
	cache misses.
	(Can you design an algorithm that changes a data structure
	\emph{without}
	inflicting expensive cache misses on concurrent readers?
	On subsequent readers?)
	\fi
} \QuickQuizEnd

이 세개의 RCU 컴포넌트는 동시에 수행되는 읽기 쓰레드에 상관 없이 데이터가
업데이트 되도록 하고, 놀랍도록 다양한, 다른 종류의 RCU 기반의 알고리즘들을
구현하는 다른 방법들로 조합될 수 있는데, 그 중 일부는 다음 섹션에서
설명하겠습니다.
\iffalse

These three RCU components
allow data to be updated in face of concurrent readers, and
can be combined in different ways to
implement a surprising variety of different types of RCU-based algorithms.
However, it is usually better to work at higher levels of abstraction.
To this end, the next section describes the Linux-kernel API, which
includes simple data structures such as lists.
\fi
