% defer/defer.tex

\QuickQuizChapter{chp:Deferred Processing}{Deferred Processing}

\epigraph{All things come to those who wait.}{\emph{Violet Fane}}

The strategy of deferring work goes back before the dawn of recorded
history. It has occasionally been derided as procrastination or
even as sheer laziness.
However, in the last few decades workers have recognized this strategy's value
in simplifying and streamlining parallel algorithms~\cite{Kung80,HMassalinPhD}.
Believe it or not, ``laziness'' in parallel programming often outperforms and
out-scales industriousness!
These performance and scalability benefits stem from the fact that
deferring work often enables weakening of synchronization primitives,
thereby reducing synchronization overhead.
General approaches work deferral include
reference counting (Section~\ref{sec:defer:Reference Counting}),
hazard pointers (Section~\ref{sec:defer:Hazard Pointers}),
sequence locking (Section~\ref{sec:defer:Sequence Locks}),
and RCU (Section~\ref{sec:defer:Read-Copy Update (RCU)}).
Finally, Section~\ref{sec:defer:Which to Choose?}
describes how to choose among the work-deferral schemes covered in
this chapter and Section~\ref{sec:defer:What About Updates?}
discusses the role of updates.

This chapter will use a simplified demultiplexing algorithm to demonstrate
the value of these approaches and to allow them to be compared.
Demultiplexing algorithms are used in operating-system kernels to
deliver each incomoing TCP/IP packets to its receiving process.
This particular algorithm is a simplified version of the classic 1980s
packet-train-optimized algorithm used in BSD UNIX~\cite{VanJacobson88},
consisting of a simple linked list.\footnote{
	In other words, this is not OpenBSD, NetBSD, or even
	FreeBSD, but none other than Pre-BSD.}
Modern demultiplexing algorithms use more complex data structures,
for example, hash tables~\cite{McKenney92b}, however, as in
Chapter~\ref{chp:Counting}, an extremely simple algorithm will
help highlight issues specific to parallelism in an extremely
easy-to-understand setting.

We further simplify the algorithm by reducing the search key from
a quadruple consisting of source and destination IP addresses and
ports all the down to a simple integer.
The value looked up and returned will also be a simple integer,
so that the data structure is as shown in
Figure~\ref{fig:defer:Pre-BSD Packet Demultiplexing List}, which
directs packets of address~42 to process~1, address~56 to
process~2, and address~17 to process~10.
Assuming that processes are long-lived and receive a large number
of packets, this list will be searched frequently and updated
rarely.
In Chapter~\ref{chp:Hardware and its Habits}
we learned that the best ways to evade laws of physics, such as
the finite speed of light and the atomic nature of matter, is to
either partition the data or to rely on read-mostly sharing.
In this chapter, we will use the Pre-BSD packet demultiplexing
list to evaluate a number of techniques involving read-mostly
sharing.

\begin{figure}[tb]
\begin{center}
\resizebox{3in}{!}{\includegraphics{defer/DemuxList}}
\end{center}
\caption{Pre-BSD Packet Demultiplexing List}
\label{fig:defer:Pre-BSD Packet Demultiplexing List}
\end{figure}

\input{defer/refcnt}
\input{defer/hazptr}
\input{defer/seqlock}
\input{defer/rcu}
\input{defer/rcuexercises}
\input{defer/whichtochoose}
\input{defer/updates}

% @@@ compare and contrast the various mechanisms.
