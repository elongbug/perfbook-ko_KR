% appendix/whymb/whymemorybarriers.tex
% mainfile: ../../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:app:whymb:Why Memory Barriers?}{Why Memory Barriers?}{qqzwhymb}
%
\Epigraph{Order!
	  Order in the court!}
	 {\emph{Unknown}}

그래서 뭐가 CPU 설계자들을 불쌍한 의심없던 SMP 소프트웨어 설계자들에게
memory barreir 를 가하게 홀렸을까요?

짧게 말하자면, 메모리 참조를 재배치 하는 것은 훨씬 나은 성능을 가능하게
하며, 따라서 메모리 배리어는 올바른 오퍼레이션이 순서 맞춰진 메모리 참조에
의존적인 동기화 기능들 같은 것을 위해 순서를 강제하기 위해 필요합니다.

\iffalse

So what possessed CPU designers to cause them to inflict memory barriers
on poor unsuspecting SMP software designers?

In short, because reordering memory references allows much better performance,
and so memory barriers are needed to force ordering in things like
synchronization primitives whose correct operation depends on ordered
memory references.

\fi

이 질문에 대한 더 자세한 답은 CPU 캐쉬가 어떻게 동작하는지, 특히 캐쉬가 정말로
잘 동작하기 위해 무엇을 필요로 하는지에 대한 좋은 이해가 필요합니다.
다음 섹션들은:
\begin{enumerate}
\item	캐쉬의 구조를 보이고,
\item	캐쉬 일관성 프로토콜이 어떻게 CPU 들이 각 메모리 위치의 값에 동의를
	하는 것을 보장하는지 설명하며, 마지막으로
\item	스토어 버퍼와 무효화 큐가 어떻게 캐쉬와 캐쉬 일관성 프로토콜에게 높은
	성능을 이루게 돕는지 설명합니다.
\end{enumerate}
우린 메모리 배리어가 좋은 성능과 확장성을 가능하게 하기 위해 필요하며 CPU 는
그것들과 그것들이 접근하고자 하는 메모리 사이의 연결부보다 수십수백배 빠르다는
사실에서 기인한 필요한 악마임을 보게 될 겁니다.

\iffalse

Getting a more detailed answer to this question requires a good understanding
of how CPU caches work, and especially what is required to make
caches really work well.
The following sections:
\begin{enumerate}
\item	present the structure of a cache,
\item	describe how cache-coherency protocols ensure that CPUs agree
	on the value of each location in memory, and, finally,
\item	outline how store buffers and invalidate queues help
	caches and cache-coherency protocols achieve high performance.
\end{enumerate}
We will see that memory barriers are a necessary evil that is required
to enable good performance and scalability, an evil that stems from
the fact that CPUs are orders of magnitude faster than are both the
interconnects between them and the memory they are attempting to access.

\fi

\section{Cache Structure}
\label{sec:app:whymb:Cache Structure}

현대의 CPU 는 현대의 메모리 시스템보다 훨씬 빠릅니다.
2006년의 CPU 는 나노세컨드당 열개의 명령을 수행할 수도 있으나, 메인 메모리에서
데이터 항목을 가져오는데에는 수십 나노세컨드를 필요로 합니다.
이 속도에서의 괴리---수백배 이상의---는 현대 CPU 에서 찾아볼 수 있는 수백
메가바이트 캐쉬를 초래했습니다.
이 캐쉬들은
\cref{fig:app:whymb:Modern Computer System Cache Structure}
에 보인 것처럼 CPU 들과 연관되어지며, 보통 수 사이클만에 접근될 수
있습니다.\footnote{
	작은 레벨 1 의 캐쉬를 1 사이클 액세스 시간을 가질 만큼 CPU 에 가깝게,
	더 큰 레벨 2 의 캐쉬를 더 긴 액세스 속도, 대략 10 클락 사이클로
	위치하는 식의 다중 레벨의 캐쉬를 사용하는 것은 표준적인 방법입니다.
	고성능 CPU 들은 종종 3 또는 4 레벨의 캐쉬조차 같습니다.}

\iffalse

Modern CPUs are much faster than are modern memory systems.
A 2006 CPU might be capable of executing ten instructions per nanosecond,
but will require many tens of nanoseconds to fetch a data item from
main memory.
This disparity in speed---more than two orders of magnitude---has
resulted in the multi-megabyte caches found on modern CPUs.
These caches are associated with the CPUs as shown in
\cref{fig:app:whymb:Modern Computer System Cache Structure},
and can typically be accessed in a few cycles.\footnote{
	It is standard practice to use multiple levels of cache,
	with a small level-one cache close to the CPU with
	single-cycle access time, and a larger level-two cache
	with a longer access time, perhaps roughly ten clock cycles.
	Higher-performance CPUs often have three or even four levels
	of cache.}

\fi

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{appendix/whymb/cacheSC}}
\caption{Modern Computer System Cache Structure}
\label{fig:app:whymb:Modern Computer System Cache Structure}
\end{figure}

데이터는 ``캐쉬 라인'' 이라 불리는 고정 길이 블록으로 CPU 들의 캐쉬와
메모리 사이를 떠다니는데, 그 크기는 보통 2 의 승수이며, 16 에서 256 바이트
사이를 오갑니다.
어떤 데이터 항목이 어떤 CPU 에 의해 처음 액세스 될 때, 이는 해당 CPU 의 캐쉬에
존재하지 않을 텐데, 이는 ``캐쉬 미스 (cache miss)'' (또는, 더 구체적으로는
``startup'' 또는 ``warmup'' 캐쉬 미스) 가 일어났음을 의미합니다.
이 캐쉬 미스는 이 CPU 가 해당 항목이 메모리로부터 읽어들여지기까지 수백
사이클을 기다려야 (또는 ``stall'' 되어야) 함을 의미합니다.
그러나, 이 항목은 이 CPU 의 캐쉬로 읽혀들여질 것이므로, 뒤따르는 액세스들은 이
항목을 캐쉬에서 발견하고 따라서 완전한 속도를 낼 수 있을 겁니다.

어느정도 시간이 지나면 이 CPU 의 캐쉬는 가득찰 테고, 뒤따르는 미스들은 새로
읽어온 항목을 위한 공간을 마련하기 위해 존재하는 항목을 내버려야 할 겁니다.
이런 캐쉬 미스는 ``capacity miss'' 라고
명명되는데, 캐쉬의 제한된 용량 때문에 일어났기 때문입니다.
그러나, 대부분의 캐쉬는 완전히 가득 차지 않았을 때 조차도 새 항목을 위한 공간을
위해 오래된 항목을 버려야 할 수 있습니다.
이는 커다란 캐쉬는
\cref{fig:app:whymb:CPU Cache Structure} 에 보인 것처럼
고정 크기 해쉬 버킷과 (또는 CPU 설계자들이 부르는 대로라면 ``set'') 체이닝
없는 하드웨어 해쉬 테이블로 구현되어있다는 사실 때문입니다.

\iffalse

Data flows among the CPUs' caches and memory in fixed-length blocks
called ``cache lines'', which are normally a power of two in size,
ranging from 16 to 256 bytes.
When a given data item is first accessed by a given CPU, it will
be absent from that CPU's cache, meaning that a ``cache miss''
(or, more specifically, a ``startup'' or ``warmup'' cache miss)
has occurred.
The cache miss means that the CPU will
have to wait (or be ``stalled'') for hundreds of cycles while the
item is fetched from memory.
However, the item will be loaded into that CPU's cache, so that
subsequent accesses will find it in the cache and therefore run
at full speed.

After some time, the CPU's cache will fill, and subsequent
misses will likely need to eject an item from the cache in order
to make room for the newly fetched item.
Such a cache miss is termed a ``capacity miss'', because it is caused
by the cache's limited capacity.
However, most caches can be forced to eject an old item to make room
for a new item even when they are not yet full.
This is due to the fact that large caches are implemented as hardware
hash tables with fixed-size hash buckets (or ``sets'', as CPU designers
call them) and no chaining, as shown in
\cref{fig:app:whymb:CPU Cache Structure}.

\fi

이 캐쉬는 16개의 ``set'' 과 두개의 ``way'' 를 가져 총 32 개의 ``라인'' 을
가지며, 각 항목은 256 바이트의 정렬된 메모리 블록인 256-바이트 ``캐쉬 라인'' 을
갖습니다.
이 캐쉬 라인 크기는 약간 큰 크기이지만 16 진수 계산을 훨씬 간단하게 해줄
겁니다.
하드웨어 용어에서, 이는 2-way set-associative 캐쉬라 불리며, 16개의 버킷을 갖고
각 버킷의 해쉬 체인은 최대 두개의 원소로 제한된 소프트웨어 해쉬 테이블과
유사합니다.
이 크기 (이 경우 32 캐쉬 라인) 와 associativity (이 경우 2) 는 합쳐져서 이
캐쉬의 ``geometry'' 라고 불립니다.
이 캐쉬는 하드웨어로 구현되었으므로, 그 해쉬 함수는 굉장히 간단합니다:
메모리 주소에서 4개 비트를 꺼냅니다.

\iffalse

This cache has sixteen ``sets'' and two ``ways'' for a total of 32
``lines'', each entry containing a single 256-byte ``cache line'',
which is a 256-byte-aligned block of memory.
This cache line size is a little on the large size, but makes the hexadecimal
arithmetic much simpler.
In hardware parlance, this is a two-way set-associative cache, and
is analogous to a software hash table with
sixteen buckets, where each bucket's hash chain is limited to
at most two elements.
The size (32 cache lines in this case) and the associativity (two in
this case) are collectively called the cache's ``geometry''.
Since this cache is implemented in hardware, the hash function is
extremely simple: extract four bits from the memory address.

\fi

\begin{figure}
\centering
\small
\begin{picture}(170,170)(0,0)

	% Addresses

	\put(0,0){\makebox(20,10){\tt 0xF}}
	\put(0,10){\makebox(20,10){\tt 0xE}}
	\put(0,20){\makebox(20,10){\tt 0xD}}
	\put(0,30){\makebox(20,10){\tt 0xC}}
	\put(0,40){\makebox(20,10){\tt 0xB}}
	\put(0,50){\makebox(20,10){\tt 0xA}}
	\put(0,60){\makebox(20,10){\tt 0x9}}
	\put(0,70){\makebox(20,10){\tt 0x8}}
	\put(0,80){\makebox(20,10){\tt 0x7}}
	\put(0,90){\makebox(20,10){\tt 0x6}}
	\put(0,100){\makebox(20,10){\tt 0x5}}
	\put(0,110){\makebox(20,10){\tt 0x4}}
	\put(0,120){\makebox(20,10){\tt 0x3}}
	\put(0,130){\makebox(20,10){\tt 0x2}}
	\put(0,140){\makebox(20,10){\tt 0x1}}
	\put(0,150){\makebox(20,10){\tt 0x0}}

	% Way 0

	\put(20,163){\makebox(80,10){Way 0}}
	\put(20,0){\framebox(80,10){\tt }}
	\put(20,10){\framebox(80,10){\tt 0x12345E00}}
	\put(20,20){\framebox(80,10){\tt 0x12345D00}}
	\put(20,30){\framebox(80,10){\tt 0x12345C00}}
	\put(20,40){\framebox(80,10){\tt 0x12345B00}}
	\put(20,50){\framebox(80,10){\tt 0x12345A00}}
	\put(20,60){\framebox(80,10){\tt 0x12345900}}
	\put(20,70){\framebox(80,10){\tt 0x12345800}}
	\put(20,80){\framebox(80,10){\tt 0x12345700}}
	\put(20,90){\framebox(80,10){\tt 0x12345600}}
	\put(20,100){\framebox(80,10){\tt 0x12345500}}
	\put(20,110){\framebox(80,10){\tt 0x12345400}}
	\put(20,120){\framebox(80,10){\tt 0x12345300}}
	\put(20,130){\framebox(80,10){\tt 0x12345200}}
	\put(20,140){\framebox(80,10){\tt 0x12345100}}
	\put(20,150){\framebox(80,10){\tt 0x12345000}}

	% Way 1

	\put(100,163){\makebox(80,10){Way 1}}
	\put(100,0){\framebox(80,10){\tt }}
	\put(100,10){\framebox(80,10){\tt 0x43210E00}}
	\put(100,20){\framebox(80,10){\tt }}
	\put(100,30){\framebox(80,10){\tt }}
	\put(100,40){\framebox(80,10){\tt }}
	\put(100,50){\framebox(80,10){\tt }}
	\put(100,60){\framebox(80,10){\tt }}
	\put(100,70){\framebox(80,10){\tt }}
	\put(100,80){\framebox(80,10){\tt }}
	\put(100,90){\framebox(80,10){\tt }}
	\put(100,100){\framebox(80,10){\tt }}
	\put(100,110){\framebox(80,10){\tt }}
	\put(100,120){\framebox(80,10){\tt }}
	\put(100,130){\framebox(80,10){\tt }}
	\put(100,140){\framebox(80,10){\tt }}
	\put(100,150){\framebox(80,10){\tt }}

\end{picture}
\caption{CPU Cache Structure}
\label{fig:app:whymb:CPU Cache Structure}
\end{figure}

\Cref{fig:app:whymb:CPU Cache Structure} 에서, 각 상자는 256-바이트 캐쉬 라인을
가질 수 있는 캐쉬 항목에 연관됩니다.
그러나, 캐쉬 항목은 이 그림의 빈 상자로 표시되어 있듯 비어있을 수 있습니다.
나머지 상자들은 그것들이 가진 캐쉬 라인의 메모리 주소로 표시되어 있습니다.
캐쉬 라인은 256 바이트로 정렬되어 있어야 하므로, 각 주소의 아래쪽 8 비트는 0
이며, 하드웨어 해쉬 함수는 그보다 높은 쪽 4개의 비트를 해쉬 라인 숫자와
매치되게 합니다.

\iffalse

In \cref{fig:app:whymb:CPU Cache Structure},
each box corresponds to a cache entry, which
can contain a 256-byte cache line.
However, a cache entry can be empty, as indicated by the empty boxes
in the figure.
The rest of the boxes are flagged with the memory address of the cache line
that they contain.
Since the cache lines must be 256-byte aligned, the low eight bits of
each address are
zero, and the choice of hardware hash function means that the next-higher
four bits match the hash line number.

\fi

이 그림에 보여진 상황은 프로그램의 코드가 주소 0x43210E00 부터 0x43210EFF 에
위치하고, 이 프로그램이 0x12345000 부터 0x12345EFF 의 데이터를 순차적으로
액세스 했을 때 일어날 수도 있을 겁니다.
이 프로그램이 이제 0x12345F00 을 액세스 하려 한다고 해봅시다.
이 위치는 라인 0xF 로 해쉬 되며, 이 라인의 두 way 는 비어있으므로, 연관된
256-바이트 라인이 수용될 수 있습니다.
이 프로그램이 라인 0x0 으로 해쉬 되는 0x1233000 위치를 접근하려 한다면, 연관된
256-바이트 캐쉬 라인이 way 1 에 수용될 수 있습니다.256-바이트 캐쉬 라인이 way 1
에 수용될 수 있습니다.
그러나, 프로그램이 라인 0xE 로 해쉬되는 0x1233E00 위치를 액세스 하려 하면, 이
새로운 캐쉬 라인을 위한 고간을 만들기 위해 존재하는 라인 중 하나가 버려져야
합니다.
이 버려진 라인이 나중에 다시 액세스 된다면 캐쉬 미스가 일어날 겁니다.
그런 캐쉬 미스는 ``associativity miss'' 라 불립니다.

\iffalse

The situation depicted in the figure might arise if the program's code
were located at address 0x43210E00 through 0x43210EFF, and this program
accessed data sequentially from 0x12345000 through 0x12345EFF\@.
Suppose that the program were now to access location 0x12345F00.
This location hashes to line 0xF, and both ways of this line are
empty, so the corresponding 256-byte line can be accommodated.
If the program were to access location 0x1233000, which hashes to line
0x0, the corresponding 256-byte cache line can be accommodated in
way 1.
However, if the program were to access location 0x1233E00, which hashes
to line 0xE, one of the existing lines must be ejected from the cache
to make room for the new cache line.
If this ejected line were accessed later, a cache miss would result.
Such a cache miss is termed an ``associativity miss''.

\fi

지금까지는 CPU 가 데이터를 읽는 경우만 생각해 봤습니다.
쓰기를 할때는 무슨 일이 일어날까요?
모든 CPU 가 어떤 데이터 항목의 값에 대해 동의한느 것은 중요하므로, 어떤 CPU 는
어떤 데이터 항목에 쓰기를 하기 전에 먼저 그것이 다른 CPU 들의 캐쉬에서
없어지거나 ``무효화 (invalidate)'' 되게 해야만 합니다.
이 무효화가 완료되면, 이 CPU 는 안전하게 데이터 항목을 수정할 수 있습니다.
이 데이터 항목이 이 CPU 의 캐쉬에 존재하지만 읽기 전용이었다면, 이 과정은
``write miss'' 라고 불립니다.
특정 CPU 가 다른 CPU 들의 캐쉬로부터 특정 데이터 항목을 완전히 무효화 하면,
해당 CPU 는 이 데이터 ㅎ아목을 반복적으로 쓸 수 (그리고 읽을 수) 있을 겁니다.

\iffalse

Thus far, we have been considering only cases where a CPU reads
a data item.
What happens when it does a write?
Because it is important that all CPUs agree on the value of a given
data item, before a given CPU writes to that data item, it must first
cause it to be removed, or ``invalidated'', from other CPUs' caches.
Once this invalidation has completed, the CPU may safely modify the
data item.
If the data item was present in this CPU's cache, but was read-only,
this process is termed a ``write miss''.
Once a given CPU has completed invalidating a given data item from other
CPUs' caches, that CPU may repeatedly write (and read) that data item.

\fi

나중에 다른 CPU 들 중 하나가 이 데이터 항목을 접근하려 하면 캐쉬 미스가 일어날
텐데, 이번엔 이 앞의 CPU 가 그 항목에 쓰기를 하기 위해 무효화를 했기
때문입니다.
이런 종류의 캐쉬 미스는 ``communication miss'' 라 불리는데, 이는 보통 CPU 들이
이 데이터 항목을 통신을 위해 사용하기 때문입니다 (예를 들면, 락은 CPU 들 사이에
상호 배타 알고리즘을 통신하기 위해 사용되는 데이터 항목입니다).

분명하게, 모든 CPU 가 데이터의 일관된 시선을 유지하게 보장하는 데에는 큰 주의가
필요합니다.
이 모든 읽기, 무효화, 그리고 쓰기를 놓고 볼 때, 데이터가 손실되거나 (아마도 더
나쁘게도) 다른 CPU 들이 같은 데이터 항목에 대해 그들의 캐쉬에서는 다른 값을
갖고 있는 경우를 상상하기는 쉽습니다.
이 문제들은 ``캐쉬 일관성 프로토콜'' 에 의해 방지되는데, 다음 섹션에서
설명합니다.

\iffalse

Later, if one of the other CPUs attempts to access the data item, it
will incur a cache miss, this time because the first CPU invalidated
the item in order to write to it.
This type of cache miss is termed a ``communication miss'', since it
is usually due to several CPUs using the data items to communicate
(for example, a lock is a data item that is used to communicate among
CPUs using a mutual-exclusion algorithm).

Clearly, much care must be taken to ensure that all CPUs maintain
a coherent view of the data.
With all this fetching, invalidating, and writing, it is easy to
imagine data being lost or (perhaps worse) different CPUs having
conflicting values for the same data item in their respective
caches.
These problems are prevented by ``cache-coherency protocols'',
described in the next section.

\fi

\section{Cache-Coherence Protocols}
\label{sec:app:whymb:Cache-Coherence Protocols}

캐쉬 일관성 프로토콜은 비일관적이거나 손실된 데이터를 막을 수 있게끔 캐쉬 라인
상태들을 관리합니다.
이 프로토콜은 수십개의 상태를 가질 정도로\footnote{
	Culler 등의 책~\cite{DavidECuller1999} 의 670 과 671 페이지에서 각각
	SGI Origin2000 과 Sequent (지금은 IBM) NUMA-Q 를 위한 9개 상태와 26개
	상태 다이어그램들을 보시기 바랍니다.
	두 다이어그램은 실제의 것보다 훨씬 간단합니다.}
매우 복잡할 수 있으나 우리의 목적을 위해선 4개 상태의 MESI 캐쉬 일관성
프로토콜에 대해서만 걱정하면 됩니다.

\iffalse

Cache-coherency protocols manage cache-line states so as to prevent
inconsistent or lost data.
These protocols can be quite complex, with many tens
of states,\footnote{
	See Culler et al.~\cite{DavidECuller1999} pages 670 and 671
	for the nine-state and 26-state diagrams for SGI Origin2000
	and Sequent (now IBM) NUMA-Q, respectively.
	Both diagrams are significantly simpler than real life.}
but for our purposes we need only concern ourselves with the
four-state MESI cache-coherence protocol.

\fi

\subsection{MESI States}
\label{sec:app:whymb:MESI States}

MESI 는 ``modified'', ``exclusive'', ``shared`, 그리고 ``invalid'' 라는 각 캐쉬
라인이 이 프로토콜을 사용해 취할 수 있는 네개의 상태를 의미합니다.
따라서 이 프로토콜을 사용하는 캐쉬는 각 캐쉬 라인에 해당 라인의 물리 주소와
데이터에 더해 2 비트의 상태 ``tag'' 를 유지합니다.

``modified'' 상태의 라인은 연관된 CPU 로부터의 최근의 메모리 스토어를 의미하며,
이 연관된 메모리는 다른 CPU 의 캐쉬에는 보이지 않을 것이 보장됩니다.
따라서 ``modified'' 상태의 캐쉬 라인은 이 CPU 에 ``owned (소유되어 있다)'' 라고
말해집니다.
이 캐쉬는 데이터의 최신 복사본만을 쥐고 있으므로, 이 캐쉬는 궁극적으로는 이를
메모리에 다시 쓰거나 다른 캐쉬에게 넘겨줄 책임을 가지며, 이는 이 라인이 다른
데이터를 잡아두기 위해 재사용되기 전에 행해져야만 합니다.

\iffalse

MESI stands for ``modified'', ``exclusive'', ``shared'', and ``invalid'',
the four states a given cache line can take on using this
protocol.
Caches using this protocol therefore maintain a two-bit state ``tag'' on each
cache line in addition to that line's physical address and data.
% cite Schimmel's book on virtual caches.

A line in the ``modified'' state has been subject to a recent memory store
from the corresponding CPU, and the corresponding memory is guaranteed
not to appear in any other CPU's cache.
Cache lines in the ``modified'' state can thus be said to be ``owned''
by the CPU\@.
Because this cache holds the only up-to-date copy of the data, this
cache is ultimately responsible for either writing it back to memory
or handing it off to some other cache, and must do so before reusing
this line to hold other data.

\fi

``exclusive'' 상태는 ``modified'' 상태와 매우 비슷한데, 한가지 예외는 이 캐쉬
라인은 아직 연관된 CPU 에 의해 수정되지 않았다는 것으로, 이는 결국 메모리에
위치해 있는 이 캐쉬 라인의 데이터의 복사본도 최신의 것이라는 겁니다.
그러나, 이 CPU 는 언제든 이 라인에 스토어를 행할 수 있으므로, 다른 CPU 에게
물어보지 않고서는 ``exclusive'' 상태의 라인은 연관된 CPU 에게 소유되어 있다고
말해질 수 있습니다.
그러나, 메모리의 연관된 값이 최신의 것이기 때문에, 이 캐쉬는 이 데이터를
메모리에 다시 쓰거나 다른 CPU 에게 넘겨주지 않고도 버릴 수 있습니다.

``shared'' 상태의 라인은 최소 하나의 다른 CPU 의 캐쉬에 복사되어 있을 수도
있어서, 이 CPU 는 다른 CPU 에게 자문을 먼저 구하지 않고는 이 라인에 스토어를 할
수 없습니다.
``exclusive'' 상태에서와 마찬가지로, 메모리에 있는 이 연관된 값은 최신의
것이어서, 이 캐쉬는 이 데이터를 메모리에 다시 쓰거나 다른 CPU 에게 넘겨주지
않고도 버릴 수 있습니다.

\iffalse

The ``exclusive'' state is very similar to the ``modified'' state,
the single exception being that the cache line has not yet been
modified by the corresponding CPU, which in turn means that the
copy of the cache line's data that resides in memory is up-to-date.
However, since the CPU can store to this line at any time, without
consulting other CPUs, a line in the ``exclusive'' state can still
be said to be owned by the corresponding CPU\@.
That said, because the corresponding value in memory is up to date,
this cache can discard this data without writing it back to memory
or handing it off to some other CPU\@.

A line in the ``shared'' state might be replicated in at least
one other CPU's cache, so that this CPU is not permitted to store
to the line without first consulting with other CPUs.
As with the ``exclusive'' state, because the corresponding value
in memory is up to date,
this cache can discard this data without writing it back to memory
or handing it off to some other CPU\@.

\fi

``invalid'' 상태의 라인은 비어 있는데, 달리 말해 데이터를 쥐고 있지 않습니다.
캐쉬에 새 데이터가 들어오면, 이는 가능하다면 ``invalid'' 상태의 캐쉬 라인에
위치하게 됩니다.
다른 상태의 라인을 교체하는 것은 교체된 라인이 미래에 참조될 때 비용이 높은
캐쉬 미스를 초래하기 때문에 이 방법이 선호됩니다.

모든 CPU 는 캐쉬 라인들로 운반되는 데이터에 대해 일관된 모습을 봐야만 하므로,
이 캐쉬 일관성 프로토콜은 시스템을 돌아다니는 캐쉬 라인들의 움직임을 조정하는
메세지를 제공합니다.

\iffalse

A line in the ``invalid'' state is empty, in other words, it holds
no data.
When new data enters the cache, it is placed into a
cache line that was in the ``invalid'' state if possible.
This approach is preferred because replacing a line in any other
state could result in an expensive cache miss should the replaced
line be referenced in the future.

Since all CPUs must maintain a coherent view of the data carried in
the cache lines, the cache-coherence protocol provides messages
that coordinate the movement of cache lines through the system.

\fi

\subsection{MESI Protocol Messages}
\label{sec:app:whymb:MESI Protocol Messages}

앞의 섹션에 설명된 많은 변환은 CPU 들 사이의 통신을 필요로 합니다.
CPU 들이 하나의 공유 버스에 있다면, 다음 메세지가 충분합니다:

\iffalse

Many of the transitions described in the previous section require
communication among the CPUs.
If the CPUs are on a single shared bus, the following messages suffice:

\fi

\begin{description}[style=nextline]
\item	[Read:]
	``Read'' 메세지는 읽혀질 캐쉬 라인의 물리 주소를 담습니다.
\item	[Read Response:]
	``Read response'' 메세지는 앞의 ``read'' 메세지에 의해 요청된 데이터를
	담습니다.
	이 ``read response'' 메세지는 메모리 또는 다른 캐쉬들 가운데 하나에
	의해 제공될 수도 있습니다.
	예를 들어, 캐쉬들 가운데 하나가 ``modified'' 상태로 해당 데이터를
	가지고 있다면, 그 캐쉬는 ``read response'' 메세지를 제공해야만 합니다.
\item	[Invalidate:]
	``Invalidate'' 메세지는 무효화 될 캐쉬 라인의 물리 주소를 담습니다.
	모든 다른 캐쉬들은 그들의 캐쉬에서 연관된 데이터를 제거하고 응답해야만
	합니다.
\item	[Invalidate Acknowledge:]
	``Invalidate'' 메세지를 받는 CPU 는 자신의 캐쉬에서 명시된 데이터를
	제거한 후 ``invalidate acknowledge'' 메세지를 가지고 응답해야만 합니다.

\iffalse

\item	[Read:]
	The ``read'' message contains the physical address of the cache line
	to be read.
\item	[Read Response:]
	The ``read response'' message contains the data requested by an
	earlier ``read'' message.
	This ``read response'' message might be supplied either by
	memory or by one of the other caches.
	For example, if one of the caches has the desired data in
	``modified'' state, that cache must supply the ``read response''
	message.
\item	[Invalidate:]
	The ``invalidate'' message contains the physical address of the
	cache line to be invalidated.
	All other caches must remove the corresponding data from their
	caches and respond.
\item	[Invalidate Acknowledge:]
	A CPU receiving an ``invalidate'' message must respond with an
	``invalidate acknowledge'' message after removing the specified
	data from its cache.

\fi

\item	[Read Invalidate:]
	``Read invalidate'' 메세지는 읽혀질 캐쉬 라인의 물리 주소를 담고
	있으며, 동시에 다른 캐쉬들이 이 데이터를 제거하게 지시합니다.
	따라서, 이는 그 이름이 암시하듯 ``read'' 와 ``invalidate'' 의
	조합입니다.
	``Read invalidate'' 메세지는 그에 대한 응답으로 ``read response'' 와
	``invalidate acknowledge'' 메세지의 집합을 모두 요구합니다.
\item	[Writeback:]
	``Writeback'' 메세지는 메모리에 도로 쓰여질 (그리고 아마도 다른 CPU 의
	캐쉬에도 기어들어가게 될--``snooped''--) 주소와 데이터를 포함합니다.
	이 메세지는 캐쉬들이 다른 데이터를 위한 공간을 만들기 위해 ``modified''
	상태의 라인을 제거하는 걸 가능하게 합니다.

\iffalse

\item	[Read Invalidate:]
	The ``read invalidate'' message contains the physical address
	of the cache line to be read, while at the same time directing
	other caches to remove the data.
	Hence, it is a combination of a ``read'' and an ``invalidate'',
	as indicated by its name.
	A ``read invalidate'' message requires both a ``read response''
	and a set of ``invalidate acknowledge'' messages in reply.
\item	[Writeback:]
	The ``writeback'' message contains both the address and the
	data to be written back to memory (and perhaps ``snooped''
	into other CPUs' caches along the way).
	This message permits caches to eject lines in the ``modified''
	state as needed to make room for other data.

\fi

\end{description}

\QuickQuiz{
	Writeback 메세지는 어디서 생겨나서 어디로 향하게 되나요?

	\iffalse

	Where does a writeback message originate from and where does
	it go to?

	\fi

}\QuickQuizAnswer{
	Writeback 메세지는 특정 CPU 에서, 어떤 설계에서는 특정 CPU 의 캐쉬의
	특정 레벨에서---또는 심지어 여러 CPU 들에 공유되어 있는 캐쉬에서도---
	생성됩니다.
	핵심은 특정 캐쉬가 특정 데이터 항목을 위한 공간이 없으며, 따라서 공간을
	만들기 위해 이 캐쉬에서 어떤 데이터 조각들이 제거되야 한다는 겁니다.
	메모리나 다른 캐쉬에 복사되어 있는 데이터 조각이 존재한다면 그 조각은
	writeback 메세지의 필요 없이 단순히 폐기될 수 있을 겁니다.

	다른 한편, 만약 제거될 수 있는 모든 데이터 조각이 modified 상태여서
	유일한 최신 상태의 데이터 복사본은 이 캐쉬에만 있다면, 이 데이터 항목들
	가운데 하나는 어딘가 다른곳에 복사되어야만 합니다.
	이 복사 오퍼레이션은 ``witeback message'' 를 사용해 취해집니다.

	\iffalse

	The writeback message originates from a given CPU, or in some
	designs from a given level of a given CPU's cache---or even
	from a cache that might be shared among several CPUs.
	The key point is that a given cache does not have room for
	a given data item, so some other piece of data must be ejected
	from the cache to make room.
	If there is some other piece of data that is duplicated in some
	other cache or in memory, then that piece of data may be simply
	discarded, with no writeback message required.

	On the other hand, if every piece of data that might be ejected
	has been modified so that the only up-to-date copy is in this
	cache, then one of those data items must be copied somewhere
	else.
	This copy operation is undertaken using a ``writeback message''.

	\fi

	Writeback 메세지의 목적지는 이 새 값을 저장할 어딘가가 됩니다.
	이는 메인 메모리일 수도 있으나, 다른 캐쉬일 수도 있습니다.
	그게 캐쉬라면, 이는 보통 같은 CPU 를 위한 더 높은 레벨의 캐쉬인데, 예를
	들어 level-1 캐쉬는 level-2 캐쉬로 도로 쓰기를 할수도 있습니다.
	그러나, 어떤 하드웨어 설계는 CPU 간 writeback 을 허용해서 CPU~0 의
	캐쉬는 writeback 메세지를 CPU~1 에게 보낼 수도 있습니다.
	이는 일반적으로 예를 들면 최근에 read request 를 보냈다던지 하는 식으로
	CPU~1 이 어떻게든 이 데이터에의 흥미를 알게 된 경우 행해질 겁니다.

	요약하자면, writeback 메세지는 공간이 부족한 시스템의 어느 부분에서
	보내어지며, 이 데이터를 수용할 수 있는 시스템의 어떤 다른 부분에서
	받아지게 됩니다.

	\iffalse

	The destination of the writeback message has to be something
	that is able to store the new value.
	This might be main memory, but it also might be some other cache.
	If it is a cache, it is normally a higher-level cache for the
	same CPU, for example, a level-1 cache might write back to a
	level-2 cache.
	However, some hardware designs permit cross-CPU writebacks,
	so that CPU~0's cache might send a writeback message to CPU~1.
	This would normally be done if CPU~1 had somehow indicated
	an interest in the data, for example, by having recently
	issued a read request.

	In short, a writeback message is sent from some part of the
	system that is short of space, and is received by some other
	part of the system that can accommodate the data.

	\fi

}\QuickQuizEnd

흥미롭게도, 공유 메모리 멀티프로세서 시스템은 장막을 걷어보면 실제로
메세지 전달 (message-passing) 컴퓨터입니다.
이는 분산된 공유 메모리를 사용하는 SMP 머신들의 클러스터는 시스템 구조의 두개의
다른 수준에서의 공유 메모리를 구현하기 위해 메세지 전달을 사용하고 있음을
의미합니다.

\iffalse

Interestingly enough, a shared-memory multiprocessor system really
is a message-passing computer under the covers.
This means that clusters of SMP machines that use distributed shared memory
are using message passing to implement shared memory at two different
levels of the system architecture.

\fi

\QuickQuizSeries{%
\QuickQuizB{
	두개의 CPU 가 동시에 같은 캐쉬 라인을 무효화 하려 하면 어떻게 되나요?

	\iffalse

	What happens if two CPUs attempt to invalidate the
	same cache line concurrently?

	\fi

}\QuickQuizAnswerB{
	해당 CPU 들 가운데 하나가 공유 버스에의 액세스를 먼저 얻고, 그 CPU 가
	``승리'' 합니다.
	다른 CPU 는 자신의 해당 캐쉬 라인 복사본을 무효화 하고 그 다른 CPU 에게
	``invalidate acknowledge'' 메세지를 보내야만 합니다.

	물론, 진 CPU 는 곧바로 ``read invalidate'' 트랜잭션을 요청할 것으로
	예상될 수 있으며, 따라서 승리한 CPU 의 승리는 짧은 것일 겁니다.

	\iffalse

	One of the CPUs gains access
	to the shared bus first,
	and that CPU ``wins''.  The other CPU must invalidate its copy of the
	cache line and transmit an ``invalidate acknowledge'' message
	to the other CPU\@.

	Of course, the losing CPU can be expected to immediately issue a
	``read invalidate'' transaction, so the winning CPU's victory will
	be quite ephemeral.

	\fi

}\QuickQuizEndB
%
\QuickQuizM{
	거대 멀티프로세서에서 ``invalidate'' 메세지가 나타날 때, 모든 CPU 는
	``invalidate acknowledge'' 응답을 보내야만 합니다.
	이로 인한 ``invalidate acknowledge'' 응답의 ``폭풍'' 은 시스템 버스를
	완전히 포화시키지 않을까요?

	\iffalse

	When an ``invalidate'' message appears in a large multiprocessor,
	every CPU must give an ``invalidate acknowledge'' response.
	Wouldn't the resulting ``storm'' of ``invalidate acknowledge''
	responses totally saturate the system bus?

	\fi

}\QuickQuizAnswerM{
	이 거대 규모 멀티프로세서가 실제로 그런 방식으로 구현되어 있다면 그럴
	수도 있습니다.
	특히 NUMA 머신 같은 거대한 멀티프로세서들은 이것을 포함한 문제들을 막기
	위해 ``directory-based'' 캐쉬 일관성 프로토콜이라 불리는 것을
	사용합니다.

	\iffalse

	It might, if large-scale multiprocessors were in fact implemented
	that way.  Larger multiprocessors, particularly NUMA machines,
	tend to use so-called ``directory-based'' cache-coherence
	protocols to avoid this and other problems.

	\fi

}\QuickQuizEndM
%
\QuickQuizE{
	SMP 머신이 정말로 메세지 전달을 어쨌든 사용한다면, 왜 SMP 를 신경쓰죠?

	\iffalse

	If SMP machines are really using message passing
	anyway, why bother with SMP at all?

	\fi

}\QuickQuizAnswerE{
	과거의 수십년간 이 주제에 대한 상당한 논란이 있었습니다.
	한가지 답은 캐쉬 일관성 프로토콜은 상당히 간단하며, 따라서 하드웨어에
	직접 구현될 수 있어서 소프트웨어 메세지 전달로는 얻어질 수 없는
	대역폭과 응답시간 향상을 얻는다는 겁니다.
	또다른 답은 거대 SMP 머신과 작은 SMP 머신의 클러스터의 상대적 가격
	때문에 경제로부터 진실을 찾을 수 있다는 겁니다.
	세번째 답은 SMP 프로그래밍 모델은 분산 시스템의 그것보다 더 사용하기
	쉽다는 것입니다만, 반박론자들은 HPC 클러스터와 MPI 의 등장을 이야기할
	수도 있겠습니다.
	그러므로 토론은 계속됩니다.

	\iffalse

	There has been quite a bit of controversy on this topic over
	the past few decades.  One answer is that the cache-coherence
	protocols are quite simple, and therefore can be implemented
	directly in hardware, gaining bandwidths and latencies
	unattainable by software message passing.  Another answer is that
	the real truth is to be found in economics due to the relative
	prices of large SMP machines and that of clusters of smaller
	SMP machines.  A third answer is that the SMP programming
	model is easier to use than that of distributed systems, but
	a rebuttal might note the appearance of HPC clusters and MPI\@.
	And so the argument continues.

	\fi

}\QuickQuizEndE
}

\subsection{MESI State Diagram}
\label{sec:app:whymb:MESI State Diagram}

특정 캐쉬 라인의 상태는
\cref{fig:app:whymb:MESI Cache-Coherency State Diagram} 에 보인 것처럼 프로토콜
메세지가 보내지고 받아짐에 따라 바뀝니다.

\iffalse

A given cache line's state changes
as protocol messages are sent and received, as
shown in \cref{fig:app:whymb:MESI Cache-Coherency State Diagram}.

\fi

\begin{figure}[htb]
\centering
% \resizebox{3in}{!}{\includegraphics{appendix/whymb/MESI}}
\includegraphics{appendix/whymb/MESI}
\caption{MESI Cache-Coherency State Diagram}
\label{fig:app:whymb:MESI Cache-Coherency State Diagram}
\end{figure}

이 그림에서의 전환 움직임들은 다음과 같습니다:

\iffalse

The transition arcs in this figure are as follows:

\fi

\begin{description}[style=nextline]
\item	[Transition (a):]
	캐쉬 라인이 메모리에 도로 쓰여지나, 이 CPU 는 자신의 캐쉬에 이를
	유지하며 더 나아가 이를 수정할 권리를 유지합니다.
	이 전환은 ``writeback'' 메세지를 필요로 합니다.
\item	[Transition (b):]
	이 CPU 가 이미 배타적 액세스를 가지고 있는 캐쉬 라인에 쓰기를 합니다.
	이 전환은 어떤 메세지도 보내지거나 받아질 필요를 갖지 않습니다.
\item	[Transition (c):]
	이 CPU 가 수정한 캐쉬 라인을 위한 ``read invalidate'' 메세지를
	받습니다.
	이 CPU 는 자신의 지역 복사본을 무효화 하고, 이 데이터를 요청한 CPU 에게
	보내며 자신이 더이상 그 지역 복사본을 가지고 있지 않음을 알리는 ``read
	response'' 와 ``invalidate acknowledge'' 메세지를 응답해야 합니다.

\iffalse

\item	[Transition (a):]
	A cache line is written back to memory, but the CPU retains
	it in its cache and further retains the right to modify it.
	This transition requires a ``writeback'' message.
\item	[Transition (b):]
	The CPU writes to the cache line that it already had exclusive
	access to.
	This transition does not require any messages to be sent or
	received.
\item	[Transition (c):]
	The CPU receives a ``read invalidate'' message for a cache line
	that it has modified.
	The CPU must invalidate its local copy, then respond with both a
	``read response'' and an ``invalidate acknowledge'' message,
	both sending the data to the requesting CPU and indicating
	that it no longer has a local copy.

\fi

\item	[Transition (d):]
	이 CPU 가 자신의 캐쉬에 존재하지 않던 데이터 항목에 대해 어토믹
	read-modify-write 오퍼레이션을 행합니다.
	이는 ``read invalidate'' 를 보내고, ``read response'' 를 통해 그
	데이터를 받습니다.
	이 CPU 는 완전한 ``invalidate acknowledge'' 응답 집합을 받으면 이
	전환을 완료할 수 있습니다.
\item	[Transition (e):]
	이 CPU 가 자신의 캐쉬에 read-only 로 가지고 있던 데이터 항목에 어토믹
	read-modify-write 오퍼레이션을 행합니다.
	이는 ``invalidate'' 메세지를 보내야 하며, 이 전환을 완료하기 전에
	완전한 ``invalidate acknowledge'' 응답 집합을 기다려야 합니다.
\item	[Transition (f):]
	어떤 다른 CPU 가 이 캐쉬 라인을 읽고, 그 데이터가 이 CPU 의 캐쉬에서
	제공되어서, 읽기 전용 복사본을 유지하며, 이를 메모리에 도로 쓰기 할
	수도 있습니다.
	이 전환은 ``read'' 메세지의 도착으로 시작되고, 이 CPU 는 요청된
	데이터를 포함한 ``read response'' 메세지로 응답합니다.

\iffalse

\item	[Transition (d):]
	The CPU does an atomic read-modify-write operation on a data item
	that was not present in its cache.
	It transmits a ``read invalidate'', receiving the data via
	a ``read response''.
	The CPU can complete the transition once it has also received a
	full set of ``invalidate acknowledge'' responses.
\item	[Transition (e):]
	The CPU does an atomic read-modify-write operation on a data item
	that was previously read-only in its cache.
	It must transmit ``invalidate'' messages, and must wait for a
	full set of ``invalidate acknowledge'' responses before completing
	the transition.
\item	[Transition (f):]
	Some other CPU reads the cache line, and it is supplied from
	this CPU's cache, which retains a read-only copy, possibly also
	writing it back to memory.
	This transition is initiated by the reception of a ``read''
	message, and this CPU responds with a ``read response'' message
	containing the requested data.

\fi

\item	[Transition (g):]
	어떤 다른 CPU 가 이 캐쉬 라인의 데이터 항목을 읽고, 그 데이터가 이 CPU
	의 캐쉬 또는 메모리에서 제공됩니다.
	어느 경우든, 이 CPU 는 읽기 전용 복사본을 유지합니다.
	이 전환은 ``read'' 메세지의 도착으로 시작되며, 이 CPU 는 요청된
	데이터를 담은 ``read response'' 메세지를 응답합니다.
\item	[Transition (h):]
	이 CPU 는 이 캐쉬 라인에 어떤 데이터 항목을 곧 써야 할 것임을 깨닫고
	따라서 ``invalidate'' 메세지를 보냅니다.
	이 CPU 는 완전한 ``invalidate acknowledge'' 응답 집합을 받기 전까지는
	이 전환을 끝내지 못합니다.
	대안적으로, 모든 다른 CPU 들이 ``writeback'' 메세지를 통해 이 캐쉬
	라인을 자신들의 캐쉬에서 제거하여 (아마도 다른 캐쉬 라인을 위한 공간을
	만들기 위해) 이 CPU 가 이를 캐쉬에 두는 마지막 CPU 가 되게 할수도
	있습니다.
\item	[Transition (i):]
	어떤 다른 CPU 가 이 CPU 의 캐쉬에만 있는 캐쉬라인의 데이터 항목에 대해
	어토믹 read-modify-write 오퍼레이션을 수행해서, 이 CPU 는 자신의
	캐쉬에서 이 라인을 무효화 합니다.
	이 전환은 ``read invalidate'' 메세지의 도착으로 시작되며, 이 CPU 는
	``read response'' 와 ``invalidate acknowledge'' 메세지를 모두
	응답합니다.

\iffalse

\item	[Transition (g):]
	Some other CPU reads a data item in this cache line,
	and it is supplied either from this CPU's cache or from memory.
	In either case, this CPU retains a read-only copy.
	This transition is initiated by the reception of a ``read''
	message, and this CPU responds with a ``read response'' message
	containing the requested data.
\item	[Transition (h):]
	This CPU realizes that it will soon need to write to some data
	item in this cache line, and thus transmits an ``invalidate'' message.
	The CPU cannot complete the transition until it receives a full
	set of ``invalidate acknowledge'' responses.
	Alternatively, all other CPUs eject this cache line from
	their caches via ``writeback'' messages (presumably to make room
	for other cache lines),
	so that this CPU is the last CPU caching it.
\item	[Transition (i):]
	Some other CPU does an atomic read-modify-write operation on
	a data item in a cache line held only in this CPU's cache,
	so this CPU invalidates it from its cache.
	This transition is initiated by the reception of a ``read invalidate''
	message, and this CPU responds with both a ``read response''
	and an ``invalidate acknowledge'' message.

\fi

\item	[Transition (j):]
	이 CPU 는 자신의 캐쉬에 있지 않던 캐쉬 라인의 데이터 항목에 스토어를
	행하고, 따라서 ``read invalidte'' 메세지를 보냅니다.
	이 CPU 는 ``read response'' 와 완전환 ``invalidate acknowledge'' 메세지
	집합을 받기 전까지 이 전환을 완료할 수 없습니다.
\item	[Transition (k):]
	이 CPU 가 자신의 캐쉬에 없던 캐쉬 라인의 데이터 항목을 로드합니다.
	이 CPU 는 ``read'' 메세지를 보내며, 연관된 ``read response'' 를 받으면
	전환을 완료합니다.
\item	[Transition (l):]
	어떤 다른 CPU 가 이 캐쉬 라인의 데이터 항목으로 스토어를 하지만, 이
	캐쉬 라인은 다른 CPU 의 캐쉬에 (이 현재 CPU 의 캐쉬 같은) 있으므로 이
	캐쉬 라인을 읽기 전용 상태로 유지합니다.
	이 전환은 ``invalidate'' 메세지의 수신으로 시작되며, 이 CPU 는
	``invalidate acknowledge'' 메세지를 응답합니다.

\iffalse

\item	[Transition (j):]
	This CPU does a store to a data item in a cache line that was not
	in its cache, and thus transmits a ``read invalidate'' message.
	The CPU cannot complete the transition until it receives the
	``read response'' and a full set of ``invalidate acknowledge''
	messages.
	The cache line will presumably transition to ``modified'' state via
	transition (b) as soon as the actual store completes.
\item	[Transition (k):]
	This CPU loads a data item in a cache line that was not
	in its cache.
	The CPU transmits a ``read'' message, and completes the
	transition upon receiving the corresponding ``read response''.
\item	[Transition (l):]
	Some other CPU does a store to
	a data item in this cache line, but holds this cache line in read-only
	state due to its being held in other CPUs' caches (such as the
	current CPU's cache).
	This transition is initiated by the reception of an ``invalidate''
	message, and this CPU responds with
	an ``invalidate acknowledge'' message.

\fi

\end{description}

\QuickQuiz{
	하드웨어는 앞서 설명된 지연된 전환을 어떻게 처리하나요?

	\iffalse

	How does the hardware handle the delayed transitions
	described above?

	\fi

}\QuickQuizAnswer{
	일반적으로 상태를 더함으로써 처리합니다만, 이 추가적인 상태는 이 캐쉬
	라인에 정말로 저장될 필요는 없는데, 한번에 몇개의 라인만이 전환되고
	있을 것이라는 사실 덕입니다.
	하지만 전환을 지연해야 하는 필요는 실제 세계의 캐쉬 일관성 프로토콜이
	이 부록에 묘사된 과하게 단순화된 MESI 프로토콜보다 훨씬 더 복잡해지게
	만드는 하나의 문제입니다.
	Hennessy 와 Patterson 의 고전적인 컴퓨터 구조 소개~\cite{Hennessy95a}
	는 이 문제 여럿을 다룹니다.

	\iffalse

	Usually by adding additional states, though these additional
	states need not be actually stored with the cache line, due to
	the fact that only a few lines at a time will be transitioning.
	The need to delay transitions is but one issue that results in
	real-world cache coherence protocols being much more complex than
	the over-simplified MESI protocol described in this appendix.
	Hennessy and Patterson's classic introduction to computer
	architecture~\cite{Hennessy95a} covers many of these issues.

	\fi

}\QuickQuizEnd

\subsection{MESI Protocol Example}
\label{sec:app:whymb:MESI Protocol Example}

이제 초기에는 메모리의 address~0 에 위치해 있는 데이터의 캐쉬 라인이 네개의 CPU
시스템에서의 단일 캐쉬 라인만 갖는 여러 캐쉬들을 이동하는 모습을 캐쉬 라인의
관점에서 봅시다.
\Cref{tab:app:whymb:Cache Coherence Example}
이 이 데이터의 흐름을 보이는데, 첫번째 열은 오퍼레이션의 순서를, 두번째 열은 그
오퍼레이션을 행하는 CPU 를, 세번째 열은 수행되는 오퍼레이션을, 나머지 네개의
열은 각 CPU 의 캐쉬 라인의 상태를 (메모리 주소에 이어 MESI 상태), 그리고 마지막
두개의 열은 연관된 메모리 내용이 최신인지 (``V'') 아닌지 (``I'') 보입니다.

\iffalse

Let's now look at this from the perspective of a cache line's worth
of data, initially residing in memory at address~0,
as it travels through the various single-line direct-mapped caches
in a four-CPU system.
\Cref{tab:app:whymb:Cache Coherence Example}
shows this flow of data, with the first column showing the sequence
of operations, the second the CPU performing the operation,
the third the operation being performed, the next four the state
of each CPU's cache line (memory address followed by MESI state),
and the final two columns whether the corresponding memory contents
are up to date (``V'') or not (``I'').

\fi

처음에는 이 데이터가 위치해 있는 CPU 캐쉬 라인들은 ``invalid'' 상태에 있으며,
그 데이터는 메모리에서 유효합니다.
CPU~0 이 address~0 에서 데이터를 로드하면, 이는 CPU~0 의 캐쉬에 ``shared''
상태로 들어가게 되며, 이는 여전히 메모리에서 유효합니다.
CPU~3  역시 address~0 에서 이 데이터를 로드해서, 두 CPU 의 캐쉬에서 ``shared''
상태로 있게 하며, 이 데이터는 여전히 메모리 상에서 유효합니다.
이어서 CPU~0 이 다른 캐쉬 라인을 (address~8) 로드하는데, 이는 address~0 의
데이터를 무효화를 통해 자신의 캐쉬에서 밖으로 내보내고 address~8 의 데이터로
교체합니다.
CPU~2 가 이제 address~0 으로부터의 로드를 합니다만, 이 CPU 는 자신이 곧 거기에
스토어를 해야 할 것임을 깨달아 배타적 복사본을 얻기 위해 ``read invalidate''
메세지를 사용하여 이를 CPU~3 의 캐쉬에서 무효화 시킵니다 (메모리의 복사본은
여전히 최신의 것으로 남아있지만).
이어서 CPU~2 는 예상된 스토어를 행하여 이 상태를 ``modified'' 로 바꿉니다.
메모리에 있는 이 데이터의 사본은 이제 최신이 아닙니다.
CPU~1 은 원자적 값 증가를 행하는데, CPU~2 의 캐쉬로부터 데이터를 얻고 이를
무효화 하기 위해 ``read invalidate'' 메세지를 사용하며, 따라서 CPU~1 의 캐쉬는
``modified'' 상태가 됩니다 (그리고 메모리의 복사본은 최신이 아닌 상태로
유지됩니다).
마지막으로, CPU~1 이 address~8 의 캐쉬 라인을 읽는데, address~0 의 데이터를
메모리로 도로 내보내기 위해 ``writeback'' 메세지를 사용합니다.

\iffalse

Initially, the CPU cache lines in which the data would reside are
in the ``invalid'' state, and the data is valid in memory.
When CPU~0 loads the data at address~0, it enters the ``shared'' state in
CPU~0's cache, and is still valid in memory.
CPU~3 also loads the data at address~0, so that it is in the
``shared'' state in both CPUs' caches, and is still valid in memory.
Next CPU~0 loads some other cache line (at address~8),
which forces the data at address~0 out of its cache via an invalidation,
replacing it with the data at address~8.
CPU~2 now does a load from address~0, but this CPU realizes that it will
soon need to store to it, and so it uses a ``read invalidate'' message
in order to gain an exclusive copy, invalidating
it from CPU~3's cache (though the copy in memory remains up to date).
Next CPU~2 does its anticipated store, changing the state to ``modified''.
The copy of the data in memory is now out of date.
CPU~1 does an atomic increment, using a ``read invalidate'' to snoop
the data from CPU~2's cache
and invalidate it, so that the copy in CPU~1's cache is in the ``modified''
state (and the copy in memory remains out of date).
Finally, CPU~1 reads the cache line at address~8, which uses a
``writeback'' message to push address~0's data back out to memory.

\fi

\begin{table*}
\small
\centering
\renewcommand*{\arraystretch}{1.2}
\rowcolors{6}{}{lightgray}
\begin{tabular}{rclcccccc}
	\toprule
	& & & \multicolumn{4}{c}{CPU Cache} & \multicolumn{2}{c}{Memory} \\
	\cmidrule(lr){4-7} \cmidrule(l){8-9}
	Sequence \# & CPU \# & Operation & 0 & 1 & 2 & 3 & 0 & 8 \\
	\cmidrule(r){1-3} \cmidrule(lr){4-7} \cmidrule(l){8-9}
%	Seq CPU Operation	------------- CPU -------------   - Memory -
%				   0	   1	   2	   3	    0   8
	0 &   & Initial State	& $-$/I & $-$/I & $-$/I & $-$/I   & V & V \\
	1 & 0 & Load		& 0/S &   $-$/I & $-$/I & $-$/I   & V & V \\
	2 & 3 & Load		& 0/S &   $-$/I & $-$/I & 0/S     & V & V \\
	3 & 0 & Invalidation	& 8/S &   $-$/I & $-$/I & 0/S     & V & V \\
	4 & 2 & RMW		& 8/S &   $-$/I & 0/E &   $-$/I   & V & V \\
	5 & 2 & Store		& 8/S &   $-$/I & 0/M &   $-$/I   & I & V \\
	6 & 1 & Atomic Inc	& 8/S &   0/M &   $-$/I & $-$/I   & I & V \\
	7 & 1 & Writeback	& 8/S &   8/S &   $-$/I & $-$/I   & V & V \\
	\bottomrule
\end{tabular}
\caption{Cache Coherence Example}
\label{tab:app:whymb:Cache Coherence Example}
\end{table*}

우린 데이터가 이 CPU 의 캐쉬들 중 일부에 남겨져 있는채로 이를 끝냄을 알아두시기
바랍니다.

\iffalse

Note that we end with data in some of the CPU's caches.

\fi

\QuickQuiz{
	어떤 순서의 오퍼레이션들이 이 CPU 의 캐쉬들을 ``invalid'' 상태로
	되돌릴까요?

	\iffalse

	What sequence of operations would put the CPUs' caches
	all back into the ``invalid'' state?

	\fi

}\QuickQuizAnswer{
	CPU 의 명령 집합에 특수한 ``내 캐쉬를 비워줘'' 명령이
	없는한 그런 순서의 오퍼레이션 집합은 존재하지 않습니다.
	대부분의 CPU 는 그런 명령을 갖습니다.

	\iffalse

	There is no such sequence, at least in absence of special
	``flush my cache'' instructions in the CPU's instruction set.
	Most CPUs do have such instructions.

	\fi

}\QuickQuizEnd

\section{Stores Result in Unnecessary Stalls}
\label{sec:app:whymb:Stores Result in Unnecessary Stalls}

\Cref{fig:app:whymb:Modern Computer System Cache Structure}
에 보인 캐쉬 구조가 특정 CPU 에서 특정 데이터 항목으로의 반복된 읽기와 쓰기에
대해 좋은 성능을 제공하지만, 해당 캐쉬 라인으로의 첫번째 쓰기는 성능이 상당히
떨어집니다.
이를 이해하기 위해, CPU~0 에 의한 CPU~1 의 캐쉬에 있는 캐쉬 라인으로의
쓰기에서의 시간 흐름을 보이는
\cref{fig:app:whymb:Writes See Unnecessary Stalls} 를 생각해 봅시다.
CPU~0 는 이 캐쉬 라인에 쓰기를 할 수 있게 되기 전에 이 캐쉬 라인이 도착하길
기다려야만 하며, CPU~0 은 연장된 시간 동안 멈춰있어야만 합니다.\footnote{
	한 CPU 의 캐쉬에서 다른 캐쉬로 캐쉬 라인을 이동시키는데 걸리는 시간은
	일반적으로 간단한 레지스터에서 레지스터로의 명령을 수행하는 것보다 수십
	수백배 더 깁니다.}

\iffalse

Although the cache structure shown in
\cref{fig:app:whymb:Modern Computer System Cache Structure}
provides good performance for repeated reads and writes from a given CPU
to a given item of data, its performance for the first write to
a given cache line is quite poor.
To see this, consider
\cref{fig:app:whymb:Writes See Unnecessary Stalls},
which shows a timeline of a write by CPU~0 to a cacheline held in
CPU~1's cache.
Since CPU~0 must wait for the cache line to arrive before it can
write to it, CPU~0 must stall for an extended period of time.\footnote{
	The time required to transfer a cache line from one CPU's cache
	to another's is typically a few orders of magnitude more than
	that required to execute a simple register-to-register instruction.}

\fi

\begin{figure}[htb]
\centering
% \resizebox{3in}{!}{\includegraphics{appendix/whymb/cacheSCwrite}}
\includegraphics{appendix/whymb/cacheSCwrite}
\caption{Writes See Unnecessary Stalls}
\label{fig:app:whymb:Writes See Unnecessary Stalls}
\end{figure}

하지만 CPU~0 이 그렇게 오래 멈춰있게 할 진짜 이유는 없습니다---어쨌건, 어떤
데이터가 CPU~1 이 보내는 캐쉬 라인에 있건, CPU~0 은 무조건적으로 이를 덮어쓸
겁니다.

\iffalse

But there is no real reason to force CPU~0 to stall for so long---after
all, regardless of what data happens to be in the cache line that CPU~1
sends it, CPU~0 is going to unconditionally overwrite it.

\fi

\subsection{Store Buffers}
\label{sec:app:whymb:Store Buffers}

이 불필요한 쓰기에서 멈춰있음을 방지하는 한가지 방법은
\cref{fig:app:whymb:Caches With Store Buffers} 에 보인 것처럼 각 CPU 와 그들의
캐쉬 사이에 ``store buffer (스토어 버퍼)'' 를 두는 겁니다.
이 스토어 버퍼가 추가되면 CPU~0 는 자신의 쓰기를 자신의 스토어 버퍼에 기록하고
수행을 계속할 수 있습니다.
마침내 이 캐쉬 라인이 CPU~1 에서 CPU~0 으로 넘어가게 되면, 이 데이터는 스토어
버퍼에서 이 캐쉬 라인으로 이동하게 됩니다.

\iffalse

One way to prevent this unnecessary stalling of writes is to add
``store buffers'' between each CPU and its cache, as shown in
\cref{fig:app:whymb:Caches With Store Buffers}.
With the addition of these store buffers, CPU~0 can simply record
its write in its store buffer and continue executing.
When the cache line does finally make its way from CPU~1 to CPU~0,
the data will be moved from the store buffer to the cache line.

\fi

\QuickQuiz{
	하지만 스토어 버퍼의 주요 목적이 멀티프로세서 캐쉬 일관성
	프로토콜에서의 응답 지연을 감추기 위함이라면, 단일프로세서는 왜 스토어
	버퍼를 갖죠?

	\iffalse

	But if the main purpose of store buffers is to hide acknowledgment
	latencies in multiprocessor cache-coherence protocols, why
	do uniprocessors also have store buffers?

	\fi

}\QuickQuizAnswer{
	스토어 버퍼의 목적은 멀티프로세서 캐쉬 일관성 프로토콜에서의 응답
	지연을 감추기 위한 것만이 아니라 일반적인 메모리 응답시간을 감추는
	것이기 때문입니다.
	메모리는 유니프로세서에서의 캐쉬보다 훨씬 느리므로, 유니프로세서에서의
	스토어 버퍼는 write-miss 응답시간을 감추는데 도움이 됩니다.

	\iffalse

	Because the purpose of store buffers is not just to hide
	acknowledgement latencies in multiprocessor cache-coherence protocols,
	but to hide memory latencies in general.
	Because memory is much slower than is cache on uniprocessors,
	store buffers on uniprocessors can help to hide write-miss
	latencies.

	\fi

}\QuickQuizEnd

\begin{figure}[htb]
\centering
\resizebox{3in}{!}{\includegraphics{appendix/whymb/cacheSB}}
\caption{Caches With Store Buffers}
\label{fig:app:whymb:Caches With Store Buffers}
\end{figure}

스토어 버퍼는 특정 CPU 에, 또는 하드웨어 멀티쓰레딩에서는 코어에 지역적입니다.
어느 쪽이던, 특정 CPU 는 자신에게 할당된 스토어 버퍼에만 액세스 할 수 있습니다.
예를 들어,
\cref{fig:app:whymb:Caches With Store Buffers} 에서 CPU~0 는 CPU~1 의 스토어
버퍼에 접근할 수 없고 반대도 마찬가지입니다.
이 제한은 문제를 분리함으로써 하드웨어를 단순화 시킵니다:
스토어 버퍼는 연속되는 쓰기를 위한 성능을 개선시키며 CPU 사이의 (또는 코어들
사이의, 경우에 따라) 통신을 위한 책임은 캐쉬 일관성 프로토콜에 의해 처리됩니다.
그러나, 이 제한 아래에서조차 처리되어야만 하는 복잡성이 존재하는데 이는 다음 두
섹션에서 다룹니다.

\iffalse

These store buffers are local to a given CPU or, on systems with
hardware multithreading, local to a given core.
Either way, a given CPU is permitted to access only the store buffer
assigned to it.
For example, in
\cref{fig:app:whymb:Caches With Store Buffers}, CPU~0 cannot
access CPU~1's store buffer and vice versa.
This restriction simplifies the hardware by separating concerns:
The store buffer improves performance for consecutive writes, while
the responsibility for communicating among CPUs (or cores, as the
case may be) is fully shouldered by the cache-coherence protocol.
However, even given this restriction, there are complications that must
be addressed, which are covered in the next two sections.

\fi

\subsection{Store Forwarding}
\label{sec:app:whymb:Store Forwarding}

첫번째 복잡성인 자기 일관성의 위배를 보기 위해 0으로 초기화 되는 변수 \qco{a}
와 \qco{b} 를 갖고 변수 \qco{a} 를 담는 캐쉬 라인은 초기에 CPU~1 에 소유되며
\qco{b} 를 담는 캐쉬라인은 초기에 CPU~0 에 소유되는 다음 코드를 생각해 봅시다:

\iffalse

To see the first complication, a violation of self-consistency,
consider the following code with variables \qco{a} and \qco{b} both initially
zero, and with the cache line containing variable \qco{a} initially
owned by CPU~1 and that containing \qco{b} initially owned by CPU~0:

\fi

\begin{VerbatimN}[fontsize=\footnotesize,samepage=true]
a = 1;
b = a + 1;
assert(b == 2);
\end{VerbatimN}

이 단정은 실패할 거라 예상할 겁니다.
그러나,
\cref{fig:app:whymb:Caches With Store Buffers} 에 보인 매우 단순한 구조를
사용할 만큼 어리석은 사람이 있다면 놀랄 겁니다.
그런 시스템은 다음 이벤트 집합을 볼 잠재성이 있습니다:

\iffalse

One would not expect the assertion to fail.
However, if one were foolish enough to use the very simple architecture
shown in
\cref{fig:app:whymb:Caches With Store Buffers},
one would be surprised.
Such a system could potentially see the following sequence of events:

\fi

\begin{sequence}
\item	CPU~0 이 \co{a = 1} 을 수행합니다.
\item	CPU~0 이 \qco{a} 가 캐쉬에 있는지 확인하고, 그렇지 않음을 발견합니다.
\item	따라서 CPU~0 이 \qco{a} 를 담는 캐쉬 라인의 배타적 소유권을 갖기 위해
	``read invalidate'' 메세지를 보냅니다.
\item	CPU~0 이 \qco{a} 로의 스토어를 스토어 버퍼에 기록합니다.
\item	CPU~1 이 ``read invalidate'' 메세지를 받고, 이 캐쉬 라인을 보내고
	자신의 캐쉬로부터 이 캐쉬라인을 제거함으로써 응답합니다.
\item	CPU~0 이 \co{b = a + 1} 을 수행합니다.
\item	CPU~0 이 CPU~1 로부터 여전히 값이 0인 \qco{a} 를 담은 캐쉬 라인을
	받습니다.
\item	CPU~0 이 자신의 캐쉬에서 \qco{a} 를 로드하고 그 값이 0임을 보게 됩니다.
	\label{item:app:whymb:Need Store Buffer}
\item	CPU~0 이 자신의 스토어 버퍼로부터의 항목을 새로 도착한 캐쉬 라인에
	적용해 자신의 캐쉬의 \qco{a} 의 값을 1 로 만듭니다.
\item	CPU~0 이 앞의 \qco{a} 에서 로드한 값 0에 1 을 더하고 이를 \qco{b} 를
	담는 캐쉬 라인에 (이미 CPU~0 에 소유되어 있다고 가정합니다) 저장합니다.
\item	CPU~0 이 \co{assert(b == 2)} 를 수행하고, 실패합니다.

\iffalse

\item	CPU~0 starts executing the \co{a = 1}.
\item	CPU~0 looks \qco{a} up in the cache, and finds that it is missing.
\item	CPU~0 therefore sends a ``read invalidate'' message in order to
	get exclusive ownership of the cache line containing \qco{a}.
\item	CPU~0 records the store to \qco{a} in its store buffer.
\item	CPU~1 receives the ``read invalidate'' message, and responds
	by transmitting the cache line and removing that cacheline from
	its cache.
\item	CPU~0 starts executing the \co{b = a + 1}.
\item	CPU~0 receives the cache line from CPU~1, which still has
	a value of zero for \qco{a}.
\item	CPU~0 loads \qco{a} from its cache, finding the value zero.
	\label{item:app:whymb:Need Store Buffer}
\item	CPU~0 applies the entry from its store buffer to the newly
	arrived cache line, setting the value of \qco{a} in its cache
	to one.
\item	CPU~0 adds one to the value zero loaded for \qco{a} above,
	and stores it into the cache line containing \qco{b}
	(which we will assume is already owned by CPU~0).
\item	CPU~0 executes \co{assert(b == 2)}, which fails.

\fi

\end{sequence}

문제는 우리가 하나는 캐쉬에 그리고 다른 하나는 스토어 버퍼에, \qco{a} 의 두
복사본을 갖는다는 겁니다.

이 예는 각 CPU 가 자신의 오퍼레이션들을 프로그램 순서대로 행해지는 것으로
본다는 매우 중요한 보장을 깨버립니다.
이 보장을 깨는 것은 소프트웨어 종류에 있어 강력한 반직관이어서 하드웨어
사람들은 이에 공감하고
\cref{fig:app:whymb:Caches With Store Forwarding} 에 보인 것처럼
각 CPU 가 로드할 때 캐쉬만이 아니라 스토어 버퍼도 참조하는 (또는 ``snoop''
하는) ``store forwarding'' 을 구현했습니다.
달리 말하면, 특정 CPU 의 스토어는 뒤따르는 로드에 캐쉬를 거칠 필요 없이 곧바로
전달됩니다.

\iffalse

The problem is that we have two copies of \qco{a}, one in the cache and
the other in the store buffer.

This example breaks a very important guarantee, namely that each CPU
will always see its own operations as if they happened in program order.
Breaking this guarantee is violently counter-intuitive to software types,
so much so
that the hardware guys took pity and implemented ``store forwarding'',
where each CPU refers to (or ``snoops'') its store buffer as well
as its cache when performing loads, as shown in
\cref{fig:app:whymb:Caches With Store Forwarding}.
In other words, a given CPU's stores are directly forwarded to its
subsequent loads, without having to pass through the cache.

\fi

\begin{figure}[htb]
\centering
\resizebox{3in}{!}{\includegraphics{appendix/whymb/cacheSBf}}
\caption{Caches With Store Forwarding}
\label{fig:app:whymb:Caches With Store Forwarding}
\end{figure}

Store forwarding 이 있다면, 앞의 흐름에서의
항목~\ref{item:app:whymb:Need Store Buffer} 은 스토어 버퍼 안의 \qco{a} 의 값 1
을 보고, \qco{b} 의 마지막 값은 사람들이 바라는대로 2 가 될 겁니다.

\iffalse

With store forwarding in place, item~\ref{item:app:whymb:Need Store Buffer}
in the above sequence would have found the correct value of 1 for \qco{a} in
the store buffer, so that the final value of \qco{b} would have been 2,
as one would hope.

\fi

\subsection{Store Buffers and Memory Barriers}
\label{sec:app:whymb:Store Buffers and Memory Barriers}

To see the second complication, a violation of global memory ordering,
consider the following code sequences
with variables \qco{a} and \qco{b} initially zero:

\begin{VerbatimN}[fontsize=\footnotesize,samepage=true]
void foo(void)
{
	a = 1;
	b = 1;
}

void bar(void)
{
	while (b == 0) continue;
	assert(a == 1);
}
\end{VerbatimN}

Suppose CPU~0 executes foo() and CPU~1 executes bar().
Suppose further that the cache line containing \qco{a} resides only in CPU~1's
cache, and that the cache line containing \qco{b} is owned by CPU~0.
Then the sequence of operations might be as follows:
\begin{sequence}
\item	CPU~0 executes \co{a = 1}.  The cache line is not in
	CPU~0's cache, so CPU~0 places the new value of \qco{a} in its
	store buffer and transmits a ``read invalidate'' message.
	\label{seq:app:whymb:Store Buffers and Memory Barriers}
\item	CPU~1 executes \co{while (b == 0) continue}, but the cache line
	containing \qco{b} is not in its cache.
	It therefore transmits a ``read'' message.
\item	CPU~0 executes \co{b = 1}.
	It already owns this cache line (in other words, the cache line
	is already in either the ``modified'' or the ``exclusive'' state),
	so it stores the new value of \qco{b} in its cache line.
\item	CPU~0 receives the ``read'' message, and transmits the
	cache line containing the now-updated value of \qco{b}
	to CPU~1, also marking the line as ``shared'' in its own cache.
\item	CPU~1 receives the cache line containing \qco{b} and installs
	it in its cache.
\item	CPU~1 can now finish executing \co{while (b == 0) continue},
	and since it finds that the value of \qco{b} is 1, it proceeds
	to the next statement.
\item	CPU~1 executes the \co{assert(a == 1)}, and, since CPU~1 is
	working with the old value of \qco{a}, this assertion fails.
\item	CPU~1 receives the ``read invalidate'' message, and
	transmits the cache line containing \qco{a} to CPU~0 and
	invalidates this cache line from its own cache.
	But it is too late.
\item	CPU~0 receives the cache line containing \qco{a} and applies
	the buffered store just in time to fall victim to CPU~1's
	failed assertion.
\end{sequence}

\QuickQuiz{
	In \cref{seq:app:whymb:Store Buffers and Memory Barriers} above,
	why does CPU~0 need to issue a ``read invalidate''
	rather than a simple ``invalidate''?
}\QuickQuizAnswer{
	Because the cache line in question contains more than just the
	variable \co{a}.
}\QuickQuizEnd

The hardware designers cannot help directly here, since the CPUs have
no idea which variables are related, let alone how they might be related.
Therefore, the hardware designers provide memory-barrier instructions
to allow the software to tell the CPU about such relations.
The program fragment must be updated to contain the memory barrier:

\begin{VerbatimN}[fontsize=\footnotesize,samepage=true]
void foo(void)
{
	a = 1;
	smp_mb();
	b = 1;
}

void bar(void)
{
	while (b == 0) continue;
	assert(a == 1);
}
\end{VerbatimN}

The memory barrier \co{smp_mb()} will cause the CPU to flush its store
buffer before applying each subsequent store to its variable's cache line.
The CPU could either simply stall until the store buffer was empty
before proceeding, or it could use the store buffer to hold subsequent
stores until all of the prior entries in the store buffer had been
applied.

With this latter approach the sequence of operations might be as follows:
\begin{sequence}
\item	CPU~0 executes \co{a = 1}.  The cache line is not in
	CPU~0's cache, so CPU~0 places the new value of ``a'' in its
	store buffer and transmits a ``read invalidate'' message.
\item	CPU~1 executes \co{while (b == 0) continue}, but the cache line
	containing ``b'' is not in its cache.
	It therefore transmits a ``read'' message.
\item	CPU~0 executes \co{smp_mb()}, and marks all current store-buffer
	entries (namely, the \co{a = 1}).
\item	CPU~0 executes \co{b = 1}.
	It already owns this cache line (in other words, the cache line
	is already in either the ``modified'' or the ``exclusive'' state),
	but there is a marked entry in the store buffer.
	Therefore, rather than store the new value of ``b'' in the
	cache line, it instead places it in the store buffer (but
	in an \emph{unmarked} entry).
\item	CPU~0 receives the ``read'' message, and transmits the
	cache line containing the original value of ``b''
	to CPU~1.
	It also marks its own copy of this cache line as ``shared''.
\item	CPU~1 receives the cache line containing ``b'' and installs
	it in its cache.
\item	CPU~1 can now load the value of ``b'',
	but since it finds that the value of ``b'' is still 0, it repeats
	the \co{while} statement.
	The new value of ``b'' is safely hidden in CPU~0's store buffer.
\item	CPU~1 receives the ``read invalidate'' message, and
	transmits the cache line containing ``a'' to CPU~0 and
	invalidates this cache line from its own cache.
\item	CPU~0 receives the cache line containing ``a'' and applies
	the buffered store, placing this line into the ``modified''
	state.
\item	Since the store to ``a'' was the only
	entry in the store buffer that was marked by the \co{smp_mb()},
	CPU~0 can also store the new value of ``b''---except for the
	fact that the cache line containing ``b'' is now in ``shared''
	state.
\item	CPU~0 therefore sends an ``invalidate'' message to CPU~1.
\item	CPU~1 receives the ``invalidate'' message, invalidates the
	cache line containing ``b'' from its cache, and sends an
	``acknowledgement'' message to CPU~0.
\item	CPU~1 executes \co{while (b == 0) continue}, but the cache line
	containing ``b'' is not in its cache.
	It therefore transmits a ``read'' message to CPU~0.
\item	CPU~0 receives the ``acknowledgement'' message, and puts
	the cache line containing ``b'' into the ``exclusive'' state.
	CPU~0 now stores the new value of ``b'' into the cache line.
\item	CPU~0 receives the ``read'' message, and transmits the
	cache line containing the new value of ``b''
	to CPU~1.
	It also marks its own copy of this cache line as ``shared''.%
	\label{seq:app:whymb:Store buffers: All copies shared}
\item	CPU~1 receives the cache line containing ``b'' and installs
	it in its cache.
\item	CPU~1 can now load the value of ``b'',
	and since it finds that the value of ``b'' is 1, it
	exits the \co{while} loop and proceeds
	to the next statement.
\item	CPU~1 executes the \co{assert(a == 1)}, but the cache line containing
	``a'' is no longer in its cache.
	Once it gets this cache from CPU~0, it will be
	working with the up-to-date value of ``a'', and the assertion
	therefore passes.
\end{sequence}

\QuickQuiz{
	After \cref{seq:app:whymb:Store buffers: All copies shared}
	in \cref{sec:app:whymb:Store Buffers and Memory Barriers} on
	page~\pageref{seq:app:whymb:Store buffers: All copies shared},
	both CPUs might drop the cache line containing the new value of
	``b''.
	Wouldn't that cause this new value to be lost?
}\QuickQuizAnswer{
	It might, and that is why real hardware takes steps to avoid
	this problem.
	A traditional approach, pointed out by Vasilevsky Alexander,
	is to write this cache line back to main memory before marking
	the cache line as ``shared''.
	A more efficient (though more complex) approach is to use
	additional state to indicate whether or not the cache line
	is ``dirty'', allowing the writeback to happen.
	Year-2000 systems went further, using much more state in order to
	avoid redundant writebacks~\cite[Figure 8.42]{DavidECuller1999}.
	It would be reasonable to assume that complexity has not decreased
	in the meantime.
}\QuickQuizEnd

As you can see, this process involves no small amount of bookkeeping.
Even something intuitively simple, like ``load the value of a'' can
involve lots of complex steps in silicon.

\section{Store Sequences Result in Unnecessary Stalls}
\label{sec:app:whymb:Store Sequences Result in Unnecessary Stalls}

Unfortunately, each store buffer must be relatively small, which means
that a CPU executing a modest sequence of stores can fill its store
buffer (for example, if all of them result in cache misses).
At that point, the CPU must once again wait for invalidations to complete
in order to drain its store buffer before it can continue executing.
This same situation can arise immediately after a memory barrier, when
\emph{all} subsequent store instructions must wait for invalidations to
complete, regardless of whether or not these stores result in cache misses.

This situation can be improved by making invalidate acknowledge
messages arrive more quickly.
One way of accomplishing this is to use per-CPU queues of
invalidate messages, or ``invalidate queues''.

\subsection{Invalidate Queues}
\label{sec:app:whymb:Invalidate Queues}

One reason that invalidate acknowledge messages can take so long
is that they must ensure that the corresponding cache line is
actually invalidated, and this invalidation can be delayed if
the cache is busy, for example, if the CPU is intensively loading
and storing data, all of which resides in the cache.
In addition, if a large number of invalidate messages arrive
in a short time period, a given CPU might fall behind in processing
them, thus possibly stalling all the other CPUs.

However, the CPU need not actually invalidate the cache line
before sending the acknowledgement.
It could instead queue the invalidate message with the understanding
that the message will be processed before the CPU sends any further
messages regarding that cache line.

\subsection{Invalidate Queues and Invalidate Acknowledge}
\label{sec:app:whymb:Invalidate Queues and Invalidate Acknowledge}

\Cref{fig:app:whymb:Caches With Invalidate Queues}
shows a system with invalidate queues.
A CPU with an invalidate queue may acknowledge an invalidate message
as soon as it is placed in the queue, instead of having to wait until
the corresponding line is actually invalidated.
Of course, the CPU must refer to its invalidate queue when preparing
to transmit invalidation messages---if an entry for the corresponding
cache line is in the invalidate queue, the CPU cannot immediately
transmit the invalidate message; it must instead wait until the
invalidate-queue entry has been processed.

\begin{figure}[htb]
\centering
\resizebox{3in}{!}{\includegraphics{appendix/whymb/cacheSBfIQ}}
\caption{Caches With Invalidate Queues}
\label{fig:app:whymb:Caches With Invalidate Queues}
\end{figure}

Placing an entry into the invalidate queue is essentially a promise
by the CPU to process that entry before transmitting any MESI protocol
messages regarding that cache line.
As long as the corresponding data structures are not highly contended,
the CPU will rarely be inconvenienced by such a promise.

However, the fact that invalidate messages can be buffered in the
invalidate queue provides additional opportunity for memory-misordering,
as discussed in the next section.

\subsection{Invalidate Queues and Memory Barriers}
\label{sec:app:whymb:Invalidate Queues and Memory Barriers}

Let us suppose that CPUs queue invalidation requests, but respond to
them immediately.
This approach minimizes the cache-invalidation latency seen by CPUs
doing stores, but can defeat memory barriers, as seen in the following
example.

Suppose the values of ``a'' and ``b'' are initially zero,
that ``a'' is replicated read-only (MESI ``shared'' state),
and that ``b''
is owned by CPU~0 (MESI ``exclusive'' or ``modified'' state).
Then suppose that CPU~0 executes \co{foo()} while CPU~1 executes
function \co{bar()} in the following code fragment:

\begin{fcvlabel}[ln:app:whymb:Breaking mb]
\begin{VerbatimN}[fontsize=\footnotesize,samepage=true,commandchars=\\\[\]]
void foo(void)
{
	a = 1;
	smp_mb();	\lnlbl[mb]
	b = 1;
}

void bar(void)
{
	while (b == 0) continue;
	assert(a == 1);
}
\end{VerbatimN}
\end{fcvlabel}

Then the sequence of operations might be as follows:
\begin{fcvref}[ln:app:whymb:Breaking mb]
\begin{sequence}
\item	CPU~0 executes \co{a = 1}.  The corresponding
	cache line is read-only in
	CPU~0's cache, so CPU~0 places the new value of ``a'' in its
	store buffer and transmits an ``invalidate'' message in order
	to flush the corresponding cache line from CPU~1's cache.
	\label{seq:app:whymb:Invalidate Queues and Memory Barriers}
\item	CPU~1 executes \co{while (b == 0) continue}, but the cache line
	containing ``b'' is not in its cache.
	It therefore transmits a ``read'' message.
\item	CPU~1 receives CPU~0's ``invalidate'' message, queues it, and
	immediately responds to it.
\item	CPU~0 receives the response from CPU~1, and is therefore free
	to proceed past the \co{smp_mb()} on \clnref{mb} above, moving
	the value of ``a'' from its store buffer to its cache line.
\item	CPU~0 executes \co{b = 1}.
	It already owns this cache line (in other words, the cache line
	is already in either the ``modified'' or the ``exclusive'' state),
	so it stores the new value of ``b'' in its cache line.
\item	CPU~0 receives the ``read'' message, and transmits the
	cache line containing the now-updated value of ``b''
	to CPU~1, also marking the line as ``shared'' in its own cache.
\item	CPU~1 receives the cache line containing ``b'' and installs
	it in its cache.
\item	CPU~1 can now finish executing \co{while (b == 0) continue},
	and since it finds that the value of ``b'' is 1, it proceeds
	to the next statement.
\item	CPU~1 executes the \co{assert(a == 1)}, and, since the
	old value of ``a'' is still in CPU~1's cache,
	this assertion fails.
\item	Despite the assertion failure, CPU~1 processes the queued
	``invalidate'' message, and (tardily)
	invalidates the cache line containing ``a'' from its own cache.
\end{sequence}
\end{fcvref}

\QuickQuiz{
	In \cref{seq:app:whymb:Invalidate Queues and Memory Barriers}
	of the first scenario in
	\cref{sec:app:whymb:Invalidate Queues and Memory Barriers},
	why is an ``invalidate'' sent instead of a ''read invalidate''
	message?
	Doesn't CPU~0 need the values of the other variables that share
	this cache line with ``a''?
}\QuickQuizAnswer{
	CPU~0 already has the values of these variables, given that it
	has a read-only copy of the cache line containing ``a''.
	Therefore, all CPU~0 need do is to cause the other CPUs to discard
	their copies of this cache line.
	An ``invalidate'' message therefore suffices.
}\QuickQuizEnd

There is clearly not much point in accelerating invalidation responses
if doing so causes memory barriers to effectively be ignored.
However, the memory-barrier instructions can interact with
the invalidate queue, so that when a given CPU executes a memory
barrier, it marks all the entries currently in its invalidate queue,
and forces any subsequent load to wait until all marked entries
have been applied to the CPU's cache.
Therefore, we can add a memory barrier to function \co{bar} as follows:

\begin{fcvlabel}[ln:app:whymb:Add mb]
\begin{VerbatimN}[fontsize=\footnotesize,samepage=true,commandchars=\\\[\]]
void foo(void)
{
	a = 1;
	smp_mb();		\lnlbl[mb1]
	b = 1;
}

void bar(void)
{
	while (b == 0) continue;
	smp_mb();
	assert(a == 1);
}
\end{VerbatimN}
\end{fcvlabel}

\QuickQuiz{
	Say what???
	Why do we need a memory barrier here, given that the CPU cannot
	possibly execute the \co{assert()} until after the
	\co{while} loop completes?
}\QuickQuizAnswer{
	Suppose that memory barrier was omitted.

	Keep in mind that CPUs are free to speculatively execute later
	loads, which can have the effect of executing the assertion
	before the \co{while} loop completes.
	Furthermore, compilers assume that only the currently executing
	thread is updating the variables, and this assumption allows
	the compiler to hoist the load of \co{a} to precede the
	loop.

	In fact, some compilers would transform the loop to a branch
	around an infinite loop as follows:

\begin{VerbatimN}[fontsize=\footnotesize,samepage=true]
void foo(void)
{
	a = 1;
	smp_mb();
	b = 1;
}

void bar(void)
{
	if (b == 0)
		for (;;)
			continue;
	assert(a == 1);
}
\end{VerbatimN}

	Given this optimization, the code would behave in a completely
	different way than the original code.
	If \co{bar()} observed \qco{b == 0}, the assertion could of
	course not be reached at all due to the infinite loop.
	However, if \co{bar()} loaded the value \qco{1} just as
	\qco{foo()} stored it, the CPU might still have the old
	zero value of \qco{a} in its cache, which would cause
	the assertion to fire.
	You should of course use volatile casts (for example, those
	volatile casts implied by the C11 relaxed atomic load operation)
	to prevent the compiler from optimizing your parallel code
	into oblivion.
	But volatile casts would not prevent a weakly ordered CPU
	from loading the old value for \qco{a} from its cache, which
	means that this code also requires the explicit memory barrier
	in \qco{bar()}.

	In short, both compilers and CPUs aggressively apply
	code-reordering optimizations, so you must clearly communicate
	your constraints using the compiler directives and memory barriers
	provided for this purpose.
%
}\QuickQuizEnd

\begin{fcvref}[ln:app:whymb:Add mb]
With this change, the sequence of operations might be as follows:
\begin{sequence}
\item	CPU~0 executes \co{a = 1}.  The corresponding
	cache line is read-only in
	CPU~0's cache, so CPU~0 places the new value of ``a'' in its
	store buffer and transmits an ``invalidate'' message in order
	to flush the corresponding cache line from CPU~1's cache.
\item	CPU~1 executes \co{while (b == 0) continue}, but the cache line
	containing ``b'' is not in its cache.
	It therefore transmits a ``read'' message.
\item	CPU~1 receives CPU~0's ``invalidate'' message, queues it, and
	immediately responds to it.
\item	CPU~0 receives the response from CPU~1, and is therefore free
	to proceed past the \co{smp_mb()} on \clnref{mb1} above, moving
	the value of ``a'' from its store buffer to its cache line.
\item	CPU~0 executes \co{b = 1}.
	It already owns this cache line (in other words, the cache line
	is already in either the ``modified'' or the ``exclusive'' state),
	so it stores the new value of ``b'' in its cache line.
\item	CPU~0 receives the ``read'' message, and transmits the
	cache line containing the now-updated value of ``b''
	to CPU~1, also marking the line as ``shared'' in its own cache.
\item	CPU~1 receives the cache line containing ``b'' and installs
	it in its cache.
\item	CPU~1 can now finish executing \co{while (b == 0) continue},
	and since it finds that the value of ``b'' is 1, it proceeds
	to the next statement, which is now a memory barrier.
\item	CPU~1 must now stall until it processes all pre-existing
	messages in its invalidation queue.
\item	CPU~1 now processes the queued
	``invalidate'' message, and
	invalidates the cache line containing ``a'' from its own cache.
\item	CPU~1 executes the \co{assert(a == 1)}, and, since the
	cache line containing ``a'' is no longer in CPU~1's cache,
	it transmits a ``read'' message.
\item	CPU~0 responds to this ``read'' message with the cache line
	containing the new value of ``a''.
\item	CPU~1 receives this cache line, which contains a value of 1 for
	``a'', so that the assertion does not trigger.
\end{sequence}
\end{fcvref}

With much passing of MESI messages, the CPUs arrive at the correct answer.
This section illustrates why CPU designers must be extremely careful
with their cache-coherence optimizations.

\section{Read and Write Memory Barriers}
\label{sec:app:whymb:Read and Write Memory Barriers}

In the previous section, memory barriers were used to mark entries in
both the store buffer and the invalidate queue.
But in our code fragment, \co{foo()} had no reason to do anything
with the invalidate queue, and \co{bar()} similarly had no reason
to do anything with the store buffer.

Many CPU architectures therefore provide weaker memory-barrier
instructions that do only one or the other of these two.
Roughly speaking, a ``read memory barrier'' marks only the invalidate
queue and a ``write memory barrier'' marks only the store buffer,
while a full-fledged memory barrier does both.

The effect of this is that a read memory barrier orders only loads
on the CPU that executes it, so that all loads preceding the read memory
barrier will appear to have completed before any load following the
read memory barrier.
Similarly, a write memory barrier orders
only stores, again on the CPU that executes it, and again so that
all stores preceding the write memory barrier will appear to have
completed before any store following the write memory barrier.
A full-fledged memory barrier orders both loads and stores, but again
only on the CPU executing the memory barrier.

If we update \co{foo} and \co{bar} to use read and write memory
barriers, they appear as follows:

\begin{VerbatimN}[fontsize=\footnotesize,samepage=true]
void foo(void)
{
	a = 1;
	smp_wmb();
	b = 1;
}

void bar(void)
{
	while (b == 0) continue;
	smp_rmb();
	assert(a == 1);
}
\end{VerbatimN}

Some computers have even more flavors of memory barriers, but
understanding these three variants will provide a good introduction
to memory barriers in general.

\section{Example Memory-Barrier Sequences}
\label{sec:app:whymb:Example Memory-Barrier Sequences}

This section presents some seductive but subtly broken uses of
memory barriers.
Although many of them will work most of the time, and some will
work all the time on some specific CPUs, these uses must be avoided
if the goal is to produce code that works reliably on all CPUs.
To help us better see the subtle breakage, we first need to focus
on an ordering-hostile architecture.

\subsection{Ordering-Hostile Architecture}
\label{sec:app:whymb:Ordering-Hostile Architecture}

A number of ordering-hostile computer systems have been produced over
the decades,
but the nature of the hostility has always been extremely subtle,
and understanding it has required detailed knowledge of the specific
hardware.
Rather than picking on a specific hardware vendor, and as a presumably
attractive alternative to dragging the reader through detailed
technical specifications, let us instead design a mythical but maximally
memory-ordering-hostile computer architecture.\footnote{
	Readers preferring a detailed look at real hardware
	architectures are encouraged to consult CPU vendors'
	manuals~\cite{ALPHA95,AMDOpteron02,IntelItanium02v2,PowerPC94,MichaelLyons05a,SPARC94,IntelXeonV3-96a,IntelXeonV2b-96a,IBMzSeries04a},
	Gharachorloo's dissertation~\cite{Gharachorloo95},
	Peter Sewell's work~\cite{PeterSewell2021weakmemory}, or
	the excellent hardware-oriented primer by
	Sorin, Hill, and Wood~\cite{DanielJSorin2011MemModel}.}

This hardware must obey the following ordering
constraints~\cite{PaulMcKenney2005i,PaulMcKenney2005j}:
\begin{enumerate}
\item	Each CPU will always perceive its own memory accesses
	as occurring in program order.
\item	CPUs will reorder a given operation with a store only
	if the two operations are referencing different locations.
\item	All of a given CPU's loads preceding a read memory barrier
	(\co{smp_rmb()}) will be perceived by all CPUs to precede
	any loads following that read memory barrier.
\item	All of a given CPU's stores preceding a write memory barrier
	(\co{smp_wmb()}) will be perceived by all CPUs to precede
	any stores following that write memory barrier.
\item	All of a given CPU's accesses (loads and stores) preceding a
	full memory barrier
	(\co{smp_mb()}) will be perceived by all CPUs to precede
	any accesses following that memory barrier.
\end{enumerate}

\QuickQuiz{
	Does the guarantee that each CPU sees its own memory accesses
	in order also guarantee that each user-level thread will see
	its own memory accesses in order?
	Why or why not?
}\QuickQuizAnswer{
	No.  Consider the case where a thread migrates from one CPU to
	another, and where the destination CPU perceives the source
	CPU's recent memory operations out of order.  To preserve
	user-mode sanity, kernel hackers must use memory barriers in
	the context-switch path.  However, the locking already required
	to safely do a context switch should automatically provide
	the memory barriers needed to cause the user-level task to see
	its own accesses in order.  That said, if you are designing a
	super-optimized scheduler, either in the kernel or at user level,
	please keep this scenario in mind!
}\QuickQuizEnd

Imagine a large non-uniform cache architecture (NUCA) system that,
in order to provide fair allocation
of interconnect bandwidth to CPUs in a given node, provided per-CPU
queues in each node's interconnect interface, as shown in
\cref{fig:app:whymb:Example Ordering-Hostile Architecture}.
Although a given CPU's accesses are ordered as specified by memory
barriers executed by that CPU, however, the relative order of a
given pair of CPUs' accesses could be severely reordered,
as we will see.\footnote{
	Any real hardware architect or designer will no doubt be
	objecting strenuously,
	as they just might be a bit upset about the prospect of working
	out which queue should handle a message involving a cache line
	that both CPUs accessed, to say nothing of the many races that
	this example poses.
	All I can say is ``Give me a better example''.}

\begin{figure}[htb]
\centering
\resizebox{3in}{!}{\includegraphics{appendix/whymb/hostileordering}}
\caption{Example Ordering-Hostile Architecture}
\label{fig:app:whymb:Example Ordering-Hostile Architecture}
\end{figure}

\subsection{Example 1}
\label{sec:app:whymb:Example 1}

\Cref{lst:app:whymb:Memory Barrier Example 1}
shows three code fragments, executed concurrently by CPUs~0, 1, and 2.
Each of ``a'', ``b'', and ``c'' are initially zero.

\floatstyle{plaintop}
\restylefloat{listing}

\begin{listing}
\scriptsize
\centering{\tt
\begin{tabular}{l|l|l}
	\multicolumn{1}{c|}{\nf{CPU~0}} &
		\multicolumn{1}{c|}{\nf{CPU~1}} &
			\multicolumn{1}{c}{\nf{CPU~2}} \\
	\hline
	\hline
	a = 1;		 &		& \\
	\tco{smp_wmb();} & while (b == 0); & \\
	b = 1;		 & c = 1;	& z = c; \\
			 &		& \tco{smp_rmb();} \\
			 &		& x = a; \\
			 &		& assert(z == 0 || x == 1); \\
\end{tabular}}
\caption{Memory Barrier Example 1}
\label{lst:app:whymb:Memory Barrier Example 1}
\end{listing}

Suppose CPU~0 recently experienced many cache misses, so that its
message queue is full, but that CPU~1 has been running exclusively within
the cache, so that its message queue is empty.
Then CPU~0's assignment to ``a'' and ``b'' will appear in Node~0's cache
immediately (and thus be visible to CPU~1), but will be blocked behind
CPU~0's prior traffic.
In contrast, CPU~1's assignment to ``c'' will sail through CPU~1's
previously empty queue.
Therefore, CPU~2 might well see CPU~1's assignment to ``c'' before
it sees CPU~0's assignment to ``a'', causing the assertion to fire,
despite the memory barriers.

Therefore, portable code cannot rely on this assertion not firing,
as both the compiler and the CPU can reorder the code so as to trip
the assertion.

\QuickQuiz{
	Could this code be fixed by inserting a memory barrier
	between CPU~1's ``while'' and assignment to ``c''?
	Why or why not?
}\QuickQuizAnswer{
	No.  Such a memory barrier would only force ordering local to CPU~1.
	It would have no effect on the relative ordering of CPU~0's and
	CPU~1's accesses, so the assertion could still fail.
	However, all mainstream computer systems provide one mechanism
	or another to provide ``transitivity'', which provides
	intuitive causal ordering: if B saw the effects of A's accesses,
	and C saw the effects of B's accesses, then C must also see
	the effects of A's accesses.
	In short, hardware designers have taken at least a little pity
	on software developers.
}\QuickQuizEnd

\subsection{Example 2}
\label{sec:app:whymb:Example 2}

\Cref{lst:app:whymb:Memory Barrier Example 2}
shows three code fragments, executed concurrently by CPUs~0, 1, and 2.
Both ``a'' and ``b'' are initially zero.

\begin{listing}
\scriptsize
\centering{\tt
\begin{tabular}{l|l|l}
	\multicolumn{1}{c|}{\nf{CPU~0}} &
		\multicolumn{1}{c|}{\nf{CPU~1}} &
			\multicolumn{1}{c}{\nf{CPU~2}} \\
	\hline
	\hline
	a = 1;	     & while (a == 0); & \\
		     & \tco{smp_mb();}	& y = b; \\
		     & b = 1;		& \tco{smp_rmb();} \\
		     &			& x = a; \\
		     &			& assert(y == 0 || x == 1); \\
\end{tabular}}
\caption{Memory Barrier Example 2}
\label{lst:app:whymb:Memory Barrier Example 2}
\end{listing}

Again, suppose CPU~0 recently experienced many cache misses, so that its
message queue is full, but that CPU~1 has been running exclusively within
the cache, so that its message queue is empty.
Then CPU~0's assignment to ``a'' will appear in Node~0's cache
immediately (and thus be visible to CPU~1), but will be blocked behind
CPU~0's prior traffic.
In contrast, CPU~1's assignment to ``b'' will sail through CPU~1's
previously empty queue.
Therefore, CPU~2 might well see CPU~1's assignment to ``b'' before
it sees CPU~0's assignment to ``a'', causing the assertion to fire,
despite the memory barriers.

In theory, portable code should not rely on this example code fragment,
however, as before, in practice it actually does work on most
mainstream computer systems.

\subsection{Example 3}
\label{sec:app:whymb:Example 3}

\Cref{lst:app:whymb:Memory Barrier Example 3}
shows three code fragments, executed concurrently by CPUs~0, 1, and 2.
All variables are initially zero.

\begin{listing*}
\scriptsize
\centering{\tt
\begin{tabular}{r|l|l|l}
	& \multicolumn{1}{c|}{\nf{CPU~0}} &
		\multicolumn{1}{c|}{\nf{CPU~1}} &
			\multicolumn{1}{c}{\nf{CPU~2}} \\
	\hline
	\hline
 1 &	a = 1; &			& \\
 2 &	\tco{smp_wmb();}&		& \\
 3 &	b = 1;		& while (b == 0); & while (b == 0); \\
 4 &			& \tco{smp_mb();}& \tco{smp_mb();} \\
 5 &			& c = 1;	& d = 1; \\
 6 &	while (c == 0); &		& \\
 7 &	while (d == 0); &		& \\
 8 &	\tco{smp_mb();}	&		& \\
 9 &	e = 1; &			& assert(e == 0 || a == 1); \\
\end{tabular}}
\caption{Memory Barrier Example 3}
\label{lst:app:whymb:Memory Barrier Example 3}
\end{listing*}

\floatstyle{ruled}
\restylefloat{listing}

Note that neither CPU~1 nor CPU~2 can proceed to line~5 until they see
CPU~0's assignment to ``b'' on line~3.
Once CPU~1 and~2 have executed their memory barriers on line~4, they
are both guaranteed to see all assignments by CPU~0 preceding its memory
barrier on line~2.
Similarly, CPU~0's memory barrier on line~8 pairs with those of CPUs~1 and~2
on line~4, so that CPU~0 will not execute the assignment to ``e'' on
line~9 until after its assignment to ``b'' is visible to both of the
other CPUs.
Therefore, CPU~2's assertion on line~9 is guaranteed \emph{not} to fire.

\QuickQuizSeries{%
\QuickQuizB{
	Suppose that lines~3--5 for CPUs~1 and~2 in
	\cref{lst:app:whymb:Memory Barrier Example 3}
	are in an interrupt
	handler, and that the CPU~2's line~9 runs at process level.
	In other words, the code in all three columns of the table
	runs on the same CPU, but the first two columns run in an
	interrupt handler, and the third column runs at process
	level, so that the code in third column can be interrupted
	by the code in the first two columns.
	What changes, if any, are required to enable the code to work
	correctly, in other words, to prevent the assertion from firing?
}\QuickQuizAnswerB{
	The assertion must ensure that the load of
	``e'' precedes that of ``a''.
	In the Linux kernel, the \co{barrier()} primitive may be used to
	accomplish this in much the same way that the memory barrier was
	used in the assertions in the previous examples.
	For example, the assertion can be modified as follows:

\begin{VerbatimU}[fontsize=\footnotesize]
r1 = e;
barrier();
assert(r1 == 0 || a == 1);
\end{VerbatimU}

	No changes are needed to the code in the first two columns,
	because interrupt handlers run atomically from the perspective
	of the interrupted code.
}\QuickQuizEndB
%
\QuickQuizE{
	If CPU~2 executed an \co{assert(e==0||c==1)} in the example in
	\cref{lst:app:whymb:Memory Barrier Example 3},
	would this assert ever trigger?
}\QuickQuizAnswerE{
	The result depends on whether the CPU supports ``transitivity''.
	In other words, CPU~0 stored to ``e'' after seeing CPU~1's
	store to ``c'', with a memory barrier between CPU~0's load
	from ``c'' and store to ``e''.
	If some other CPU sees CPU~0's store to ``e'', is it also
	guaranteed to see CPU~1's store?

	All CPUs I am aware of claim to provide transitivity.
}\QuickQuizEndE
}

The Linux kernel's \co{synchronize_rcu()} primitive uses an algorithm
similar to that shown in this example.

\section{Are Memory Barriers Forever?}
\label{sec:app:whymb:Are Memory Barriers Forever?}

There have been a number of recent systems that are significantly less
aggressive about out-of-order execution in general and re-ordering
memory references in particular.
Will this trend continue to the point where memory barriers are a thing
of the past?

The argument in favor would cite proposed massively multi-threaded hardware
architectures, so that each thread would wait until memory was ready,
with tens, hundreds, or even thousands of other threads making progress
in the meantime.
In such an architecture, there would be no need for memory barriers,
because a given thread would simply wait for all outstanding operations
to complete before proceeding to the next instruction.
Because there would be potentially thousands of other threads, the
CPU would be completely utilized, so no CPU time would be wasted.

The argument against would cite the extremely limited number of applications
capable of scaling up to a thousand threads, as well as increasingly
severe realtime requirements, which are in the tens of microseconds
for some applications.
The realtime-response requirements are difficult enough to meet as is,
and would be even more difficult to meet given the extremely low
single-threaded throughput implied by the massive multi-threaded
scenarios.

Another argument in favor would cite increasingly sophisticated
latency-hiding hardware implementation techniques that might well allow
the CPU to provide the illusion of fully sequentially consistent
execution while still providing almost all of the performance advantages
of out-of-order execution.
A counter-argument would cite the increasingly severe power-efficiency
requirements presented both by battery-operated devices and by
environmental responsibility.

Who is right?
We have no clue, so we are preparing to live with either scenario.

\section{Advice to Hardware Designers}
\label{sec:app:whymb:Advice to Hardware Designers}

There are any number of things that hardware designers can do
to make the lives of software people difficult.
Here is a list of a few such things that we have encountered in
the past, presented here in the hope that it might help prevent
future such problems:
\begin{enumerate}
\item	I/O devices that ignore cache coherence.

	This charming misfeature can result in DMAs from memory
	missing recent changes to the output buffer, or, just as
	bad, cause input buffers to be overwritten by the contents
	of CPU caches just after the DMA completes.
	To make your system work in face of such misbehavior,
	you must carefully flush the CPU caches of any location
	in any DMA buffer before presenting that buffer to the
	I/O device.
	Similarly, you need to flush the CPU caches of any location
	in any DMA buffer after DMA to that buffer completes.
	And even then, you need to be \emph{very} careful to avoid
	pointer bugs, as even a misplaced read to an input buffer
	can result in corrupting the data input!

\item	External busses that fail to transmit cache-coherence data.

	This is an even more painful variant of the above problem,
	but causes groups of devices---and even memory itself---to
	fail to respect cache coherence.
	It is my painful duty to inform you that as embedded systems
	move to multicore architectures, we will no doubt see a fair
	number of such problems arise.
	By the year 2021, there were some efforts to address
	these problems with new interconnect standards, with some
	debate as to how effective these standards will really
	be~\cite{WilliamGWong2019CCIX-CXL}.

\item	Device interrupts that ignore cache coherence.

	This might sound innocent enough---after all, interrupts
	aren't memory references, are they?
	But imagine a CPU with a split cache, one bank of which is
	extremely busy, therefore holding onto the last cacheline
	of the input buffer.
	If the corresponding I/O-complete interrupt reaches this
	CPU, then that CPU's memory reference to the last cache
	line of the buffer could return old data, again resulting
	in data corruption, but in a form that will be invisible
	in a later crash dump.
	By the time the system gets around to dumping the offending
	input buffer, the DMA will most likely have completed.

\item	Inter-processor interrupts (IPIs) that ignore cache coherence.

	This can be problematic if the IPI reaches its destination
	before all of the cache lines in the corresponding message
	buffer have been committed to memory.

\item	Context switches that get ahead of cache coherence.

	If memory accesses can complete too wildly out of order,
	then context switches can be quite harrowing.
	If the task flits from one CPU to another before all the
	memory accesses visible to the source CPU make it to the
	destination CPU, then the task could easily see the corresponding
	variables revert to prior values, which can fatally confuse
	most algorithms.

\item	Overly kind simulators and emulators.

	It is difficult to write simulators or emulators that force
	memory re-ordering, so software that runs just fine in
	these environments can get a nasty surprise when it first
	runs on the real hardware.
	Unfortunately, it is still the rule that the hardware is more
	devious than are the simulators and emulators, but we hope that
	this situation changes.
\end{enumerate}

Again, we encourage hardware designers to avoid these practices!

\QuickQuizAnswersChp{qqzwhymb}
