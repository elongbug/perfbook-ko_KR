% appendix/questions/concurrentparallel.tex
% mainfile: ../../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\section{What is the Difference Between ``Concurrent'' and ``Parallel''?}
\label{sec:app:questions:What is the Difference Between ``Concurrent'' and ``Parallel''?}

고전적 컴퓨팅 관점에서, ``동시 (concurrent)'' 와 ``병렬 (parallel)'' 은 분명
유의어입니다.
그러나, 이는 많은 사람들이 그 둘 사이의 차이를 그리는 걸 멈추게 하지 않았으며,
이 차이는 두개의 다른 관점에서 이해될 수 있는 것으로 드러났습니다.

첫번째 관점은 ``병렬'' 을 ``데이터 병렬'' 의 약자로 여기고 ``동시'' 를 그 외의
모든것으로 여기게 하는 겁니다.
이 관점에서, 병렬 검퓨팅에서 전체 문제의 각 부분은 다른 부분과의 소통 없이
완전히 독립적으로 진행될 수 있습니다.
이 경우, 부분들간의 조정은 조금만, 또는 아예 요구되지 않을 수 있습니다.
대조적으로, 동시 컴퓨팅은 경쟁되는 락, 트랜잭션, 또는 다른 동기화 메커니즘등의
형태로 더 상호 의존성을 가질 수도 있습니다.

\iffalse

From a classic computing perspective, ``concurrent'' and ``parallel''
are clearly synonyms.
However, this has not stopped many people from drawing distinctions
between the two, and it turns out that these distinctions can be
understood from a couple of different perspectives.

The first perspective treats ``parallel'' as an abbreviation for
``data parallel'', and treats ``concurrent'' as pretty much everything
else.
From this perspective, in parallel computing, each partition of the
overall problem can proceed completely independently, with no
communication with other partitions.
In this case, little or no coordination among partitions is required.
In contrast, concurrent computing might well have tight interdependencies,
in the form of contended locks, transactions, or other synchronization
mechanisms.

\fi

\QuickQuiz{
	프로그램의 한 부분이 RCU read-side 기능을 유일한 동기화 메커니즘으로
	사용한다고 해봅시다.
	이는 병렬성입니까 동시성입니까?

	\iffalse

	Suppose a portion of a program uses RCU read-side primitives
	as its only synchronization mechanism.
	Is this parallelism or concurrency?

	\fi

}\QuickQuizAnswer{
	그렇습니다.

	\iffalse

	Yes.

	\fi

}\QuickQuizEnd

이는 또한 왜 그런 차이가 중요한지 질문을 던지게 하는데, 이는 아랫단의
스케쥴러에 적용되는 두번째 관점을 가져오게 합니다.
스케쥴러는 상당한 복잡도와 능력을 가지는데, 대략적인 경험적 규칙으로, 병렬
프로세스들 여럿이 더 긴밀하고 비정규적으로 통신할수록, 스케쥴러에 더 높은
수준의 정교성이 필요시 됩니다.
따라서, 병렬 컴퓨팅의 상호의존성 제거는 병렬 컴퓨팅 프로그램이 가장 단순한
스케쥴러에서도 잘 동작함을 의미합니다.
실제로, 순수한 병렬 컴퓨팅 프로그램은 임의로 쪼개지고 단일 프로세스에
섞여지더라도 성공적으로 수행될 수 있습니다.\footnote{
	그래요, 이는 데이터 병렬 컴퓨팅 프로그램은 순차적 수행에 잘 맞음을
	의미합니다.
	왜 물어보시죠?}
반대로, 동시 컴퓨팅 프로그램은 스케쥴러의 그 부분에 상당한 미묘한 트릭을 필요로
하게 할 수 있습니다.

\iffalse

This of course begs the question of why such a distinction matters,
which brings us to the second perspective, that of the underlying scheduler.
Schedulers come in a wide range of complexities and capabilities, and
as a rough rule of thumb, the more tightly and irregularly a set of
parallel processes communicate, the higher the level of sophistication
required from the scheduler.
As such, parallel computing's avoidance of interdependencies means that
parallel-computing programs run well on the least-capable schedulers.
In fact, a pure parallel-computing program can run successfully after
being arbitrarily subdivided and interleaved onto a uniprocessor.\footnote{
	Yes, this does mean that data-parallel-computing programs are
	best-suited for sequential execution.
	Why did you ask?}
In contrast, concurrent-computing programs might well require extreme
subtlety on the part of the scheduler.

\fi

One could argue that we should simply demand a reasonable level of
competence from the scheduler, so that we could simply ignore any
distinctions between parallelism and concurrency.
Although this is often a good strategy,
there are important situations where efficiency,
performance, and scalability concerns sharply limit the level
of competence that the scheduler can reasonably offer.
One important example is when the scheduler is implemented in
hardware, as it often is in SIMD units or GPGPUs.
Another example is a workload where the units of work are quite
short, so that even a software-based scheduler must make hard choices
between subtlety on the one hand and efficiency on the other.

Now, this second perspective can be thought of as making the workload
match the available scheduler, with parallel workloads able to
use simple schedulers and concurrent workloads requiring
sophisticated schedulers.

Unfortunately, this perspective does not always align with the
dependency-based distinction put forth by the first perspective.
For example, a highly interdependent lock-based workload
with one thread per CPU can make do with a trivial scheduler
because no scheduler decisions are required.
In fact, some workloads of this type can even be run one after another
on a sequential machine.
Therefore, such a workload would be labeled ``concurrent'' by the first
perspective and ``parallel'' by many taking the second perspective.

\QuickQuiz{
	In what part of the second (scheduler-based) perspective would
	the lock-based single-thread-per-CPU workload be considered
	``concurrent''?
}\QuickQuizAnswer{
	The people who would like to arbitrarily subdivide and interleave
	the workload.
	Of course, an arbitrary subdivision might end up separating
	a lock acquisition from the corresponding lock release, which
	would prevent any other thread from acquiring that lock.
	If the locks were pure spinlocks, this could even result in
	deadlock.
}\QuickQuizEnd

Which is just fine.
No rule that humankind writes carries any weight against the objective
universe, not even rules dividing multiprocessor programs into categories
such as ``concurrent'' and ``parallel''.

This categorization failure does not mean such rules are useless,
but rather that you should take on a suitably skeptical frame of mind when
attempting to apply them to new situations.
As always, use such rules where they apply and ignore them otherwise.

In fact, it is likely that new categories will arise in addition
to parallel, concurrent, map-reduce, task-based, and so on.
Some will stand the test of time, but good luck guessing which!
