% cpu/overheads.tex

\section{Overheads}
\label{sec:cpu:Overheads}

이 섹션에서는 앞의 섹션에서 나열했던 각 장애들의 실제 성능 오버헤드를 보입니다.
하지만, 그 전에 하드웨어 시스템 아키텍쳐에 대해서 간략히 알아보는게 필요합니다.
다음 섹션에서 이를 다룹니다.

\iffalse
This section presents actual overheads of the obstacles to performance
listed out in the previous section.
However, it is first necessary to get a rough view of hardware system
architecture, which is the subject of the next section.
\fi

\subsection{Hardware System Architecture}
\label{sec:cpu:Hardware System Architecture}

\begin{figure}[tb]
\begin{center}
\resizebox{3in}{!}{\includegraphics{cpu/SystemArch}}
\end{center}
\caption{System Hardware Architecture}
\label{fig:cpu:System Hardware Architecture}
\end{figure}

Figure~\ref{fig:cpu:System Hardware Architecture} 는 8 코어 컴퓨터 시스템의
간략한 구조를 보여줍니다.
각 다이는 CPU 코더 한쌍을 가지고 있고, 각 코어는 캐쉬를 갖고, 각 캐쉬는 쌍을
이루는 코어와 통신을 할 수 있도록 하는 연결이 되어 있습니다.
그림 중간의 system interconnect 는 네개의 다이가 통신할 수 있도록 하고, 메인
메모리로 각 다이를 연결합니다.

이 시스템에서 데이터는 하나의 2의 승수로 정렬된 메모리의 블락들로, 보통 32 에서
256 바이트 사이인 ``캐쉬 라인'' 단위로 움직입니다.
한 CPU 는 메모리로부터 자신의 레지스터로 변수를 가져오려면, 그 변수를 가지고
있는 캐쉬 라인을 자신의 캐쉬로 가져와야 합니다.
비슷하게, 한 CPU 는 자신의 레지스터로부터 메모리로 어떤 값을 쓰려 할 때, 일단
그 변수를 담고 있는 캐쉬 라인을 자신의 캐쉬로 가져와야 합니다만, 또한 다른 CPU
가 그 캐쉬 라인의 카피를 들고 있지 않음을 보장해야 합니다.

예를 들어, 만약 CPU~0 CPU~7 의 캐쉬에 있는 캐쉬라인에 있는 변수에
compare-and-swap (CAS) 오퍼레이션을 하려 하면, 다음의 간략화한 이벤트들이
일어날 겁니다:

\iffalse
Figure~\ref{fig:cpu:System Hardware Architecture}
shows a rough schematic of an eight-core computer system.
Each die has a pair of CPU cores, each with its cache, as well as an
interconnect allowing the pair of CPUs to communicate with each other.
The system interconnect in the middle of the diagram allows the
four dies to communicate, and also connects them to main memory.

Data moves through this system in units of ``cache lines'', which
are power-of-two fixed-size aligned blocks of memory, usually ranging
from 32 to 256 bytes in size.
When a CPU loads a variable from memory to one of its registers, it must
first load the cacheline containing that variable into its cache.
Similarly, when a CPU stores a value from one of its registers into
memory, it must also load the cacheline containing that variable into
its cache, but must also ensure that no other CPU has a copy of that
cacheline.

For example, if CPU~0 were to perform a compare-and-swap (CAS) operation on a
variable whose cacheline resided in CPU~7's cache, the following
over-simplified sequence of events might ensue:
\fi

\begin{enumerate}
\item	CPU~0 는 자신의 로컬 캐쉬를 확인하고, 그 캐쉬라인이 없음을 발견합니다.
\item	요청은 CPU~0 와 CPU~1 사이의 연결로 넘겨져서 CPU~1 의 로컬 캐쉬를
	확인해봅니다만, 역시 해당 캐쉬라인이 없음을 발견합니다.
\item	요청은 시스템 연결부로 넘겨지고, 다른 세개의 다이들을 체크합니다. 결과,
	해당 캐쉬라인은 CPU~6과 CPU~7 이 위치한 다이에 있음을 확인합니다.
\item	요청은 CPU~6 과 CPU~7 연결부로 넘겨져 각 CPU 의 캐쉬를 확인해 해당 캐쉬
	라인이 CPU~7 의 캐쉬에 있음을 발견합니다.
\item	CPU~7 은 해당 캐쉬라인을 자신의 연결부로 넘기고, 자신의 캐쉬에서 해당
	캐쉬라인을 비워버립니다.
\item	CPU~6 와 CPU~7 의 연결부는 해당 캐쉬 라인을 시스템 연결부로 넘깁니다.
\item	시스템 연결부는 해당 캐쉬 라인을 CPU~0 과 CPU~1 연결부로 넘깁니다.
\item	CPU~0 과 CPU~1 연결부는 해당 캐쉬라인을 CPU~0 의 캐쉬로 보냅니다.
\item	CPU~0 는 이제 자신의 캐쉬 안에 있는 변수에 CAS 오퍼레이션을 수행할 수
	있습니다.
\end{enumerate}

\iffalse
\begin{enumerate}
\item	CPU~0 checks its local cache, and does not find the cacheline.
\item	The request is forwarded to CPU~0's and 1's interconnect,
	which checks CPU~1's local cache, and does not find the cacheline.
\item	The request is forwarded to the system interconnect, which
	checks with the other three dies, learning that the cacheline
	is held by the die containing CPU~6 and 7.
\item	The request is forwarded to CPU~6's and 7's interconnect, which
	checks both CPUs' caches, finding the value in CPU~7's cache.
\item	CPU~7 forwards the cacheline to its interconnect, and also
	flushes the cacheline from its cache.
\item	CPU~6's and 7's interconnect forwards the cacheline to the
	system interconnect.
\item	The system interconnect forwards the cacheline to CPU~0's and 1's
	interconnect.
\item	CPU~0's and 1's interconnect forwards the cacheline to CPU~0's
	cache.
\item	CPU~0 can now perform the CAS operation on the value in its cache.
\end{enumerate}
\fi

\QuickQuiz{}
	이제 \emph{간략화된} 거라구요?
	이것보다 더 복잡한게 어떻게 \emph{가능하죠}?
	\iffalse
	This is a \emph{simplified} sequence of events?
	How could it \emph{possibly} be any more complex?
	\fi
\QuickQuizAnswer{
	이 예는 다음을 포함해 몇가지 가능한 복잡한 경우를 뺐습니다:
	\begin{enumerate}
	\item	해당 캐쉬라인에 대해 다른 CPU 들도 동사에 CAS 오퍼레이션을
		수행하려 하고 있을 수 있습니다.
	\item	해당 캐쉬라인은 리드 온리로 다른 CPU 들의 캐쉬들에 복사되어
		있을 수 있는데, 이 경우엔 그 캐쉬들도 비워야 할 필요가
		생깁니다.
	\item	CPU~7 은 해당 요청이 도착했을 때 해당 캐쉬 라인에 뭔가 연산을
		수행하고 있었을 수 있고, 이 경우 CPU~7 은 자신의 연산이 끝날
		때까지 해당 요청을 잠시 대기하고 있게 해야 합니다.
	\item	CPU~7 은 (예를 들어, 다른 데이터를 위한 공간을 만들기 위해)
		해당 캐쉬라인을 캐쉬에서 없앴을 수 있고, 이로 인해 요청이
		도착한 시점에서는 캐쉬라인이 메모리에 있을 수 있습니다.
	\item	캐쉬라인에서 고칠 수 있는 에러가 났을 수 있는데, 그렇다면 해당
		데이터가 사용되기 전에 그 에러는 고쳐져야 합니다.
	\iffalse
	This sequence ignored a number of possible complications,
	including:

	\begin{enumerate}
	\item	Other CPUs might be concurrently attempting to perform
		CAS operations involving this same cacheline.
	\item	The cacheline might have been replicated read-only in
		several CPUs' caches, in which case, it would need to
		be flushed from their caches.
	\item	CPU~7 might have been operating on the cache line when
		the request for it arrived, in which case CPU~7 might
		need to hold off the request until its own operation
		completed.
	\item	CPU~7 might have ejected the cacheline from its cache
		(for example, in order to make room for other data),
		so that by the time that the request arrived, the
		cacheline was on its way to memory.
	\item	A correctable error might have occurred in the cacheline,
		which would then need to be corrected at some point before
		the data was used.
	\fi
	\end{enumerate}

	제품 품질의 캐쉬 일관성 메커니즘들은 이런 종류의 여러 복잡한
	경우~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}
	때문에 엄청나게 복잡합니다.

	\iffalse
	Production-quality cache-coherence mechanisms are extremely
	complicated due to these sorts of
	considerations~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}.
	\fi

} \QuickQuizEnd

\QuickQuiz{}
	왜 CPU~7 의 캐쉬에서 해당 캐쉬라인을 비워야 하죠?

	\iffalse
	Why is it necessary to flush the cacheline from CPU~7's cache?
	\fi
\QuickQuizAnswer{
	만약 해당 캐쉬라인이 CPU~7 의 캐쉬에서 비워지지 않는다면, CPU~0 과
	CPU~7 은 같은 변수에 대해 서로 다른 값을 보게 될 겁니다.
	이런 종류의 비일관성은 병렬 소프트웨어를 매우 복잡하게 만들 수 있고,
	때문에 하드웨어 설계자들은 그런 문제를 없애려 노력해 왔습니다.
	\iffalse
	If the cacheline was not flushed from CPU~7's cache, then
	CPUs~0 and 7 might have different values for the same set
	of variables in the cacheline.
	This sort of incoherence would greatly complicate parallel
	software, and so hardware architects have been convinced to
	avoid it.
	\fi
} \QuickQuizEnd

이 간략화된 시나리오는 \emph{캐쉬 코히런시
프로토콜}~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}
의 시작일 뿐입니다.

\iffalse
This simplified sequence is just the beginning of a discipline called
\emph{cache-coherency protocols}~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}.
\fi

\subsection{Costs of Operations}
\label{sec:cpu:Costs of Operations}

\begin{table}
\centering
\begin{tabular}{l||r|r}
				& 	 	& Ratio \\
	Operation		& Cost (ns) 	& (cost/clock) \\
	\hline
	\hline
	Clock period		&           0.6	&           1.0 \\
	\hline
	Best-case CAS		&          37.9	&          63.2 \\
	\hline
	Best-case lock		&          65.6	&         109.3 \\
	\hline
	Single cache miss	&         139.5	&         232.5 \\
	\hline
	CAS cache miss		&         306.0	&         510.0 \\
	\hline
	Comms Fabric		&       3,000	&       5,000 \\
	\hline
	Global Comms		& 130,000,000	& 216,000,000 \\
\end{tabular}
\caption{Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}
\label{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}
\end{table}

The overheads of some common operations important to parallel programs are
displayed in
Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}.
This system's clock period rounds to 0.6ns.
Although it is not unusual for modern microprocessors to be able to
retire multiple instructions per clock period, the operations's costs are
nevertheless normalized to a clock period in the third column, labeled
``Ratio''.
The first thing to note about this table is the large values of many of
the ratios.

The best-case compare-and-swap (CAS) operation consumes almost forty
nanoseconds, a duration more than sixty times that of the clock period.
Here, ``best case'' means that the same CPU now performing the CAS operation
on a given variable was the last CPU to operate on this variable, so
that the corresponding cache line is already held in that CPU's cache.
Similarly, the best-case lock operation (a ``round trip'' pair consisting
of a lock acquisition followed by a lock release) consumes more than
sixty nanoseconds, or more than one hundred clock cycles.
Again, ``best case'' means that the data structure representing the
lock is already in the cache belonging to the CPU acquiring and releasing
the lock.
The lock operation is more expensive than CAS because it requires two
atomic operations on the lock data structure.

An operation that misses the cache consumes almost one hundred and forty
nanoseconds, or more than two hundred clock cycles.
The code used for this cache-miss measurement passes the cache line
back and forth between a pair of CPUs, so this cache miss is satisfied
not from memory, but rather from the other CPU's cache.
A CAS operation, which must look at the old value of the variable as
well as store a new value, consumes over three hundred nanoseconds, or
more than five hundred clock cycles.
Think about this a bit.
In the time required to do \emph{one} CAS operation, the CPU could have
executed more than \emph{five hundred} normal instructions.
This should demonstrate the limitations not only of fine-grained locking,
but of any other synchronization mechanism relying on fine-grained
global agreement.

\QuickQuiz{}
	Surely the hardware designers could be persuaded to improve
	this situation!
	Why have they been content with such abysmal performance
	for these single-instruction operations?
\QuickQuizAnswer{
	The hardware designers \emph{have} been working on this
	problem, and have consulted with no less a luminary than
	the physicist Stephen Hawking.
	Hawking's observation was that the hardware designers have
	two basic problems~\cite{BryanGardiner2007}:

	\begin{enumerate}
	\item	the finite speed of light, and
	\item	the atomic nature of matter.
	\end{enumerate}

\begin{table}
\centering
\begin{tabular}{l||r|r}
				& 	 	& Ratio \\
	Operation		& Cost (ns) 	& (cost/clock) \\
	\hline
	\hline
	Clock period		&           0.4	&           1.0 \\
	\hline
	``Best-case'' CAS	&          12.2	&          33.8 \\
	\hline
	Best-case lock		&          25.6	&          71.2 \\
	\hline
	Single cache miss	&          12.9	&          35.8 \\
	\hline
	CAS cache miss		&           7.0	&          19.4 \\
	\hline
	Off-Core		&		&		\\
	\hline
	Single cache miss	&          31.2	&          86.6 \\
	\hline
	CAS cache miss		&          31.2	&          86.5 \\
	\hline
	Off-Socket		&		&		\\
	\hline
	Single cache miss	&          92.4	&         256.7 \\
	\hline
	CAS cache miss		&          95.9	&         266.4 \\
	\hline
	Comms Fabric		&       4,500	&       7,500 \\
	\hline
	Global Comms		& 195,000,000	& 324,000,000 \\
\end{tabular}
\caption{Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
\label{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
\end{table}

	The first problem limits raw speed, and the second limits
	miniaturization, which in turn limits frequency.
	And even this sidesteps the power-consumption issue that
	is currently holding production frequencies to well below
	10 GHz.

	Nevertheless, some progress is being made, as may be seen
	by comparing
	Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
	with
	Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}
	on
	page~\pageref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}.
	Integration of hardware threads in a single core and multiple
	cores on a die have improved latencies greatly, at least within the
	confines of a single core or single die.
	There has been some improvement in overall system latency,
	but only by about a factor of two.
	Unfortunately, neither the speed of light nor the atomic nature
	of matter has changed much in the past few years.

	Section~\ref{sec:cpu:Hardware Free Lunch?}
	looks at what else hardware designers might be
	able to do to ease the plight of parallel programmers.
} \QuickQuizEnd

I/O operations are even more expensive.
As shown in the ``Comms Fabric'' row,
high performance (and expensive!) communications fabric, such as
InfiniBand or any number of proprietary interconnects, has a latency
of roughly three microseconds, during which time five \emph{thousand}
instructions might have been executed.
Standards-based communications networks often require some sort of
protocol processing, which further increases the latency.
Of course, geographic distance also increases latency, with the
theoretical speed-of-light latency around the world coming to
roughly 130 \emph{milliseconds}, or more than 200 million clock
cycles, as shown in the ``Global Comms'' row.

\QuickQuiz{}
	These numbers are insanely large!
	How can I possibly get my head around them?
\QuickQuizAnswer{
	Get a roll of toilet paper.
	In the USA, each roll will normally have somewhere around 350-500
	sheets.
	Tear off one sheet to represent a single clock cycle, setting it aside.
	Now unroll the rest of the roll.

	The resulting pile of toilet paper will likely represent a single
	CAS cache miss.

	For the more-expensive inter-system communications latencies,
	use several rolls (or multiple cases) of toilet paper to represent
	the communications latency.

	Important safety tip: make sure to account for the needs of
	those you live with when appropriating toilet paper!
} \QuickQuizEnd

\begin{figure}[tb]
\begin{center}
\resizebox{3in}{!}{\includegraphics{cartoons/Data-chasing-light-wave}}
\end{center}
\caption{Hardware and Software: On Same Side}
\ContributedBy{Figure}{fig:cpu:Hardware and Software: On Same Side}{Melissa Broussard}
\end{figure}

In short, hardware and software engineers are really fighting on the same
side, trying to make computers go fast despite the best efforts of the
laws of physics, as fancifully depicted in
Figure~\ref{fig:cpu:Hardware and Software: On Same Side}
where our data stream is trying its best to exceed the speed of light.
The next section discusses some of the things that the hardware engineers
might (or might not) be able to do.
Software's contribution to this fight is outlined in the remaining chapters
of this book.
