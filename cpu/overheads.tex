% cpu/overheads.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\section{Overheads}
\label{sec:cpu:Overheads}
%
\epigraph{Don't design bridges in ignorance of materials, and don't design
	  low-level software in ignorance of the underlying hardware.}
	 {\emph{Unknown}}

이 섹션은 앞의 섹션에 소개된 성능 장애물들의 실제 오버헤드를 보입니다.
하지만, 먼저 하드웨어 시스템 구조의 대략적인 모습을 알아볼 필요가 있는데, 이를
다음 섹션에서 다룹니다.

\iffalse

This section presents actual overheads of the obstacles to performance
listed out in the previous section.
However, it is first necessary to get a rough view of hardware system
architecture, which is the subject of the next section.

\fi

\subsection{Hardware System Architecture}
\label{sec:cpu:Hardware System Architecture}

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{cpu/SystemArch}}
\caption{System Hardware Architecture}
\label{fig:cpu:System Hardware Architecture}
\end{figure}

Figure~\ref{fig:cpu:System Hardware Architecture}
는 여덟개 코어를 가진 컴퓨터 시스템의 대략적 모습을 보입니다.
각 다이는 한쌍의 CPU 코어를 가지는데, 각 코어는 캐쉬를 가지며, 이 한쌍의
CPU들이 서로간에 통신을 할 수 있게 하는 인터컨넥트 (interconnect) 역시
있습니다.
시스템 인터컨넥트 (system interconnect) 는 이 네개의 다이들이 서로간에, 그리고
메인 메모리와 통신할 수
있게 합니다.

데이터는 ``캐쉬 라인'' 단위로 이 시스템 내에서 이동하는데, 이는 2의 n승의
고정된 크기로 정렬된 메모리 블록이며, 일반적으로 그 크기는 32 에서 256 바이트
사이입니다.
CPU 가 메모리로부터 레지스터로 변수를 로드할 때에는 그 변수를 담고 있는
캐쉬라인을 자신의 캐쉬로 먼저 읽어와야 합니다.
비슷하게, CPU 가 변수를 자신의 레지스터로부터 메모리로 저장할 때에도 해당
변수를 담고 있는 캐쉬라인을 자신의 캐쉬로 읽어와야 합니다만, 또한 다른 CPU 가
그 캐쉬라인의 복사본을 가지고 있지 않음을 보장해야 합니다.

예를 들어, CPU~0 이 CPU~7 의 캐쉬에 있는 캐쉬라인의 변수에 쓰기를 하려 했다면,
다음과 같은 무척 단순화된 일들이 순차적으로 벌어질 수 있습니다:

\iffalse

Figure~\ref{fig:cpu:System Hardware Architecture}
shows a rough schematic of an eight-core computer system.
Each die has a pair of CPU cores, each with its cache, as well as an
interconnect allowing the pair of CPUs to communicate with each other.
The system interconnect allows the four dies to communicate with each
other and with main memory.

Data moves through this system in units of ``cache lines'', which
are power-of-two fixed-size aligned blocks of memory, usually ranging
from 32 to 256 bytes in size.
When a CPU loads a variable from memory to one of its registers, it must
first load the cacheline containing that variable into its cache.
Similarly, when a CPU stores a value from one of its registers into
memory, it must also load the cacheline containing that variable into
its cache, but must also ensure that no other CPU has a copy of that
cacheline.

For example, if CPU~0 were to write to a variable whose cacheline
resided in CPU~7's cache, the following over-simplified sequence of
events might ensue:

\fi

\begin{enumerate}
\item	CPU~0 이 자신의 캐쉬를 검사하고, 해당 캐쉬라인이 거기 없음을
	알아차립니다.
	따라서 이 쓰기를 자신의 스토어 버퍼 (store buffer) 에 기록합니다.
\item	이 캐쉬라인에 대한 요청이 CPU~0 과~1 의 인터컨넥트로 전달되는데, 이
	요청은 CPU~1 의 캐쉬를 검사하고 거기서도 이 캐쉬라인을 찾지 못합니다.
\item	이 요청은 이제 시스템 인터컨넥트로 전달되어, 다른 세개의 다이를 모두
	검사하고, 이 캐쉬라인이 CPU~6 와~7 을 가지고 있는 다이 안에 있음을 알게
	됩니다.
\item	이 요청은 이제 CPU~6 과~7 의 인터컨넥트로 전달되어, 두 CPU 의 캐쉬를
	검사하고, 이 값이 CPU~7 의 캐쉬에 있음을 발견하게 됩니다.
\item	CPU~7 은 이 캐쉬라인을 자신의 인터컨넥트로 전달하고, 자신의 캐쉬로부터
	이 캐쉬라인을 제거합니다.
\item	CPU~6 과~7 의 인터컨넥트는 이 캐쉬라인을 시스템 인터컨넥트로
	전달합니다.
\item	시스템 인터컨넥트는 이 캐쉬라인을 CPU~0 과~1 의 인터컨넥트로
	전달합니다.
\item	CPU~0 과~1 의 인터컨넥트는 이 캐쉬라인을 CPU~0 의 캐쉬로 전달합니다.
\item	CPU~0 은 이제 이 쓰기를 완료하고, 새로 도착한 이 캐쉬라인의 연관된
	부분을 스토어 버퍼에 기록된 기존 값으로부터 업데이트합니다.

\iffalse

\item	CPU~0 checks its local cache, and does not find the cacheline.
	It therefore records the write in its store buffer.
\item	A request for this cacheline is forwarded to CPU~0's and~1's
	interconnect, which checks CPU~1's local cache, and does not
	find the cacheline.
\item	This request is forwarded to the system interconnect, which
	checks with the other three dies, learning that the cacheline
	is held by the die containing CPU~6 and~7.
\item	This request is forwarded to CPU~6's and~7's interconnect, which
	checks both CPUs' caches, finding the value in CPU~7's cache.
\item	CPU~7 forwards the cacheline to its interconnect, and also
	flushes the cacheline from its cache.
\item	CPU~6's and~7's interconnect forwards the cacheline to the
	system interconnect.
\item	The system interconnect forwards the cacheline to CPU~0's and~1's
	interconnect.
\item	CPU~0's and~1's interconnect forwards the cacheline to CPU~0's
	cache.
\item	CPU~0 can now complete the write, updating the relevant portions
	of the newly arrived cacheline from the value previously
	recorded in the store buffer.

\fi

\end{enumerate}

\QuickQuizSeries{%
\QuickQuizB{
	이게 \emph{단순화된} 거라구요?
	어떻게 이보다 더 복잡한 일이 \emph{가능한가요}?

	\iffalse

	This is a \emph{simplified} sequence of events?
	How could it \emph{possibly} be any more complex?

	\fi

}\QuickQuizAnswerB{
	이건 여러 가능한 복잡 요소를 무시하고 있는데, 다음과 같은 것들입니다:

	\begin{enumerate}
	\item	다른 CPU 들이 이 캐쉬라인이 연관되는 메모리 참조 오퍼레이션을
		동시에 수행할 수도 있습니다.
	\item	이 캐쉬라인은 여러 CPU 의 캐쉬에 읽기 전용으로 복사되어
		있을수도 있는데, 이 경우는 모든 캐쉬로부터 이 캐쉬라인이
		제거되어야 합니다.
	\item	CPU~7 은 자신의 캐쉬로부터 이 캐쉬라인을 제거해서 (예를 들어,
		다른 데이터를 위한 공간을 만들기 위해), 이 요청이 도착한
		시점에서는 이 캐쉬라인이 메모리로 향하고 있을 수 있습니다.
	\item	고쳐질 수 있는 어떤 에러가 이 캐쉬라인에 발생했을 수 있는데,
		그렇다면 데이터가 사용되기 전 언젠가는 이 에러가 고쳐져야
		합니다.
	\end{enumerate}

	제품 수준의 캐쉬 일관성 메커니즘은 이런 종류의 고려할 부분들 때문에
	굉장히
	복잡합니다~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}.

	\iffalse

	This sequence ignored a number of possible complications,
	including:

	\begin{enumerate}
	\item	Other CPUs might be concurrently attempting to perform
		memory-reference operations involving this same cacheline.
	\item	The cacheline might have been replicated read-only in
		several CPUs' caches, in which case, it would need to
		be flushed from their caches.
	\item	CPU~7 might have been operating on the cache line when
		the request for it arrived, in which case CPU~7 might
		need to hold off the request until its own operation
		completed.
	\item	CPU~7 might have ejected the cacheline from its cache
		(for example, in order to make room for other data),
		so that by the time that the request arrived, the
		cacheline was on its way to memory.
	\item	A correctable error might have occurred in the cacheline,
		which would then need to be corrected at some point before
		the data was used.
	\end{enumerate}

	Production-quality cache-coherence mechanisms are extremely
	complicated due to these sorts of
	considerations~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}.
%

	\fi

}\QuickQuizEndB
%
\QuickQuizE{
	왜 이 캐쉬라인을 CPU~7 의 캐쉬로부터 제거해야 하나요?

	\iffalse

	Why is it necessary to flush the cacheline from CPU~7's cache?

	\fi

}\QuickQuizAnswerE{
	이 캐쉬라인이 CPU~7 의 캐쉬로부터 제거되지 않았다면, CPU~0 과~7 은 이
	캐쉬라인의 변수들에 다른 값을 가질 수 있습니다.
	이런 종류의 비일관성은 병렬 소프트웨어를 무척 복잡하게 만들 수
	있으므로, 현명한 하드웨어 설계자들은 이를 막습니다.

	\iffalse

	If the cacheline was not flushed from CPU~7's cache, then
	CPUs~0 and~7 might have different values for the same set
	of variables in the cacheline.
	This sort of incoherence greatly complicates parallel software,
	which is why so wise hardware architects avoid it.

	\fi

}\QuickQuizEndE
}

이 단순화된 이벤트 나열은 \emph{캐쉬 일관성 프로토콜(cache-coherency
protocols)}~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}
이라 불리는 규칙의 시작에 불과한데, 이는
Appendix~\ref{chp:app:whymb:Why Memory Barriers?} 에서 더 자세하게 다뤄집니다.
CAS 오퍼레이션에 의해 일어나는 이벤트들로부터 볼 수 있듯이, 하나의 명령이
상당한 프로토콜 트래픽을 야기할 수 있으며, 이는 여러분의 병렬 프로그램의 성능을
심각하게 하락시킬 수 있습니다.

다행히도, 어떤 변수가 업데이트는 없이 특정 기간동안 빈번하게 읽혀지기만 한다면,
해당 변수는 모든 CPU 의 캐쉬에 복사될 수 있습니다.
이 복사는 모든 CPU 가 이 \emph{읽기가 대부분인} 변수로의 매우 빠른 액세스를 할
수 있게 합니다.
Chapter~\ref{chp:Deferred Processing} 이 중요한 하드웨어 기반의 읽기가 대부분인
경우를 위한 최적화의 장점을 온전히 취하기 위한 동기화 메커니즘을 소개합니다.

\iffalse

This simplified sequence is just the beginning of a discipline called
\emph{cache-coherency protocols}~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel},
which is discussed in more detail in
Appendix~\ref{chp:app:whymb:Why Memory Barriers?}.
As can be seen in the sequence of events triggered by a CAS operation,
a single instruction can cause considerable protocol traffic, which
can significantly degrade your parallel program's performance.

Fortunately, if a given variable is being frequently read during a time
interval during which it is never updated, that variable can be replicated
across all CPUs' caches.
This replication permits all CPUs to enjoy extremely fast access to
this \emph{read-mostly} variable.
Chapter~\ref{chp:Deferred Processing} presents synchronization
mechanisms that take full advantage of this important hardware read-mostly
optimization.

\fi

\subsection{Costs of Operations}
\label{sec:cpu:Costs of Operations}

\begin{table*}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\centering\small
\begin{tabular}
  {
    l
    S[table-format = 9.1]
    S[table-format = 9.1]
    r
  }
	\toprule
	Operation	   & \multicolumn{1}{r}{Cost (ns)}
				   & {\parbox[b]{.7in}{\raggedleft Ratio\\(cost/clock)}}
					    & CPUs \\
	\midrule
	Clock period		     &   0.5 &    1.0 &			  \\
	Same-CPU CAS		     &   7.0 &   14.6 & 0		  \\
	Same-CPU lock		     &  15.4 &   32.3 & 0		  \\
	In-core blind CAS	     &   7.2 &   15.2 & 224		  \\
	In-core CAS		     &  18.0 &   37.7 & 224		  \\
	Off-core blind CAS	     &  47.5 &   99.8 & 1--27,225--251	  \\
	Off-core CAS		     & 101.9 &  214.0 & 1--27,225--251	  \\
	Off-socket blind CAS	     & 148.8 &  312.5 & 28--111,252--335  \\
	Off-socket CAS		     & 442.9 &  930.1 & 28--111,252--335  \\
	Cross-interconnect blind CAS & 336.6 &  706.8 & 112--223,336--447 \\
	Cross-interconnect CAS	     & 944.8 & 1984.2 & 112--223,336--447 \\
	\midrule
	Off-System	&	      & 	    & \\
	Comms Fabric	&       5 000 &      10 500 & \\
	Global Comms	& 195 000 000 & 409 500 000 & \\
	\bottomrule
\end{tabular}
\caption{CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs @ 2.10\,GHz}
\label{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
\end{table*}

병렬 프로그램에 중요한 일부 일반적 오퍼레이션들의 오버헤드가
Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
에 표시되어 있습니다.
이 시스템의 클락 기간은 대략 0.5\,ns 입니다.
현대의 마이크로프로세서가 클락 기간당 여러 명령을 처리할 수 있는건 흔하지만,
오퍼레이션의 비용은 ``Ratio'' 라고 라벨링 된 세번째 행에 클락 기간으로
얼만큼의 비율인지 표시되어 있습니다.
이 표에 대해 말씀드릴 첫번째 것은 그런 비율 중 큰 값이 여럿 있다는 것입니다.

같은 CPU 에서의 compare-and-swap (CAS) 오퍼레이션은 대략 7 나노세컨드를
소비하는데, 이는 클락 기간의 10배가 넘는 기간입니다.
CAS 는 하드웨어가 특정 메모리 위치의 내용을 특정 ``기존'' 값과 비교하고,
그것들이 동일하다면 특정한 ``새로운'' 값을 저장하는, 즉 CAS 오퍼레이션이
성공하는, 원자적 오퍼레이션입니다.
만약 그 값들이 동일하지 않았다면, 해당 메모리 위치는 그 (예상되지 않았던) 값을
유지하게 되고, CAS 오퍼레이션은 실패한 게 됩니다.
해당 메모리 위치의 값은 이 비교와 저장 사이에 바뀌지 않음을 하드웨어가
보장하므로 이 오퍼레이션은 원자적입니다.
CAS 의 기능은 x86 에서는 \co{lock;cmpxchg} 로 제공됩니다.

\iffalse

The overheads of some common operations important to parallel programs are
displayed in
Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}.
This system's clock period rounds to 0.5\,ns.
Although it is not unusual for modern microprocessors to be able to
retire multiple instructions per clock period, the operations' costs are
nevertheless normalized to a clock period in the third column, labeled
``Ratio''.
The first thing to note about this table is the large values of many of
the ratios.

The same-CPU compare-and-swap (CAS) operation consumes about seven
nanoseconds, a duration more than ten times that of the clock period.
CAS is an atomic operation in which the hardware compares the contents
of the specified memory location to a specified ``old'' value, and if
they compare equal, stores a specified ``new'' value, in which case the
CAS operation succeeds.
If they compare unequal, the memory location keeps its (unexpected) value,
and the CAS operation fails.
The operation is atomic in that the hardware guarantees that the memory
location will not be changed between the compare and the store.
CAS functionality is provided by the \co{lock;cmpxchg} instruction on x86.

\fi

``same-CPU'' 접두어는 특정 변수에 CAS 오퍼레이션을 수행하는 CPU 가 이 변수를
마지막으로 액세스한 CPU 임을, 따라서 여기 연관된 캐쉬라인이 이 CPU 의 캐쉬에
있음을 의미합니다.
비슷하게, same-CPU lock 오퍼레이션 (락 획득과 해제 한쌍의 ``왕복'' 작업) 은 15
나노세컨드, 달리 말하면 30 클락 사이클을 소비합니다.
이 락 오퍼레이션은 락 데이터 구조에 한번은 획득을 위해, 또한번은 해제를 위해,
두번의 어토믹 오퍼레이션을 할 것을 필요로 하기 때문에 CAS 보다 비용이 높습니다.

하나의 코어를 공유하는 하드웨어 쓰레드간의 상호작용을 하게 되는 in-core
오퍼레이션은 same-CPU 오퍼레이션과 거의 같습니다.
이 두개의 하드웨어 쓰레드는 전체 캐쉬 구조를 공유함을 생각하면 이는 크게 놀랍진
않을겁니다.

Blind CAS 의 경우, 소프트웨어는 메모리 위치를 고려치 않은 채 기존 값을
특정합니다.
이는 락을 획득하고자 하는 시도에 적합합니다.
락이 잡히지 않은 상태가 값 0 으로 표현되고 락이 잡힌 상태는 값 1 로 표현된다면,
0 을 기존값으로, 1 을 새 값으로 하는, 이 락에의 CAS 오퍼레이션은 이 락이 아직
잡히지 않은 상태라면 획득하게 됩니다.
핵심은 이 메모리 위치로의 액세스는 오직 하나, 이 CAS 오퍼레이션이 있다는
겁니다.

\iffalse

The ``same-CPU'' prefix means that the CPU now performing the CAS operation
on a given variable was also the last CPU to access this variable, so
that the corresponding cacheline is already held in that CPU's cache.
Similarly, the same-CPU lock operation (a ``round trip'' pair consisting
of a lock acquisition and release) consumes more than fifteen nanoseconds,
or more than thirty clock cycles.
The lock operation is more expensive than CAS because it requires two
atomic operations on the lock data structure, one for acquisition and
the other for release.

In-core operations involving interactions between the hardware threads
sharing a single core are about the same cost as same-CPU operations.
This should not be too surprising, given that these two hardware threads
also share the full cache hierarchy.

In the case of the blind CAS, the software specifies the old value
without looking at the memory location.
This approach is appropriate when attempting to acquire a lock.
If the unlocked state is represented by zero and the locked state
is represented by the value one, then a CAS operation on the lock
that specifies zero for the old value and one for the new value
will acquire the lock if it is not already held.
The key point is that there is only one access to the memory
location, namely the CAS operation itself.

\fi

반면, 이 평범한 CAS 오퍼레이션의 기존값은 앞서 행해진 어떤 로드로부터
얻어집니다.
예를 들어, 어떤 어토믹 증가 오퍼레이션을 구현하기 위해서는, 해당 위치의 현재
값이 읽혀지고, 새 값을 만들기 위해 그 값을 증가시킵니다.
그러고 나서는 이 CAS 오퍼레이션에 앞서 읽혀진 값이 기존값으로, 이 증가된 값은
새 값으로 명세됩니다.
이 값이 이 읽기와 CAS 사이에 변경되지 않았다면, 이는 해당 메모리 위치의 값을
증가시킬 겁니다.
하지만, 이 값이 바뀌었다면, 기존 값이 다름을 알게 되어서, 이 CAS 오퍼레이션이
실패하게 할겁니다.
핵심은 이제 해당 메모리 위치로의 두개의 액세스, 즉 읽기와 CAS 가 존재한다는
겁니다.

따라서, in-core blind CAS 는 대략 7 나노세컨드만 소비하는 반면 in-core CAS 는
약 18 나노세컨드를 소비한다는 게 놀랍지 않습니다.
이 blind 가 아닌 경우의 추가적인 로드는 공짜로 오지 않습니다.
그렇다곤 하나, 이 오퍼레이션들의 오버헤드는 단일 CPU 의 CAS 와 락에 각각
비슷합니다.

\iffalse

In contrast, a normal CAS operation's old value is derived from
some earlier load.
For example, to implement an atomic increment, the current value of
that location is loaded and that value is incremented to produce the
new value.
Then in the CAS operation, the value actually loaded would be specified
as the old value and the incremented value as the new value.
If the value had not been changed between the load and the CAS, this
would increment the memory location.
However, if the value had in fact changed, then the old value would
not match, causing a miscompare that would result in the CAS operation
failing.
The key point is that there are now two accesses to the memory location,
the load and the CAS\@.

Thus, it is not surprising that in-core blind CAS consumes only about
seven nanoseconds, while in-core CAS consumes about 18 nanoseconds.
The non-blind case's extra load does not come for free.
That said, the overhead of these operations are similar to single-CPU
CAS and lock, respectively.

\fi

\QuickQuiz{
	\Cref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	는 CPU~0 가 CPU~224 와 코어를 공유한다고 하는데요.
	그건 CPU~1 이 되어야 하는거 아닌가요???

	\iffalse

	\Cref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	shows CPU~0 sharing a core with CPU~224.
	Shouldn't that instead be CPU~1???

	\fi

}\QuickQuizAnswer{
	이것에 대해 이상하다고 생각하기 쉽지만,
	\path{/sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list}
	파일은 정말로 \co{0,224} 라는 문자열을 가지고 있습니다.
	따라서, CPU~0 의 하이퍼쓰레드 쌍둥이는 정말로 CPU~224 입니다.
	어떤 사람들은 많은 워크로드에서 두번째 하이퍼쓰레드는 추가적 성능을
	크게 내지는 못함을 인용하며 이 숫자 규칙이 순진한 어플리케이션과
	스케쥴러가 더 나은 성능을 낼 수 있을 거라 예상합니다.
	이 예상은 순진한 어플리케이션과 스케쥴러는 CPU 를 숫자 순서로 활용해서,
	더 약한 하이퍼쓰레드 쌍둥이 CPU 들을 다른 모든 코어들이 사용되기
	전까지는 사용되지 않게 놔둘거라 가정합니다.

	\iffalse

	It is easy to be sympathetic to this view, but the file
	\path{/sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list}
	really does contain the string \co{0,224}.
	Therefore, CPU~0's hyperthread twin really is CPU~224.
	Some people speculate that this numbering allows naive applications
	and schedulers to perform better, citing the fact that on many
	workloads the second hyperthread does not provide a huge
	amount of additional performance.
	This speculation assumes that naive applications and schedulers
	would utilize CPUs in numerical order, leaving aside the weaker
	hyperthread twin CPUs until all cores are in use.

	\fi

}\QuickQuizEnd

다른 코어에 있지만 같은 소켓에 위치한 CPU 들이 연관되는 blind CAS 는 대략 50
나노세컨드, 달리 말하면 약 100 클락 사이클을 소모합니다.
이 캐쉬미스 측정을 위해 사용된 코드는 해당 캐쉬라인을 한쌍의 CPU 사이에서
주고받게 하므로, 이 캐쉬 미스는 메모리에서가 아니라 다른 CPU 의 캐쉬에서
이뤄집니다.
앞서 이야기 되었듯 변수의 기존 값을 보고 새 값을 저장도 해야 하는 non-blind CAS
오퍼레이션은 약 100 나노세컨드, 즉 약 200 클락 사이클을 소모합니다.
이를 좀 더 생각해 봅시다.
CAS 오퍼레이션 \emph{하나} 를 위해 필요한 시간동안, 이 CPU 는 \emph{200개의}
평범한 명령을 수행했을 수도 있었습니다.
이는 fine-grained 락킹만이 아니라 모든 다른 동기화 메커니즘이 잘게 쪼개진
전역적 동의사항에 의존하고 있음을 보일겁니다.

\iffalse

A blind CAS involving CPUs in different cores but on the same socket
consumes almost fifty nanoseconds, or almost one hundred clock cycles.
The code used for this cache-miss measurement passes the cache line
back and forth between a pair of CPUs, so this cache miss is satisfied
not from memory, but rather from the other CPU's cache.
A non-blind CAS operation, which as noted earlier must look at the old
value of the variable as well as store a new value, consumes over one
hundred nanoseconds, or more than two hundred clock cycles.
Think about this a bit.
In the time required to do \emph{one} CAS operation, the CPU could have
executed more than \emph{two hundred} normal instructions.
This should demonstrate the limitations not only of fine-grained locking,
but of any other synchronization mechanism relying on fine-grained
global agreement.

\fi

이 한쌍의 CPU 가 서로 다른 소켓에 위치해 있다면, 이 오퍼레이션들은 상당히 더
비싸집니다.
하나의 blind CAS 오퍼레이션은 대략 150~나노세컨드, 즉 300 클락 사이클 이상을
소모합니다.
일반적인 CAS 오퍼레이션은 400~나노세컨드, 즉 거의 \emph{1000} 클락 사이클
이상을 소모합니다.

더 나쁜 것이, 모든 소켓 쌍이 똑같이 만들어지지는 않습니다.
이 시스템은 네개짜리 소켓 컴포넌트의 한 쌍으로 구성되어 있는데, 서로 다른
컴포넌트에 위치한 CPU 사이에는 추가적인 응답시간 페널티가 존재합니다.
이 경우, 하나의 blind CAS 오퍼레이션은 300 나노세컨드 이상, 즉 700 클락 사이클
이상을 소모합니다.
하나의 CAS 오퍼레이션은 거의 1 마이크로세컨드, 즉 거의 2000 클락 사이클을
소모합니다.

\iffalse

If the pair of CPUs are on different sockets, the operations are considerably
more expensive.
A blind CAS operation consumes almost 150~nanoseconds, or more than
three hundred clock cycles.
A normal CAS operation consumes more than 400~nanoseconds, or almost
\emph{one thousand} clock cycles.

Worse yet, not all pairs of sockets are created equal.
This particular system appears to be constructed as a pair of four-socket
components, with additional latency penalties when the CPUs reside
in different components.
In this case, a blind CAS operation consumes more than three hundred
nanoseconds, or more than seven hundred clock cycles.
A CAS operation consumes almost a full microsecond, or almost two
thousand clock cycles.

\fi

\QuickQuiz{
	분명 하드웨어 설계자들은 이 상황을 개선하려 노력했을 수 있을 거예요!
	왜 그들은 이 단일 명령 오퍼레이션의 끔찍한 성능에 만족하고 있는거죠?

	\iffalse

	Surely the hardware designers could be persuaded to improve
	this situation!
	Why have they been content with such abysmal performance
	for these single-instruction operations?

	\fi

}\QuickQuizAnswer{
	하드웨어 설계자들은 이 문제를 위해 일을 \emph{해왔습니다}, 그리고
	물리학자 Stephen Hawking 같은 선각자들에게 자문을 구했습니다.
	Hawking 의 발견은 하드웨어 설계자들이 두개의 기본적
	문제를~\cite{BryanGardiner2007} 가지고 있다는 것이었습니다.

	\begin{enumerate}
	\item	빛의 유한한 속도, 그리고
	\item	물질의 원자적 본성.
	\end{enumerate}

	\iffalse

	The hardware designers \emph{have} been working on this
	problem, and have consulted with no less a luminary than
	the late physicist Stephen Hawking.
	Hawking's observation was that the hardware designers have
	two basic problems~\cite{BryanGardiner2007}:

	\begin{enumerate}
	\item	The finite speed of light, and
	\item	The atomic nature of matter.
	\end{enumerate}

	\fi

\begin{table}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\centering\small
\begin{tabular}
  {
    l
    S[table-format = 9.1]
    S[table-format = 9.1]
  }
	\toprule
	Operation		& \multicolumn{1}{r}{Cost (ns)}
			& {\parbox[b]{.7in}{\raggedleft Ratio\\(cost/clock)}} \\
	\midrule
	Clock period		&           0.4	&           1.0 \\
	Same-CPU CAS		&          12.2	&          33.8 \\
	Same-CPU lock		&          25.6	&          71.2 \\
	Blind CAS		&          12.9	&          35.8 \\
	CAS			&           7.0	&          19.4 \\
	\midrule
	Off-Core		&		&		\\
	Blind CAS		&          31.2	&          86.6 \\
	CAS			&          31.2	&          86.5 \\
	\midrule
	Off-Socket		&		&		\\
	Blind CAS		&          92.4	&         256.7 \\
	CAS			&          95.9	&         266.4 \\
	\midrule
	Off-System		&		&		\\
	Comms Fabric		&       2 600   &       7 220   \\
	Global Comms		& 195 000 000	& 542 000 000   \\
	\bottomrule
\end{tabular}
\caption{Performance of Synchronization Mechanisms on 16-CPU 2.8\,GHz Intel X5550 (Nehalem) System}
\label{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
\end{table}

	이 첫번째 문제는 기본 속도를 제한하며, 두번째 문제는 소형화를 제한해서,
	결국 빈도를 제한합니다.
	그리고 이는 현재 제품들의 주파수를 10\,GHz 아래로 제한하고 있는 에너지
	소비 이슈를 회피하기도 합니다.

	또한,
	page~\pageref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz} 의
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	은 448~하드웨어 쓰레드의 거대 시스템을 보입니다.
	16~하드웨어 쓰레드를 갖는 훨씬 작은 시스템을 보이는
	\cref{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
	에 나타난 것처럼 더 작은 시스템은 종종 더 나은 응답시간을 갖습니다.
	비슷한 모습이
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	의 두개의 ``Off-core'' 열까지에 보여져 있습니다.

	\iffalse

	The first problem limits raw speed, and the second limits
	miniaturization, which in turn limits frequency.
	And even this sidesteps the power-consumption issue that
	is currently limiting production frequencies to well below
	10\,GHz.

	In addition,
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	on
	page~\pageref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	represents a reasonably large system with no fewer 448~hardware
	threads.
	Smaller systems often achieve better latency, as may be seen in
	\cref{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System},
	which represents a much smaller system with only 16~hardware threads.
	A similar view is provided by the rows of
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	down to and including the two ``Off-core'' rows.

	\fi

\begin{table*}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\centering\small
\begin{tabular}
  {
    l
    S[table-format = 9.1]
    S[table-format = 9.1]
    r
  }
	\toprule
	Operation		& \multicolumn{1}{r}{Cost (ns)}
			& {\parbox[b]{.7in}{\raggedleft Ratio\\(cost/clock)}}
			& CPUs \\
	\midrule
	Clock period		     &   0.5 &    1.0 &			  \\
	Same-CPU CAS		     &   6.2 &   13.6 & 0		  \\
	Same-CPU lock		     &  13.5 &   29.6 & 0		  \\
	In-core blind CAS	     &   6.5 &   14.3 & 6		  \\
	In-core CAS		     &  16.2 &   35.6 & 6		  \\
	Off-core blind CAS	     &  22.2 &   48.8 & 1--5,7--11	  \\
	Off-core CAS		     &  53.6 &  117.9 & 1--5,7--11	  \\
	\midrule
	Off-System	&	      & 	    & \\
	Comms Fabric	&       5 000 &      11 000 & \\
	Global Comms	& 195 000 000 & 429 000 000 & \\
	\bottomrule
\end{tabular}
\caption{CPU 0 View of Synchronization Mechanisms on 12-CPU Intel Core i7-8750H CPU @ 2.20\,GHz}
\label{tab:cpu:CPU 0 View of Synchronization Mechanisms on 12-CPU Intel Core i7-8750H CPU @ 2.20GHz}
\end{table*}

	더 나아가서, 제가 이걸 타이핑 하고 있는 랩톱과 같은 더 작은 새로운 단일
	소켓 시스템은 더욱 합리적인 응답시간을 갖는데,
	\cref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 12-CPU Intel Core i7-8750H CPU @ 2.20GHz}
	에 이 점이 보여져 있습니다.

	대안적으로, 1990년대 중반의 64-CPU 시스템은 5 마이크로세컨드를 넘는
	인터컨넥트간 응답시간을 가졌고, 따라서
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	에 보인 8 소켓 448 하드웨어 쓰레드의 괴물 같은 머신은 25년전의 비슷했던
	것에 비하면 5배가 넘는 개선을 이뤘음을 보입니다.

	\iffalse

	Furthermore, newer small-scale single-socket systems such
	as the laptop on which I am typing this also have more
	reasonable latencies, as can be seen in
	\cref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 12-CPU Intel Core i7-8750H CPU @ 2.20GHz}.

	Alternatively, a 64-CPU system in the mid 1990s had
	cross-interconnect latencies in excess of five microseconds,
	so even the eight-socket 448-hardware-thread monster shown in
	Table~\ref{tab:cpu:CPU 0 View of Synchronization Mechanisms on 8-Socket System With Intel Xeon Platinum 8176 CPUs at 2.10GHz}
	represents more than a five-fold improvement over its
	25-years-prior counterparts.

	\fi

	하드웨어 쓰레드를 하나의 다이 위의 단일 코어와 여러 코어들에
	병합시키는게 응답시간을 상당히 개선시켰는데, 최소한 단일 코어 또는 단일
	다이 내로 국한해서는 그렇습니다.
	전체 시스템 응답시간에 있어서도 약간 개선이 있었습니다만, 대략 100배
	정도에 그쳤습니다.
	불행히도, 빛의 속도도 물질의 원자적 본성도 지난 수년간 크게 바뀌지
	않았습니다~\cite{NoBugsHare2016CPUoperations}.
	따라서, 공간적/시간적 지역성은 동시성 소프트웨어의 최우선
	걱정거리입니다, 상대적으로 작은 시스템에서 돌아가고 있다 해도 말이죠.

	Section~\ref{sec:cpu:Hardware Free Lunch?}
	은 병렬 프로그래머들의 역겨을 완화시키기 위해 하드웨어 설계자들이 다른
	어떤 걸 할 수 있을지 알아봅니다.

	\iffalse

	Integration of hardware threads in a single core and multiple
	cores on a die have improved latencies greatly, at least within the
	confines of a single core or single die.
	There has been some improvement in overall system latency,
	but only by about a factor of two.
	Unfortunately, neither the speed of light nor the atomic nature
	of matter has changed much in the past few
	years~\cite{NoBugsHare2016CPUoperations}.
	Therefore, spatial and temporal locality are first-class concerns
	for concurrent software, even when running on relatively
	small systems.

	Section~\ref{sec:cpu:Hardware Free Lunch?}
	looks at what else hardware designers might be
	able to do to ease the plight of parallel programmers.

	\fi

}\QuickQuizEnd

\begin{table}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\centering\small
\begin{tabular}{lrrrrr}
	\toprule
	Level &  Scope & Line Size &   Sets & Ways &    Size \\
	\midrule
	L0    &   Core &        64 &     64 &    8 &     32K \\
	L1    &   Core &        64 &     64 &    8 &     32K \\
	L2    &   Core &        64 &   1024 &   16 &   1024K \\
	L3    & Socket &        64 & 57,344 &   11 & 39,424K \\
	\bottomrule
\end{tabular}
\caption{Cache Geometry for 8-Socket System With Intel Xeon Platinum 8176 CPUs @ 2.10\,GHz}
\label{tab:cpu:Cache Geometry for 8-Socket System With Intel Xeon Platinum 8176 CPUs @ 2.10GHz}
\end{table}

불행히도, 코어 내에서와 소켓 내에서의 통신의 빠른 속도는 공짜로 오지 않습니다.
첫째로, 하나의 코어 내에는 두개의 CPU 만 있으며 하나의 소켓 내에는 56개의
코어만 있는 반면, 이 시스템 전체에는 448개의 코어가 있습니다.
둘째로,
\cref{tab:cpu:Cache Geometry for 8-Socket System With Intel Xeon Platinum 8176 CPUs @ 2.10GHz}
에 보인 것처럼, 코어 내의 캐쉬는 소켓 내 캐쉬에 비해 상당히 작으며, 소캣 내
캐쉬 역시 시스템 전체에 구성된 1.4\,TB 의 메모리에 비하면 상당히 작습니다.
셋째로, 이 그림에 따르면, 캐쉬들은 버킷당 아이템의 수가 제한된 하드웨어
해쉬테이블로 구성되어 있습니다.
예를 들어, L3 캐쉬의 크기 (``Size'') 는 대략 40\,MB 입니다만, 각 버켓
(``Line'') 은 11개의 메모리 블락만 (``Ways'') 가질 수 있으며, 각 블락은 최대 64
바이트가 될 수 있습니다 (``Line Size'').
이 말은 이 40\,MB 캐쉬를 오버플로우 시키는데 12 바이트의 메모리만으로도 (물론
주의 깊게 골라진 주소들의 것으로) 충분하다는 말입니다.
반면에, 역시 주의 깊게 골라진 주소들을 사용하면 전체 40\,MB 를 잘 사용할 수도
있습니다.

\iffalse

Unfortunately, the high speed of within-core and within-socket communication
does not come for free.
First, there are only two CPUs within a given core and only 56 within
a given socket, compared to 448 across the system.
Second, as shown in
\cref{tab:cpu:Cache Geometry for 8-Socket System With Intel Xeon Platinum 8176 CPUs @ 2.10GHz},
the in-core caches are quite small compared to the in-socket caches, which
are in turn quite small compared to the 1.4\,TB of memory configured on
this system.
Third, again referring to the figure, the caches are organized as
a hardware hash table with a limited number of items per bucket.
For example, the raw size of the L3 cache (``Size'') is almost 40\,MB, but each
bucket (``Line'') can only hold 11 blocks of memory (``Ways''), each
of which can be at most 64 bytes (``Line Size'').
This means that only 12 bytes of memory (admittedly at carefully chosen
addresses) are required to overflow this 40\,MB cache.
On the other hand, equally careful choice of addresses might make good
use of the entire 40\,MB.

\fi

참조의 공간적 지역성은 데이터를 메모리 전역에 퍼트리는 것 만큼이나 무척
중요합니다.

I/O 오퍼레이션은 심지어 더 비쌉니다.
``Comms Fabric'' 행에 보여진 것처럼, InfiniBand 나 다른 독점적 인터컨넥트들과
같은 고성능의 (그리고 비싼!) 통신 장비는 단말간 왕복에 대략 5 마이크로세컨드의
응답시간을 갖는데, 이는 \emph{만개가 넘는} 명령이 수행될 수도 있는 시간입니다.
표준 기반의 통신 네트워크는 종종 일종의 프로토콜 처리를필요로 해서, 응답시간을
더 늘립니다.
물론, 지리적 거리 역시 응답시간을 늘리는데, 광섬유를 사용해 광속으로 지구를
한바퀴 드는데 걸리는 시간은 ``Global Comms'' 행에 보인 것처럼 대략 195
\emph{밀리세컨드}로, 달리 말하면 4억 클락 사이클이 넘습니다.

\iffalse

Spatial locality of reference is clearly extremely important, as is
spreading the data across memory.

I/O operations are even more expensive.
As shown in the ``Comms Fabric'' row,
high performance (and expensive!) communications fabric, such as
InfiniBand or any number of proprietary interconnects, has a latency
of roughly five microseconds for an end-to-end round trip, during which
time more than \emph{ten thousand} instructions might have been executed.
Standards-based communications networks often require some sort of
protocol processing, which further increases the latency.
Of course, geographic distance also increases latency, with the
speed-of-light through optical fiber latency around the world coming to
roughly 195 \emph{milliseconds}, or more than 400 million clock
cycles, as shown in the ``Global Comms'' row.

\fi

% Reference for Infiniband latency:
% http://www.hpcadvisorycouncil.com/events/2014/swiss-workshop/presos/Day_1/1_Mellanox.pdf
%     page 6/76 'Leading Interconnect, Leading Performance'
% Needs updating...

\QuickQuiz{
	이 숫자들은 미친듯이 크군요!
	이걸 어떻게 제 머리로 이해할 수 있을까요?

	\iffalse

	These numbers are insanely large!
	How can I possibly get my head around them?

	\fi

}\QuickQuizAnswer{
	두루마리 휴지 한개를 가져와 보세요.
	미국에서, 두루마리 휴지는 일반적으로 350--500 칸으로 되어 있습니다.
	하나의 클락 사이클을 나타내기 위해 한 칸을 떼어서 한쪽에 두세요.
	이제 두루마루 휴지의 나머지를 펼쳐보세요.

	그 결과 나오는 두루마리 휴지의 더미가 하나의 CAS 캐쉬 미스를 나타낼
	겁니다.

	더 비싼 시스템간 통신 응답시간을 위해선, 그 응답시간을 나타내기 위해
	두루마리 휴지 여러개 (또는 여러 상자) 를 사용하세요.

	중요한 절약 팁: 두루마리 휴지를 준비할 때에는 그것들이 여러분의 일생
	동안 모두 사용될 양인지도 생각해 두세요!\footnote{
		특히나 2020년 초, 코로나 바이러스가 창궐하는 한가운데 상점들에
		두루마리 휴지가 잘 남아있지 않은 상황에서는요!}

	\iffalse

	Get a roll of toilet paper.
	In the USA, each roll will normally have somewhere around
	350--500 sheets.
	Tear off one sheet to represent a single clock cycle, setting it aside.
	Now unroll the rest of the roll.

	The resulting pile of toilet paper will likely represent a single
	CAS cache miss.

	For the more-expensive inter-system communications latencies,
	use several rolls (or multiple cases) of toilet paper to represent
	the communications latency.

	Important safety tip: make sure to account for the needs of
	those you live with when appropriating toilet paper!\footnote{
		Especially here in early 2020, in the midst of the
		coronavirus excitement that is keeping store shelves
		free of toilet paper and much else besides!}

	\fi

}\QuickQuizEnd

\subsection{Hardware Optimizations}
\label{sec:cpu:Hardware Optimizations}

하드웨어가 어떻게 도움을 주는지 묻는건 자연스러운 일입니다, 그리고 그에 대한
대답은 ``무척!'' 입니다.

그런 하드웨어 최적화 중 하나는 큰 캐쉬라인입니다.
이는 특히 소프트웨어가 메모리를 순차적으로 액세스할 때 큰 성능 향상을
제공합니다.
예를 들어, 64 바이트 캐쉬라인이 있고 소프트웨어가 64비트 변수들을 접근한다면,
첫번째 액세스는 빛의 속도에 의한 (그 외의 것들이 없다면) 지연 때문에 여전히
느리겠지만, 뒤따르는 일곱번의 액세스는 무척 빠를 수 있습니다.
하지만, 이 최적화는 이른바 false sharing 이라 불리는, 같은 캐쉬라인에 있는 다른
변수들이 다른 CPU 에 의해 업데이트 되어 높은 캐쉬미스율을 초래하는 어두운
부분을 가지고 있습니다.
소프트웨어는 false sharing 을 방지하기 위해 많은 컴파일러에서 사용 가능한 정렬
지시어를 사용할 수 있으며, 그런 지시어를 추가하는 것은 병렬 소프트웨어 튜닝에서
흔한 단계입니다.

\iffalse

It is only natural to ask how the hardware is helping, and the answer
is ``Quite a bit!''

One hardware optimization is large cachelines.
This can provide a big performance boost, especially when software is
accessing memory sequentially.
For example, given a 64-byte cacheline and software accessing 64-bit
variables, the first access will still be slow due to speed-of-light
delays (if nothing else), but the remaining seven can be quite fast.
However, this optimization has a dark side, namely false sharing,
which happens when different variables in the same cacheline are
being updated by different CPUs, resulting in a high cache-miss rate.
Software can use the alignment directives available in many compilers
to avoid false sharing, and adding such directives is a common step
in tuning parallel software.

\fi

관련된 두번째 하드웨어 최적화는 캐쉬 prefetching 으로, 연속적인 액세스에 대해
뒤쪽 캐쉬라인을 미리 읽어들여오는 식으로 하드웨어가 반응함으로써 뒤따르는
캐쉬라인들에 대한 빛의 속도에 의한 지연을 막습니다.
물론, 하드웨어는 언제 미리 읽기를 할 지 판단하기 위해 간단한 휴리스틱을
사용해야만 하며, 이 휴리스틱은 많은 어플리케이션이 갖는 복잡한 데이터 액세스
패턴에 의해 바보같아질 수 있습니다.
다행히도, 일부 CPU 제품군은 특수한 미리읽기 명령을 제공함으로써 이를 극복
가능하게 합니다.
불행히도, 일반적인 경우에서의 이 명령의 효과는 실망적입니다.

세번째 하드웨어 최적화는 store buffer 로, 일련의 스토어 명령이 그 스토어
명령들이 연속적이지 않은 주소를 향하더라도, 그리고 이 스토어 명령들에 필요한
캐쉬라인들이 해당 CPU 의 캐쉬에 존재하지 않은 경우에도 빠르게 수행되게 합니다.
이 최적화의 어두운 부분은 메모리 순서 오류인데,
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering}
를 참고하시기 바랍니다.

\iffalse

A second related hardware optimization is cache prefetching, in which
the hardware reacts to consecutive accesses by prefetching subsequent
cachelines, thereby evading speed-of-light delays for these
subsequent cachelines.
Of course, the hardware must use simple heuristics to determine when
to prefetch, and these heuristics can be fooled by the complex data-access
patterns in many applications.
Fortunately, some CPU families allow for this by providing special
prefetch instructions.
Unfortunately, the effectiveness of these instructions in the general
case is subject to some dispute.

A third hardware optimization is the store buffer, which allows a string
of store instructions to execute quickly even when the stores are to
non-consecutive addresses and when none of the needed cachelines are
present in the CPU's cache.
The dark side of this optimization is memory misordering, for which see
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering}.

\fi

네번째 하드웨어 최적화는 투기적 수행 (speculative execution) 으로, 하드웨어가
메모리 순서 오류를 초래하지 않고 이 스토어 버퍼를 잘 사용할 수 있게 해줍니다.
이 최적화의 어두운 부분은 이 투기적 수행이 뒤틀어지고 수행 결과가 되돌이켜지고
재수행 되어야 할 때 발생하는 에너지 비효율성과 성능 하락입니다.
더 나쁜 것이, Spectre 와 Meltdown~\cite{JannHorn2018MeltdownSpectre} 의 발전은
하드웨어의 투기적 수행이 메모리 보호 하드웨어를 깨부수는 사이드 채널 공격을
가능하게 해서 비특권 프로세스가 그들은 액세스하면 안되는 메모리를 읽을 수 있게
할 수 있습니다.
투기적 수행과 클라우드 컴퓨팅의 조합은 상당한 재작업이 필요함은 분명합니다!

다섯번째 하드웨어 최적화는 커다란 캐쉬로, 개별 CPU 가 비싼 캐쉬 미스를 일으키지
않고도 큰 데이터셋을 가지고 동작할 수 있게 합니다.
커다란 캐쉬는 에너지 효율성과 캐쉬 미스 응답시간을 나쁘게 만들 수 있지만,
마이크로프로세서 제품들의 지속적인 캐쉬 크기 증가 경향은 이 최적화의 힘을
증명합니다.

마지막 하드웨어 최적화는 읽기가 대부분인 경우를 위한 복사로, 자주 읽히지만
가끔씩만 업데이트 되는 데이터는 모든 CPU 의 캐쉬에 존재하게 하는 겁니다.
이 최적화는 읽기가 대부분인 데이터는 무척 효율적으로 액세스 될 수 있게 하며,
Chapter~\ref{chp:Deferred Processing} 의 주제입니다.

\iffalse

A fourth hardware optimization is speculative execution, which can
allow the hardware to make good use of the store buffers without
resulting in memory misordering.
The dark side of this optimization can be energy inefficiency and
lowered performance if the speculative execution goes awry and must
be rolled back and retried.
Worse yet, the advent of
Spectre and Meltdown~\cite{JannHorn2018MeltdownSpectre}
made it apparent that hardware speculation can also enable side-channel
attacks that defeat memory-protection hardware so as to allow unprivileged
processes to read memory that they should not have access to.
It is clear that the combination of speculative execution and cloud
computing needs more than a bit of rework!

A fifth hardware optimization is large caches, allowing individual
CPUs to operate on larger datasets without incurring expensive cache
misses.
Although large caches can degrade energy efficiency and cache-miss
latency, the ever-growing cache sizes on production microprocessors
attests to the power of this optimization.

A final hardware optimization is read-mostly replication, in which
data that is frequently read but rarely updated is present in all
CPUs' caches.
This optimization allows the read-mostly data to be accessed
exceedingly efficiently, and is the subject of
Chapter~\ref{chp:Deferred Processing}.

\fi

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\includegraphics{cartoons/Data-chasing-light-wave}}
\caption{Hardware and Software: On Same Side}
\ContributedBy{Figure}{fig:cpu:Hardware and Software: On Same Side}{Melissa Broussard}
\end{figure}

요약하자면, 하드웨어와 소프트웨어 엔지니어들은 정말로 같은 쪽에 있으며, 둘 다
물질의 법칙의 최대의 노력에도 불구하고 컴퓨터를 더 빠르게 하기 위해
Figure~\ref{fig:cpu:Hardware and Software: On Same Side}
에 우리의 데이터 흐름이 빛의 속도를 넘기기 위해 노력하는 모습으로 그려진 것처럼
노력하고 있습니다.
다음 섹션은 최근의 연구가 실제 세계에 어떤 모습의 결과를 내게 되느냐에 따라
하드웨어 엔지니어들이 할수도 (또는 하지 못할수도) 있는, 추가적인 것들을 이야기
합니다.
이 훌륭한 목표를 향한 소프트웨어의 기여는 이 책의 나머지 챕터들에 있습니다.

\iffalse

In short, hardware and software engineers are really on the same side,
with both trying to make computers go fast despite the best efforts of
the laws of physics, as fancifully depicted in
Figure~\ref{fig:cpu:Hardware and Software: On Same Side}
where our data stream is trying its best to exceed the speed of light.
The next section discusses some additional things that the hardware engineers
might (or might not) be able to do, depending on how well recent
research translates to practice.
Software's contribution to this noble goal is outlined in the remaining
chapters of this book.

\fi
