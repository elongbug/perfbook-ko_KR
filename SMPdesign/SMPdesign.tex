% SMPdesign/SMPdesign.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{cha:Partitioning and Synchronization Design}{Partitioning and Synchronization Design}{qqzSMPdesign}
%
\Epigraph{Divide and rule.}{\emph{Philip II of Macedon}}

이 챕터는 성능, 확장성, 그리고 응답 시간을 균형맞추기 위해 관용구 또는 ``디자인
패턴''~\cite{Alexander79,GOF95,SchmidtStalRohnertBuschmann2000v2Textbook} 을
사용해서 현대의 상용 멀티코어 시스템의 장점을 갖추도록 소프트웨어를 어떻게
설계하는지 설명합니다.
제대로 쪼개진 문제는 간단하고, 확장 가능하며, 높은 성능을 갖춘 해결책을 이끄는
반면, 잘 쪼개지지 못한 문제는 느리고 복잡한 해결책을 이끌어 냅니다.
이 챕터는 여러분의 코드에 쪼개기를 몰아서 하기 (batching) 와 규칙 약화시키기
(weakening) 과 함께 설계하는 것을 도울 겁니다.
``설계'' 라는 단어가 무척 중요합니다: 여러분은 쪼개기를 첫번째로, 두번째로
몰아서 하기, 세번째로 규칙 약화시키기를, 그리고 네번째로 코드를 짜야 합니다.
이 순서를 바꾸는 것은 종종 대단한 좌절감과 함께 안좋은 성능과 확장성을 이끌어
냅니다.\footnote{
	물리 법칙에 대한 다른 훌륭한 회피 기술인 읽기 전용 복사는 (read-only
	replication)
	Chapter~\ref{chp:Deferred Processing} 에서 다뤄집니다.}

\iffalse

This chapter describes how to design software to take advantage of
modern commodity multicore systems by using idioms, or
``design patterns''~\cite{Alexander79,GOF95,SchmidtStalRohnertBuschmann2000v2Textbook},
to balance performance, scalability, and response time.
Correctly partitioned problems lead to simple, scalable, and
high-performance solutions, while poorly partitioned problems result
in slow and complex solutions.
This chapter will help you design partitioning into your code, with
some discussion of batching and weakening as well.
The word ``design'' is very important: You should partition first,
batch second, weaken third, and code fourth.
Changing this order often leads to poor performance and scalability
along with great frustration.\footnote{
	That other great dodge around the Laws of Physics, read-only
	replication, is covered in Chapter~\ref{chp:Deferred Processing}.}

\fi

그러므로, Section~\ref{sec:SMPdesign:Partitioning Exercises} 에서는 파티셔닝
(partitioning) 연습문제를 소개하고,
Section~\ref{sec:SMPdesign:Design Criteria} 에서는 파티셔닝 가능성 설계 영역을
리뷰하며,
Section~\ref{sec:SMPdesign:Synchronization Granularity}
에서 동기화 단위 크기 선택을 이야기 하고,
Section~\ref{sec:SMPdesign:Parallel Fastpath}
에서 일반적인 경우에 속도와 확장성을 제공하는 fastpath 를 제공하면서 일반적이지
않은 경우에는 더 간단하지만 덜 확장성 있는 ``slow path'' 를 사용하는, 중요한
병렬-fastpath 디자인 패턴을 개략적으로 살펴보며, 마지막으로
Section~\ref{sec:SMPdesign:Beyond Partitioning}
에서는 파티셔닝 다음 영역을 간략하게 바라봅니다.

\iffalse

To this end, Section~\ref{sec:SMPdesign:Partitioning Exercises}
presents partitioning exercises,
Section~\ref{sec:SMPdesign:Design Criteria} reviews partitionability
design criteria,
Section~\ref{sec:SMPdesign:Synchronization Granularity}
discusses synchronization granularity selection,
Section~\ref{sec:SMPdesign:Parallel Fastpath}
overviews important parallel-fastpath design patterns
that provide speed and scalability on common-case fastpaths while using
simpler less-scalable ``slow path'' fallbacks for unusual situations,
and finally
Section~\ref{sec:SMPdesign:Beyond Partitioning}
takes a brief look beyond partitioning.

\fi

\input{SMPdesign/partexercises}

\input{SMPdesign/criteria}

\section{Synchronization Granularity}
\label{sec:SMPdesign:Synchronization Granularity}
%
\epigraph{Doing little things well is a step toward doing big things better.}
	 {\emph{Harry F.~Banks}}

\begin{figure}[tb]
\centering
\resizebox{1.2in}{!}{\includegraphics{SMPdesign/LockGranularity}}
\caption{Design Patterns and Lock Granularity}
\label{fig:SMPdesign:Design Patterns and Lock Granularity}
\end{figure}

Figure~\ref{fig:SMPdesign:Design Patterns and Lock Granularity}
는 동기화 granularity 의 다른 단계들에 대한 그림을 보이는데, 각각이 다음
섹션들에서 설명됩니다.
이 섹션들은 기본적으로 락킹에 초점을 맞추고 있습니다만, 비슷한 granularity
이슈가 모든 형태의 동기화에 존재합니다.

\iffalse

Figure~\ref{fig:SMPdesign:Design Patterns and Lock Granularity}
gives a pictorial view of different levels of synchronization granularity,
each of which is described in one of the following sections.
These sections focus primarily on locking, but similar granularity
issues arise with all forms of synchronization.

\fi

\subsection{Sequential Program}
\label{sec:SMPdesign:Sequential Program}

프로그램이 단일 프로세서에서 충분히 빠르게 돌아간다면, 그리고 다른 프로세스,
쓰레드, 또는 인터럽트 핸들러와 상호작용을 하지 않는다면, 여러분은 동기화
도구들을 없애고 그것들의 오버헤드와 복잡도를 아껴야 합니다.
몇년 전, \IX{무어의 법칙} 이 최종적으로는 모든 프로그램을 이 카테고리로 강제로
위치시킬거라는 주장이 있었습니다.
하지만
Figure~\ref{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs}
에서 보이듯이, 싱글쓰레드 성능의 폭발적 증가는 2003년 즈음에 멈췄습니다.
따라서, 증가하는 성능은 지속적으로 병렬성을 필요로 할 겁니다.\footnote{
	이 그림은 이론상 클락당 하나 이상의 인스트럭션들을 끝낼 수 있는 최신의
	CPU 들에 대해서는 클락 주파수를 보이고, 가장 간단한 인스트럭션 하나를
	수행하는데에도 여러 클락을 필요로 하는 오래된 CPU 들에 대해서는 MIPS 를
	보이고 있습니다.
	이런 방법을 취하는 이유는 최신의 CPU 들의 클락당 여러 인스트럭션을 끝낼
	수 있는 능력은 일반적으로 메모리 시스템 성능에 의해 제한되기
	때문입니다.}
2006 년에 Paul 은 이 문장의 첫번째 버전을 듀얼코어 랩탑에서 타이핑 했다는 것을
생각해 보면, 그리고 2020년에 추가된 많은 그래프들이 소켓당 56개의 하드웨어
쓰레드를 가진 시스템에서 만들어졌다는 것을 생각해보면, 병렬성은 훌륭하고 정말
존재합니다.
또한 이더넷 대역폭이
Figure~\ref{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}
에 보인 바와 같이 지속적으로 성장하고 있음을 이야기 하는게 중요합니다.
이 성장은 통신 로드를 제어하기 위해 멀티쓰레드 서버 개발을 계속하는
모티베이션이 될 겁니다.

\iffalse

If the program runs fast enough on a single processor, and
has no interactions with other processes, threads, or interrupt
handlers, you should
remove the synchronization primitives and spare yourself their
overhead and complexity.
Some years back, there were those who would argue that \IX{Moore's Law}
would eventually force all programs into this category.
However, as can be seen in
Figure~\ref{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs},
the exponential increase in single-threaded performance halted in
about 2003.
Therefore,
increasing performance will increasingly require parallelism.\footnote{
	This plot shows clock frequencies for newer CPUs theoretically
	capable of retiring one or more instructions per clock, and MIPS for
	older CPUs requiring multiple clocks to execute even the
	simplest instruction.
	The reason for taking this approach is that the newer CPUs'
	ability to retire multiple instructions per clock is typically
	limited by memory-system performance.}
Given that back in 2006 Paul typed the first version of this sentence
on a dual-core laptop, and further given that many of the graphs added
in 2020 were generated on a system with 56~hardware threads per socket,
parallelism is well and truly here.
It is also important to note that Ethernet bandwidth is continuing to
grow, as shown in
Figure~\ref{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}.
This growth will continue to motivate multithreaded servers in order to
handle the communications load.

\fi

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/clockfreq}}
\caption{MIPS/Clock-Frequency Trend for Intel CPUs}
\label{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs}
\end{figure}

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/CPUvsEnet}}
\caption{Ethernet Bandwidth vs. Intel x86 CPU Performance}
\label{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}
\end{figure}

이는 여러분이 모든 프로그램을 멀티 쓰레드 방식으로 코딩해야 함을 의미하지
\emph{않음} 을 알아두시기 바랍니다.
다시 말하지만, 프로그램이 싱글 프로세서에서도 충분히 빨리 돌아간다면 SMP 동기화
기능들의 오버헤드와 복잡도를 아끼십시오.
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
의 해시 테이블 탐색 코드의 단순도는 이 점을 잘 보입니다.\footnote{
	이 섹션의 예제는 Hart 등의 것~\cite{ThomasEHart2006a} 에서 가져왔고,
	여러 파일에서 관련된 코드를 명쾌함을 위해 가져옴으로써 조금
	수정되었습니다.}
핵심은 병렬성으로 인한 속도 향상은 일반적으로 CPU 수에 의해 제한된다는
것입니다.
반면, 예를 들자면 조심스러운 데이터 구조의 선택과 같은 순차적 최적화로 인한
속도 향상은 얼마든지 클 수 있습니다.

달리 말하면, 여러분이 이런 행복한 상황에 처해 있지 않다면, 계속 읽으세요!

\iffalse

Please note that this does \emph{not} mean that you should code each
and every program in a multi-threaded manner.
Again, if a program runs quickly enough on a single processor,
spare yourself the overhead and complexity of SMP synchronization
primitives.
The simplicity of the hash-table lookup code in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
underscores this point.\footnote{
	The examples in this section are taken from Hart et
	al.~\cite{ThomasEHart2006a}, adapted for clarity
	by gathering related code from multiple files.}
A key point is that speedups due to parallelism are normally
limited to the number of CPUs.
In contrast, speedups due to sequential optimizations, for example,
careful choice of data structure, can be arbitrarily large.

On the other hand, if you are not in this happy situation, read on!

\fi

\begin{listing}[tbhp]
\begin{VerbatimL}[commandchars=\\\[\]]
struct hash_table
{
	long nbuckets;
	struct node **buckets;
};

typedef struct node {
	unsigned long key;
	struct node *next;
} node_t;

int hash_search(struct hash_table *h, long key)
{
	struct node *cur;

	cur = h->buckets[key % h->nbuckets];
	while (cur != NULL) {
		if (cur->key >= key) {
			return (cur->key == key);
		}
		cur = cur->next;
	}
	return 0;
}
\end{VerbatimL}
\caption{Sequential-Program Hash Table Search}
\label{lst:SMPdesign:Sequential-Program Hash Table Search}
\end{listing}

% ./test_hash_null.exe 1000 0/100 1 1024 1
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 96.2913  max = 98.2337  min = 90.4095  std = 2.95314
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 91.5592  max = 97.3315  min = 89.9885  std = 2.88925
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 93.3568  max = 106.162  min = 89.8828  std = 6.40418

\subsection{Code Locking}
\label{sec:SMPdesign:Code Locking}

코드 락킹은 그것이 글로벌 락을 사용한다는 사실 때문에 무척
간단합니다.\footnote{
	여러분이 그대신 데이터 구조들을 위해 여러 락을 사용한다면, 또는 Java 의
	경우 동기화된 인스턴스들을 가지고 클래스들을 사용한다면, 여러분은
	Section~\ref{sec:SMPdesign:Data Locking} 에서 이야기 되는 ``데이터
	락킹'' 을 사용하고 있는 겁니다.}
이 기법은 특히 존재하는 프로그램을 멀티프로세서에서 돌아갈 수 있게끔 코드
락킹을 사용하도록 개량하기가 쉽습니다.  만약 이 프로그램이 하나의 공유 리소스만
가지고 있다면, 코드 락킹은 최적의 성능을 제공할 수도 있습니다.
하지만, 많은 더 크도 복잡한 프로그램들이 많은 수행들이 크리티컬 섹션에서 일어날
것을 요구하며, 이는 결국 코드 락킹이 그 확장성을 제한하게끔 만듭니다.

따라서, 여러분은 크리티컬 섹션 또는 약간의 확장만이 필요한 곳에서 수행 시간의
작은 부분만을 소모하는 프로그램에서 코드 락킹을 사용해야 합니다.
이런 경우에, 코드 락킹은
Listing~\ref{lst:SMPdesign:Code-Locking Hash Table Search}
에서 보이는 것과 같이 그 순차적 버전과 매우 비슷한 비교적 간단한 프로그램을
제공할 겁니다.
하지만,
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
의 \co{hash_search()} 에서의 간단한 비교 결과 리턴은 이 리턴 전에 락을 해제해야
하므로 세개의 선언문이 되었음을 알아 두시기 바랍니다.

\iffalse

Code locking is quite simple due to the fact that it uses only
global locks.\footnote{
	If your program instead has locks in data structures,
	or, in the case of Java, uses classes with synchronized
	instances, you are instead using ``data locking'', described
	in Section~\ref{sec:SMPdesign:Data Locking}.}
It is especially
easy to retrofit an existing program to use code locking in
order to run it on a multiprocessor.  If the program has
only a single shared resource, code locking will even give
optimal performance.
However, many of the larger and more complex programs
require much of the execution to
occur in critical sections, which in turn causes code locking
to sharply limits their scalability.

Therefore, you should use code locking on programs that spend
only a small fraction of their execution time in critical sections or
from which only modest scaling is required.  In these cases,
code locking will provide a relatively simple program that is
very similar to its sequential counterpart,
as can be seen in
Listing~\ref{lst:SMPdesign:Code-Locking Hash Table Search}.
However, note that the simple return of the comparison in
\co{hash_search()} in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
has now become three statements due to the need to release the
lock before returning.

\fi

\begin{listing}[tbhp]
\begin{VerbatimL}[commandchars=\\\[\]]
spinlock_t hash_lock;

struct hash_table
{
	long nbuckets;
	struct node **buckets;
};

typedef struct node {
	unsigned long key;
	struct node *next;
} node_t;

int hash_search(struct hash_table *h, long key)
{
	struct node *cur;
	int retval;

	spin_lock(&hash_lock);
	cur = h->buckets[key % h->nbuckets];
	while (cur != NULL) {
		if (cur->key >= key) {
			retval = (cur->key == key);
			spin_unlock(&hash_lock);
			return retval;
		}
		cur = cur->next;
	}
	spin_unlock(&hash_lock);
	return 0;
}
\end{VerbatimL}
\caption{Code-Locking Hash Table Search}
\label{lst:SMPdesign:Code-Locking Hash Table Search}
\end{listing}

불행히도, 코드 락킹은 특히 여러 CPU 들이 동시에 락을 획득하려 할 때 벌어지는
``락 경쟁'' 에 취약합니다.
어린 아이들의 그룹을 (또는 아이처럼 행동하는 노인들의 그룹을) 관리해야 하는 SMP
프로그래머들은
Figure~\ref{fig:SMPdesign:Lock Contention}. 에 그려진 것과 같은, 그 중 하나만을
가질 때의 위험을 알고 있을 겁니다.

\iffalse

Unfortunately, code locking is particularly prone to ``lock contention'',
where multiple CPUs need to acquire the lock concurrently.
SMP programmers who have taken care of groups of small children
(or groups of older people who are acting like children) will immediately
recognize the danger of having only one of something,
as illustrated in Figure~\ref{fig:SMPdesign:Lock Contention}.

\fi

% ./test_hash_codelock.exe 1000 0/100 1 1024 1
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 164.115  max = 170.388  min = 161.659  std = 3.21857
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 181.17  max = 198.4  min = 162.459  std = 15.8585
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 167.651  max = 189.014  min = 162.144  std = 10.6819

% ./test_hash_codelock.exe 1000 0/100 1 1024 2
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 378.481  max = 385.971  min = 374.235  std = 4.05934
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 753.414  max = 1015.28  min = 377.734  std = 294.942
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 502.737  max = 980.924  min = 374.406  std = 239.383

이 문제에 대한 한가지 해결책, 이름하여 ``data locking'' 이 다음 섹션에서
설명됩니다.

\iffalse

One solution to this problem, named ``data locking'', is described
in the next section.

\fi

\begin{figure}[tbh]
\centering
\resizebox{2.5in}{!}{\includegraphics{cartoons/r-2014-Data-one-fighting}}
\caption{Lock Contention}
\ContributedBy{Figure}{fig:SMPdesign:Lock Contention}{Melissa Broussard}
\end{figure}

\subsection{Data Locking}
\label{sec:SMPdesign:Data Locking}

많은 데이터 구조가 쪼개지고 이 데이터 구조의 각 조각이 각자의 락을 가질 수
있습니다.
그러면 각 데이터 구조의 조각을 위한 크리티컬 섹션은 병렬로 수행될 수 있습니다,
각 조각을 위한 크리티컬 섹션은 한번에 하나씩만 수행될 수 있긴 하지만요.
경쟁이 줄어야 할 때, 그리고 동기화 오버헤드가 속도 향상을 제한하지 않을 때,
여러분은 데이터 락킹을 사용해야 합니다.
데이터 락킹은 지나치게 큰 크리티컬 섹션을 여러 데이터 구조로 분산시켜서 경쟁을
줄이는데, 예를 들어
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}
에 보인 것과 같이 해시테이블에 해시 버킷별 크리티컬 섹션을 유지하는 식입니다.
그렇게 증가된 확장성은 역시 추가된 데이터 구조, \co{struct bucket} 의 형태로
복잡도를 약간 높입니다.

\iffalse

Many data structures may be partitioned,
with each partition of the data structure having its own lock.
Then the critical sections for each part of the data structure
can execute in parallel,
although only one instance of the critical section for a given
part could be executing at a given time.
You should use data locking when contention must
be reduced, and where synchronization overhead is not
limiting speedups.
Data locking reduces contention by distributing the instances
of the overly-large critical section across multiple data structures,
for example, maintaining per-hash-bucket critical sections in a
hash table, as shown in
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}.
The increased scalability again results in a slight increase in complexity
in the form of an additional data structure, the \co{struct bucket}.

\fi

\begin{listing}[tb]
\begin{VerbatimL}
struct hash_table
{
	long nbuckets;
	struct bucket **buckets;
};

struct bucket {
	spinlock_t bucket_lock;
	node_t *list_head;
};

typedef struct node {
	unsigned long key;
	struct node *next;
} node_t;

int hash_search(struct hash_table *h, long key)
{
	struct bucket *bp;
	struct node *cur;
	int retval;

	bp = h->buckets[key % h->nbuckets];
	spin_lock(&bp->bucket_lock);
	cur = bp->list_head;
	while (cur != NULL) {
		if (cur->key >= key) {
			retval = (cur->key == key);
			spin_unlock(&bp->bucket_lock);
			return retval;
		}
		cur = cur->next;
	}
	spin_unlock(&bp->bucket_lock);
	return 0;
}
\end{VerbatimL}
\caption{Data-Locking Hash Table Search}
\label{lst:SMPdesign:Data-Locking Hash Table Search}
\end{listing}

Figure~\ref{fig:SMPdesign:Lock Contention} 에 보인 것과 같은 경쟁적인 상황과
반대로, 데이터 락킹은
Figure~\ref{fig:SMPdesign:Data Locking} 에 보인 것과 같이 하모니를 조장하며
병렬 프로그램에서 이는 \emph{거의} 항상 향상된 성능과 확장성으로 귀결됩니다.
이런 이유로, 데이터 락킹은 Sequent 에 의해 그 커널에서 무척 많이
사용되었습니다~\cite{Beck85,Inman85,Garg90,Dove90,McKenney92b,McKenney92a,McKenney93}.

\iffalse

In contrast with the contentious situation
shown in Figure~\ref{fig:SMPdesign:Lock Contention},
data locking helps promote harmony, as illustrated by
Figure~\ref{fig:SMPdesign:Data Locking}---and in parallel programs,
this \emph{almost} always translates into increased performance and
scalability.
For this reason, data locking was heavily used by Sequent in its
kernels~\cite{Beck85,Inman85,Garg90,Dove90,McKenney92b,McKenney92a,McKenney93}.

\fi

\begin{figure}[tbh]
\centering
\resizebox{2.4in}{!}{\includegraphics{cartoons/r-2014-Data-many-happy}}
\caption{Data Locking}
\ContributedBy{Figure}{fig:SMPdesign:Data Locking}{Melissa Broussard}
\end{figure}

% ./test_hash_spinlock.exe 1000 0/100 1 1024 1
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 158.118  max = 162.404  min = 156.199  std = 2.19391
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 157.717  max = 162.446  min = 156.415  std = 2.36662
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 158.369  max = 164.75  min = 156.501  std = 3.19454

% ./test_hash_spinlock.exe 1000 0/100 1 1024 2
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 223.426  max = 422.948  min = 167.858  std = 100.136
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 235.462  max = 507.134  min = 167.466  std = 135.836
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 305.807  max = 481.685  min = 167.939  std = 132.589

하지만, 어린 아이들을 맡아본 사람들은 또한번 증언할 수 있겠지만, 가지고 놀기
충분한 것을 주는 것만으로는 평안을 보장할 수 없습니다.
비슷한 상황이 SMP 프로그램에서도 일어날 수 있습니다.
예를 들어, 리눅스 커널은 파일과 디렉토리들의 캐쉬 (``dcache'' 라 불립니다) 를
유지합니다.
이 캐쉬의 각 항목은 각자의 락을 가지고 있습니다만, 루트 디렉토리와 그 직접적
후손에 연관된 항목들은 다른 것들에 비해 훨씬 더 자주 접근될 가능성이 높습니다.
이는 많은 CPU 들이 이런 인기있는 항목들의 락에 대해 경쟁하게 되는 상황을
초래하고, 이는
Figure~\ref{fig:SMPdesign:Data and Skew} 에 보인 것과 다르지 않은 상황을 초래할
수 있습니다.

\iffalse

However, as those who have taken care of small children can again attest,
even providing enough to go around is no guarantee of tranquillity.
The analogous situation can arise in SMP programs.
For example, the Linux kernel maintains a cache of files and directories
(called ``dcache'').
Each entry in this cache has its own lock, but the entries corresponding
to the root directory and its direct descendants are much more likely to
be traversed than are more obscure entries.
This can result in many CPUs contending for the locks of these popular
entries, resulting in a situation not unlike that
shown in Figure~\ref{fig:SMPdesign:Data and Skew}.

\fi

\begin{figure}[tbh]
\centering
\resizebox{\twocolumnwidth}{!}{\includegraphics{cartoons/r-2014-Data-many-fighting}}
\caption{Data Locking and Skew}
\ContributedBy{Figure}{fig:SMPdesign:Data and Skew}{Melissa Broussard}
\end{figure}

많은 경우, 알고리즘은 데이터 편중을 줄이도록 설계될 수 있고, 어떤 경우에는 이를
완전히 제거할 수도 있습니다 (예를 들면, 리눅스 커널의 dcache
에서처럼요~\cite{McKenney04a,JonathanCorbet2010dcacheRCU,NeilBrown2015PathnameLookup,NeilBrown2015RCUwalk,NeilBrown2015PathnameSymlinks}).
데이터 락킹은 해쉬 테이블과 같은 쪼개질 수 있는 데이터 구조에서, 그리고 여러
항목이 각각 주어진 데이터 구조의 인스턴스를 나타내는 상황에서도 종종
사용됩니다.
리눅스 커널의 task 리스트는 뒤의 것의 한 예로, 각 task 구조체는 각자의
\co{alloc_lock} 과 \co{pi_lock} 을 갖습니다.

동적으로 할당되는 구조체들에 대한 데이터 락킹에서의 주요 과제는 이 구조체가 이
락이 잡혀 있는 동안 존재를 유지함을 보장하는 것입니다~\cite{Gamsa99}.
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}
의 코드는 이 과제를 락들을 정적으로 할당된, 따라서 결코 해제되지 않는 해시
버킷에 위치시킴으로써 해결했습니다.
하지만, 이 트릭은 해시 테이블의 크기가 변할 수 있다면, 따라서 락이 이제
동적으로 할당되어야 한다면 동작하지 않을 겁니다.
이 경우, 해시 버킷은 그 락이 획득되어 있는 동안은 해제되는 것을 방지할 방법이
필요할 겁니다.

\iffalse

In many cases, algorithms can be designed to reduce the instance of
data skew, and in some cases eliminate it entirely
(for example, in the Linux kernel's
dcache~\cite{McKenney04a,JonathanCorbet2010dcacheRCU,NeilBrown2015PathnameLookup,NeilBrown2015RCUwalk,NeilBrown2015PathnameSymlinks}).
Data locking is often used for partitionable data structures such as
hash tables, as well as in situations where multiple entities are each
represented by an instance of a given data structure.
The Linux-kernel task list is an example of the latter, each task
structure having its own \co{alloc_lock} and \co{pi_lock}.

A key challenge with data locking on dynamically allocated structures
is ensuring that the structure remains in existence while the lock is
being acquired~\cite{Gamsa99}.
The code in
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}
finesses this challenge by placing the locks in the statically allocated
hash buckets, which are never freed.
However, this trick would not work if the hash table were resizeable,
so that the locks were now dynamically allocated.
In this case, there would need to be some means to prevent the hash
bucket from being freed during the time that its lock was being acquired.

\fi

\QuickQuiz{
	어떤 구조체를 그것의 락이 획득되어 있는 동안 해제되지 않게 하는
	방법들로는 어떤 게 있나요?

	\iffalse

	What are some ways of preventing a structure from being freed while
	its lock is being acquired?

	\fi

}\QuickQuizAnswer{
	여기 \emph{존재 보장} 문제에 대한 몇가지 해결책이 있습니다:

	\begin{enumerate}
	\item	구조체별 락이 획득되어있는 동안 잡히는 정적으로 할당된 락을
		제공하는 것으로, 이는 계층적 락킹의 한 예입니다
		(Section~\ref{sec:SMPdesign:Hierarchical Locking} 을
		참고하세요).
		물론, 이 목적으로 하나의 전역 락을 사용하는 것은 받아들이기
		어려울 정도로 높은 수준의 락 경쟁을 일으켜서 성능과 확장성을
		극적으로 떨어뜨릴 겁니다.
	\item	Chapter~\ref{chp:Locking} 에서 보인 것처럼 정적으로 할당된
		락들의 배열을 제공하고, 이 구조체의 주소를 획득할 락을 선택하기
		위해 해싱하는 것.
		충분히 높은 품질의 해시 함수가 있다면, 이는 단일 전역 락의
		확장성 제한을 막아줍니다만, 읽기가 대부분인 상황에서는 이 락
		획득 오버헤드가 받아들이기 어려울 정도의 성능 하락을 초래할 수
		있습니다.
	\iffalse

	Here are a few possible solutions to this \emph{existence guarantee}
	problem:

	\begin{enumerate}
	\item	Provide a statically allocated lock that is held while
		the per-structure lock is being acquired, which is an
		example of hierarchical locking (see
		Section~\ref{sec:SMPdesign:Hierarchical Locking}).
		Of course, using a single global lock for this purpose
		can result in unacceptably high levels of lock contention,
		dramatically reducing performance and scalability.
	\item	Provide an array of statically allocated locks, hashing
		the structure's address to select the lock to be acquired,
		as described in Chapter~\ref{chp:Locking}.
		Given a hash function of sufficiently high quality, this
		avoids the scalability limitations of the single global
		lock, but in read-mostly situations, the lock-acquisition
		overhead can result in unacceptably degraded performance.

	\fi

	\item	가비지 콜렉터를 제공하는 소프트웨어 환경이라면 그것을 사용해서
		구조체가 참조되는 동안은 할당 해제되지 못하게 하는 방법이
		있습니다.
		이는 무척 잘 동작하고, 개발자들의 어깨로부터 존재 보장 부담을
		(그 외에도 많은 것을) 제거합니다만, 프로그램에 가비지 콜렉션
		오버헤드를 부과합니다.
		가비지 콜렉션 기술은 지난 수십년간 상당히 발전했긴 합니다만, 그
		오버헤드는 어떤 어플리케이션들에게는 받아들이기 어려울 정도로
		높을 수 있습니다.
		또한, 일부 어플리케이션은 개발자들이 대부분의 가비지 콜렉션
		기반 환경들이 허용하는 것에 비해 데이터 구조의 레이아웃과
		위치에 더 많은 제어를 필요로 하기도 합니다.
	\item	가비지 콜렉터의 한 특수한 경우로, 전역 레퍼런스 카운터를, 또는
		레퍼런스 카운터의 전역적 배열을 사용하는 것.
		이는 앞의 락을 사용하는 방법에서 이야기 된 것과 유사한 강점과
		한계점을 갖습니다.

	\iffalse

	\item	Use a garbage collector, in software environments providing
		them, so that a structure cannot be deallocated while being
		referenced.
		This works very well, removing the existence-guarantee
		burden (and much else besides) from the developer's
		shoulders, but imposes the overhead of garbage collection
		on the program.
		Although garbage-collection technology has advanced
		considerably in the past few decades, its overhead
		may be unacceptably high for some applications.
		In addition, some applications require that the developer
		exercise more control over the layout and placement of
		data structures than is permitted by most garbage collected
		environments.
	\item	As a special case of a garbage collector, use a global
		reference counter, or a global array of reference counters.
		These have strengths and limitations similar to those
		called out above for locks.

	\fi

	\item	내부에서 바깥으로의 레퍼런스 카운트라고 생각될 수 있는,
		\emph{해저드 포인터} 를 사용하는 것.
		해저드 포인터 기반 알고리즘은 쓰레드별 포인터 리스트를
		유지하여서, 이 리스트에 특정 포인터가 보이는 것은 해당
		구조체로의 레퍼런스처럼 동작합니다.
		해저드 포인터는 프로덕션에서의 상당한 사용을 보이기 시작하고
		있습니다
		(\cref{sec:defer:Production Uses of Hazard Pointers} 를
		참고하세요).
	\item	트랜잭셔널 메모리 (Transactional Memory:
		TM)~\cite{Herlihy93a,DBLomet1977SIGSOFT,Shavit95} 를 사용해서
		이 데이터 구조로의 모든 참조와 수정이 어토믹하게 수행되도록
		하는 것.
		TM 이 최근 몇년간 많은 흥분을 일으켰고 프로덕션 소프트웨어에서
		일부 사용될 가능성이 보이긴 합니다만, 개발자들은 일부 주의를
		해야하는데~\cite{Blundell2005DebunkTM,Blundell2006TMdeadlock,McKenney2007PLOSTM},
		특히 성능에 중요한 코드에서 그렇습니다.
		특히, 존재 보장은 이 트랜잭션인 전역 레퍼런스로부터 업데이트
		되는 데이터 항목으로의 전체 과정을 커버할 것을 필요로 합니다.
		그것을 다른 동기화 메커니즘들과 결합해 그 취약성을 극복하는
		방법들을 포함한 TM 에 대한 더 많은 정보를 위해선
		\cref{sec:future:Transactional Memory,sec:future:Hardware
		Transactional Memory} 를 참고하세요.

	\iffalse

	\item	Use \emph{hazard pointers}~\cite{MagedMichael04a}, which
		can be thought of as an inside-out reference count.
		Hazard-pointer-based algorithms maintain a per-thread list of
		pointers, so that the appearance of a given pointer on
		any of these lists acts as a reference to the corresponding
		structure.
		Hazard pointers are starting to see significant production use
		(see \cref{sec:defer:Production Uses of Hazard Pointers}).
	\item	Use transactional memory
		(TM)~\cite{Herlihy93a,DBLomet1977SIGSOFT,Shavit95},
		so that each reference and
		modification to the data structure in question is
		performed atomically.
		Although TM has engendered much excitement in recent years,
		and seems likely to be of some use in production software,
		developers should exercise some
		caution~\cite{Blundell2005DebunkTM,Blundell2006TMdeadlock,McKenney2007PLOSTM},
		particularly in performance-critical code.
		In particular, existence guarantees require that the
		transaction covers the full path from a global reference
		to the data elements being updated.
		For more on TM, including ways to overcome some of its
		weaknesses by combining it with other synchronization
		mechanisms, see
		\cref{sec:future:Transactional Memory,sec:future:Hardware Transactional Memory}.

	\fi

	\item	극단적으로 가벼운 가비지 콜렉터 같은 것이라 생각될 수 있는 RCU
		를 사용하는 것.
		업데이트 쓰레드는 RCU 읽기 쓰레드가 아직 참조하고 있을 수 있는
		RCU 로 보호되는 데이터 구조체를 할당 해제할 수 없게 되어
		있습니다.
		RCU 는 읽기가 대부분인 데이터 구조체에서 무척 많이 사용되고
		있으며,
		\cref{sec:defer:Read-Copy Update (RCU)} 에서 길게 이야기
		됩니다.

	\iffalse

	\item	Use RCU, which can be thought of as an extremely lightweight
		approximation to a garbage collector.
		Updaters are not permitted to free RCU-protected
		data structures that RCU readers might still be referencing.
		RCU is most heavily used for read-mostly data structures,
		and is discussed at length in
		\cref{sec:defer:Read-Copy Update (RCU)}.

	\fi

	\end{enumerate}

	존재 보장의 제공에 대한 더 많은 내용을 위해선,
	Chapters~\ref{chp:Locking} 와 \ref{chp:Deferred Processing} 를
	읽어보시기 바랍니다.

	\iffalse

	For more on providing existence guarantees, see
	Chapters~\ref{chp:Locking} and \ref{chp:Deferred Processing}.

	\fi

}\QuickQuizEnd

\subsection{Data Ownership}
\label{sec:SMPdesign:Data Ownership}

데이터 소유권 (data ownership) 은 주어진 데이터 구조를 쓰레드 또는 CPU 들에게
분할하여, 각 Thread/CPU 가 각자의 데이터 구조 부분을 아무런 동기화 오버헤드
없이 액세스 할 수 있게 합니다.
하지만, 한 쓰레드가 어떤 다른 쓰레드의 데이터에 액세스 하고자 한다면, 이 첫번째
쓰레드는 직접 그럴 수는 없습니다.
그 대신, 그 첫번째 쓰레드는 이 두번째 쓰레드와 통신을 해서 이 두번째 쓰레드가
그 첫번째 쓰레드 대신 이 오퍼레이션을 수행하거나, 그 데이터를 첫번째 쓰레드에게
넘겨줘야만 합니다.

데이터 소유권은 애매해 보일 수도 있겠습니다만, 매우 자주 사용됩니다:
\begin{enumerate}
\item	한 CPU 또는 쓰레드에 의해서만 액세스 가능한 모든 변수는 (C 와 C++
	에서의 {\tt auto} 변수 같은) 해당 CPU 또는 쓰레드에게 소유됩니다.
\item	유저 인터페이스의 한 인스턴스는 연관된 사용자의 컨텍스트를 소유합니다.
	병렬 데이터베이스 엔진과 상호작용하는 어플리케이션들이 완전한 순차적
	프로그램인 것마냥 작성되는 것은 매우 흔한 일입니다.
	그런 어플리케이션은 유저 인터페이스와 사용자의 현재 동작을 소유합니다.
	따라서 명시적 병렬성은 데이터베이스 엔진 스스로에게 국한됩니다.
\item	패러미터를 가질 수 있는 시뮬레이션들은 각 쓰레드가 패러미터 공간의 특정
	영역에 대한 소유권을 각 쓰레드가 갖게 함으로써 종종 간단히 병렬화
	됩니다.
	이런 종류의 문제를 위해 설계된 컴퓨팅 프레임웍도
	있습니다~\cite{BOINC2008}.
\end{enumerate}

\iffalse

Data ownership partitions a given data structure over the threads
or CPUs, so that each thread/CPU accesses its subset of the data
structure without any synchronization overhead whatsoever.
However, if one thread wishes to access some other thread's data,
the first thread is unable to do so directly.
Instead, the first thread must communicate with the second thread,
so that the second thread performs the operation on behalf of the
first, or, alternatively, migrates the data to the first thread.

Data ownership might seem arcane, but it is used very frequently:
\begin{enumerate}
\item	Any variables accessible by only one CPU or thread
	(such as {\tt auto} variables in C
	and C++) are owned by that CPU or thread.
\item	An instance of a user interface owns the corresponding
	user's context.  It is very common for applications
	interacting with parallel database engines to be
	written as if they were entirely sequential programs.
	Such applications own the user interface and the user's current
	action.  Explicit parallelism is thus confined to the
	database engine itself.
\item	Parametric simulations are often trivially parallelized
	by granting each thread ownership of a particular region
	of the parameter space.
	There are also computing frameworks designed for this
	type of problem~\cite{BOINC2008}.
\end{enumerate}

\fi

상당한 데이터 공유가 있다면, 이 쓰레드 또는 CPU 들 간의 통신은 상당한 복잡도와
오버헤드를 초래할 수 있습니다.
더 나아가, 가장 많이 사용되는 데이터가 단일 CPU 에게 소유된다면, 해당 CPU 는
``핫스팟'' 이 될 것이고, Figure~\ref{fig:SMPdesign:Data and Skew} 에서 보인
것과 같은 결과에 이르게 할 수 있습니다.
하지만, 공유가 필요치 않은 상황에서는 데이터 소유권이 이상적인 성능을 이루며,
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search} 에 보인 것처럼
순차적 프로그램만큼이나 코드도 단순해 집니다.
그런 상황은 종종 ``당황스러울 만큼 병렬적'' 이라고 이야기 되며, 가장 좋은
상황에서는 앞서 Figure~\ref{fig:SMPdesign:Data Locking} 에서 보인 것과 같은
상황을 가능하게 합니다.

\iffalse

If there is significant sharing, communication between the threads
or CPUs can result in significant complexity and overhead.
Furthermore, if the most-heavily used data happens to be that owned
by a single CPU, that CPU will be a ``hot spot'', sometimes with
results resembling that shown in Figure~\ref{fig:SMPdesign:Data and Skew}.
However, in situations where no sharing is required, data ownership
achieves ideal performance, and with code that can be as simple
as the sequential-program case shown in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}.
Such situations are often referred to as ``embarrassingly
parallel'', and, in the best case, resemble the situation
previously shown in Figure~\ref{fig:SMPdesign:Data Locking}.

\fi

% ./test_hash_null.exe 1000 0/100 1 1024 1
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 96.2913  max = 98.2337  min = 90.4095  std = 2.95314
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 91.5592  max = 97.3315  min = 89.9885  std = 2.88925
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 93.3568  max = 106.162  min = 89.8828  std = 6.40418

% ./test_hash_null.exe 1000 0/100 1 1024 2
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 45.4526  max = 46.4281  min = 45.1954  std = 0.487791
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 46.0238  max = 49.2861  min = 45.1852  std = 1.63127
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 46.6858  max = 52.6278  min = 45.1761  std = 2.97102

데이터 소유권의 또다른 중요한 예는 데이터 읽기 전용일 때 일어나는데, 모든
쓰레드가 복사를 통해 그것을 ``소유'' 합니다.

데이터 소유권은 Chapter~\ref{chp:Data Ownership} 에서 더 자세히 다뤄집니다.

\iffalse

Another important instance of data ownership occurs when the data
is read-only, in which case,
all threads can ``own'' it via replication.

Data ownership will be presented in more detail in
Chapter~\ref{chp:Data Ownership}.

\fi

\subsection{Locking Granularity and Performance}
\label{sec:SMPdesign:Locking Granularity and Performance}

이 섹션은 수학적 동기화 효율성 관점에서 락킹 세밀도와 성능을 바라봅니다.
수학에 관심이 없는 독자 여러분은 이 섹션을 건너뛰실 수도 있겠습니다.

이는 M/M/1 큐 (queue) 에 기반해 하나의 공유 전역 변수에 대해 동작하는 동기화
메커니즘의 효율성을 위한 거친 큐잉 (queueing) 모델을 사용합니다.
M/M/1 큐잉 모델은 지수적으로 분산된 ``inter-arrival rate'' $\lambda$ 와
지수적으로 분산된 ``service rate'' $\mu$ 에 기반합니다.
이 inter-arrival rate $\lambda$ 는 이 시스템이 동기화로부터 자유로웠더라면 초당
수행했을 동기화 오퍼레이션의 평균 횟수로 생각될 수 있는데, 달리 말하면
$\lambda$ 는 각 동기화가 아닌 일 단위의 오버헤드의 반대 오버헤드입니다.
예를 들어, 만약 각 일의 단위가 트랜잭션이라면, 그리고 각 트랜잭션이 수행되는데
1 밀리세컨드를 소요한다면, 동기화 오버헤드를 제외하고, $\lambda$ 는 초당 1,000
트랜잭션이 됩니다.

\iffalse

This section looks at locking granularity and performance from
a mathematical synchronization\-/efficiency viewpoint.
Readers who are uninspired by mathematics might choose to skip
this section.

The approach is to use a crude queueing model for the efficiency of
synchronization mechanism that operate on a single shared global
variable, based on an M/M/1 queue.
M/M/1 queuing models are based on an exponentially distributed
``inter-arrival rate'' $\lambda$ and an exponentially distributed
``service rate'' $\mu$.
The inter-arrival rate $\lambda$ can be thought of as the average
number of synchronization operations per second that the system
would process if the synchronization were free, in other words,
$\lambda$ is an inverse measure of the overhead of each non-synchronization
unit of work.
For example, if each unit of work was a transaction, and if each transaction
took one millisecond to process, excluding synchronization overhead,
then $\lambda$ would be 1,000~transactions per second.

\fi

Service rate $\mu$ 는 비슷하게 정의됩니다만, 각 트랜잭션 오버헤드가 0일 때,
그리고 CPU 들이 서로의 동기화 오퍼레이션의 완료를 기다려야 한다는 사실을 무시할
때 이 시스템이 처리할 초당 동기화 오퍼레이션의 평균 수를 의미하는데, 달리
말하자면 $\mu$ 는 경쟁이 없을 때의 동기화 오버헤드라고 대략 생각될 수 있습니다.
예를 들어, 각 트랜잭션의 동기화 오퍼레이션이 하나의 어토믹 값 증가 인스트럭션을
사용하며, 이 컴퓨터 시스템은 각 CPU 에서 5 나노세컨드마다 지역변수의 어토믹 값
증가가 가능하다고 해 봅시다 (\cref{fig:count:Atomic Increment Scalability on
x86} 를 참고하세요).\footnote{
	물론, 같은 공유 변수를 값 증가시키는 8개의 CPU 가 존재한다면, 각 CPU 는
	자신의 값 증가를 위한 5 나노세컨드를 사용하기 전에 다른 CPU 들이 모두
	값 증가를 할 수 있도록 최소 35 나노세컨드를 기다려야 합니다.
	실제로, 그 기다림은 이 변수를 한 CPU 에서 다른 CPU 로 옮겨야 하는
	필요성 때문에 더 길어질 겁니다.}
따라서 $\mu$ 의 값은 초당 200,000,000 어토믹 값 증가가 됩니다.

물론, $\lambda$ 의 값은 공유 변수의 값을 증가하는 CPU 의 수가 늘어날수록
증가되는데, 각 CPU 는 개별적으로 트랜잭션을 처리할 수 있기 때문입니다 (다시
말하지만, 동기화를 무시한 겁니다):

\iffalse

The service rate $\mu$ is defined similarly, but for the average
number of synchronization operations per second that the system
would process if the overhead of each transaction was zero, and
ignoring the fact that CPUs must wait on each other to complete
their synchronization operations, in other words, $\mu$ can be roughly
thought of as the synchronization overhead in absence of contention.
For example, suppose that each transaction's synchronization operation
involves an atomic increment instruction, and that a computer system is
able to do a private-variable atomic increment every 5~nanoseconds on
each CPU
(see \cref{fig:count:Atomic Increment Scalability on x86}).\footnote{
	Of course, if there are 8 CPUs all incrementing the same
	shared variable, then each CPU must wait at least 35~nanoseconds
	for each of the other CPUs to do its increment before consuming
	an additional 5~nanoseconds doing its own increment.
	In fact, the wait will be longer due to the need
	to move the variable from one CPU to another.}
The value of $\mu$ is therefore about 200,000,000 atomic increments
per second.

Of course, the value of $\lambda$ increases as increasing numbers of CPUs
increment a shared variable because each CPU is capable of processing
transactions independently (again, ignoring synchronization):

\fi

\begin{equation}
	\lambda = n \lambda_0
\end{equation}

여기서 $n$ 은 CPU 의 수이고 $\lambda_0$ 는 단일 CPU 의 트랜잭션 처리
능력입니다.
경쟁이 없을 때 단일 CPU 가 단일 트랜잭션을 처리하는데 걸릴 것으로 예상되는
시간은 $1 / \lambda_0$ 임을 알아 두시기 바랍니다.

CPU 들은 서로가 이 단일 공유 변수의 값을 증가할 기회를 위해 ``줄을 서서
기다려야'' 하기 때문에, 예상되는 전체 대기 시간을 위해 M/M/1 큐잉 모델 표현을
사용할 수 있습니다:

\iffalse

Here, $n$ is the number of CPUs and $\lambda_0$ is the transaction-processing
capability of a single CPU\@.
Note that the expected time for a single CPU to execute a single transaction
in the absence of contention is $1 / \lambda_0$.

Because the CPUs have to ``wait in line'' behind each other to get their
chance to increment the single shared variable, we can use the M/M/1
queueing-model expression for the expected total waiting time:

\fi

\begin{equation}
	T = \frac{1}{\mu - \lambda}
\end{equation}

앞의 $\lambda$ 값을 대입해 보면:

\iffalse

Substituting the above value of $\lambda$:

\fi

\begin{equation}
	T = \frac{1}{\mu - n \lambda_0}
\end{equation}

이제, 효율성은 동기화가 있을 때 트랜잭션 하나를 처리하는데 드는 시간 ($T + 1 /
\lambda_0$) 대비 동기화가 없을 때의 그것의 ($1 / \lambda_0$) 비율입니다:

\iffalse

Now, the efficiency is just the ratio of the time required to process
a transaction in absence of synchronization ($1 / \lambda_0$)
to the time required including synchronization ($T + 1 / \lambda_0$):

\fi

\begin{equation}
	e = \frac{1 / \lambda_0}{T + 1 / \lambda_0}
\end{equation}

앞의 $T$ 값을 대입하고 간략화 하면:

\iffalse

Substituting the above value for $T$ and simplifying:

\fi

\begin{equation}
	e = \frac{\frac{\mu}{\lambda_0} - n}{\frac{\mu}{\lambda_0} - (n - 1)}
\end{equation}

하지만 $\mu / \lambda_0$ 의 값은 동기화 오버헤드 자체 (컨텐션 부재시) 대비
트래잭션 처리에 걸리는 시간 (동기화 오버헤드 부재시) 의 비율입니다.
우리가 이 비율을 $f$ 라고 부르면:

\iffalse

But the value of $\mu / \lambda_0$ is just the ratio of the time required
to process the transaction (absent synchronization overhead) to that of
the synchronization overhead itself (absent contention).
If we call this ratio $f$, we have:

\fi

\begin{equation}
	e = \frac{f - n}{f - (n - 1)}
\end{equation}

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/synceff}}
\caption{Synchronization Efficiency}
\label{fig:SMPdesign:Synchronization Efficiency}
\end{figure}

Figure~\ref{fig:SMPdesign:Synchronization Efficiency} 는 동기화 효율성 $e$ 를
오버헤드 비율 $f$ 의 일부 값에 대한 CPU/쓰레드 갯수 $n$ 의 함수로 그려냅니다.
예를 들어, 5 나노세컨드 어토믹 값 증가 연산을 다시 생각해 보면, $f=10$ 선은 매
50 나노세컨드마다 각 CPU 가 어토믹 값 증가를 하는 상황에 연관되며, $f=100$ 선은
각 CPU 가 매 500 나노세컨드마다 어토믹 값 증가를 하는 상황에 연관되어서,
결국은 수백개의 (어쩌면 수천개의) 인스트럭션들에 연관되게 됩니다.
각 선이 증가하는 CPU 또는 쓰레드 갯수에 의해 빠르게 떨어지는 것을 보면, 우린
단일 전역 공유 변수에 대한 어토믹 값 조정에 기반한 동기화 메커니즘은 현재의
상용 하드웨어 위에서 많이 사용될 경우 잘 확장되지 못할 것이라고 결론내릴 수
있습니다.
이는 Chapter~\ref{chp:Counting} 에서 이야기 했던 병렬 카운팅 알고리즘을 이끌게
하는 원동력에 대한 대략적 수학적 묘사입니다.
여러분의 실제 세계는 다를 수 있습니다.

\iffalse

Figure~\ref{fig:SMPdesign:Synchronization Efficiency} plots the synchronization
efficiency $e$ as a function of the number of CPUs/threads $n$ for
a few values of the overhead ratio $f$.
For example, again using the 5-nanosecond atomic increment, the $f=10$
line corresponds to each CPU attempting an atomic increment every
50~nanoseconds, and the $f=100$ line corresponds to each CPU attempting
an atomic increment every 500~nanoseconds, which in turn corresponds to
some hundreds (perhaps thousands) of instructions.
Given that each trace drops off sharply with increasing numbers of
CPUs or threads, we can conclude that
synchronization mechanisms based on
atomic manipulation of a single global shared variable will not
scale well if used heavily on current commodity hardware.
This is an abstract mathematical depiction of the forces leading
to the parallel counting algorithms that were discussed in
Chapter~\ref{chp:Counting}.
Your real-world mileage may differ.

\fi

그러나, 효율성의 개념은 유용한데, 동기화가 적거나 정형적이지 않을 때 조차도
그렇습니다.
예를 들어 한 행렬의 행들이 다른 행렬의 열들과 곱해져 (``dot 연산을 통해'')
세번째 행렬의 각 항을 만들게 되는 행렬 곱셈을 생각해 봅시다.
이 오퍼레이션들은 서로 충돌하지 않으므로, 첫번째 행렬의 행들을 쓰레드들로
분리시키고, 각 쓰레드는 결과 행렬의 각 행을 계산하게 하는 것이 가능합니다.
따라서 이 쓰레드들은 동기화 오버헤드건 뭐건 없이 완전히 개별적으로
\path{matmul.c} 처럼 동작할 수 있습니다.
따라서 어떤 사람들은 완전한 효율성인 1.0을 생각할 겁니다.

\iffalse

Nevertheless, the concept of efficiency is useful, and even in cases
having little or no formal synchronization.
Consider for example a matrix multiply, in which the columns of one
matrix are multiplied (via ``dot product'') by the rows of another,
resulting in an entry in a third matrix.
Because none of these operations conflict, it is possible to partition
the columns of the first matrix among a group of threads, with each thread
computing the corresponding columns of the result matrix.
The threads can therefore operate entirely independently, with no
synchronization overhead whatsoever, as is done in
\path{matmul.c}.
One might therefore expect a perfect efficiency of 1.0.

\fi

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/matmuleff}}
\caption{Matrix Multiply Efficiency}
\label{fig:SMPdesign:Matrix Multiply Efficiency}
\end{figure}

하지만,
Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
는 다른 이야기를 하는데, 특히 64-by-64 행렬 곱셈에서는 0.3 이상의 효율성을 결코
얻지 못하는데, 단일 쓰레드로 동작할 때조차 그러하며, 더 많은 쓰레드가 사용되면
효율성이 떨어집니다.\footnote{
	\cref{fig:SMPdesign:Synchronization Efficiency} 에서의 부드러운 선과
	대조적인
	\cref{fig:SMPdesign:Matrix Multiply Efficiency} 의 넓은 에러바와
	요동치는 선들은 그 실제 세계의 특성에 대한 증거를 보입니다.}
128-by-128 행렬은 좀 낫습니다만, 여전히 추가되는 쓰레드에 맞춰 훨씬 나은 성능을
보이지는 못합니다.
256-by-256 행렬은 제법 잘 확장됩니다만, 약간의 CPU 들까지만 그렇습니다.
512-by-512 행렬 곱셈의 효율성은 10개 쓰레드까지만 1.0 보다 좀 낮게 측정되며,
심지어 1024-by-1024 행렬 곱셈도 수십개의 쓰레드가 사용되었을 때에는 확연히
흔들립니다.
그러나, 이 그림은 batching 의 성능과 확장성 이익을 확연히 보이고 있습니다:
여러분이 동기화 오버헤드를 일으켜야만 한다면, 여러분의 돈의 가치를 얻을 겁니다.

\iffalse

However,
Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
tells a different story, especially for a 64-by-64 matrix multiply,
which never gets above an efficiency of about 0.3, even when running
single-threaded, and drops sharply as more threads are added.\footnote{
	In contrast to the smooth traces of
	\cref{fig:SMPdesign:Synchronization Efficiency},
	the wide error bars and jagged traces of
	\cref{fig:SMPdesign:Matrix Multiply Efficiency}
	gives evidence of its real-world nature.}
The 128-by-128 matrix does better, but still fails to demonstrate
much performance increase with added threads.
The 256-by-256 matrix does scale reasonably well, but only up to a handful
of CPUs.
The 512-by-512 matrix multiply's efficiency is measurably less
than 1.0 on as few as 10 threads, and even the 1024-by-1024 matrix
multiply deviates noticeably from perfection at a few tens of threads.
Nevertheless, this figure clearly demonstrates the performance and
scalability benefits of batching: If you must incur synchronization
overhead, you may as well get your money's worth.

\fi

\QuickQuiz{
	어떻게 싱글쓰레드 기반 64-by-64 행렬 곱셈이 1.0 보다 낮은 효율성을 가질
	수 있죠?
	Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
	의 모든 선들은 하나의 쓰레드만 사용했을 때에는 1.0의 효율성을 보여야
	하는 거 아닌가요?

	\iffalse

	How can a single-threaded 64-by-64 matrix multiple possibly
	have an efficiency of less than 1.0?
	Shouldn't all of the traces in
	Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
	have efficiency of exactly 1.0 when running on one thread?

	\fi

}\QuickQuizAnswer{
	\path{matmul.c} 프로그램은 지정된 수의 작업 쓰레드를 만드는데, 하나의
	작업 쓰레드의 경우에조차도 쓰레드 생성 오버헤드를 일으킵니다.
	단일 작업 쓰레드의 경우를 위한 쓰레드 생성 오버헤드를 제거하는 데
	필요한 변경 작업은 독자 여러분의 연습문제로 남겨둡니다.

	\iffalse

	The \path{matmul.c} program creates the specified number of
	worker threads, so even the single-worker-thread case incurs
	thread-creation overhead.
	Making the changes required to optimize away thread-creation
	overhead in the single-worker-thread case is left as an
	exercise to the reader.

	\fi

}\QuickQuizEnd

이 비효율성을 놓고 보면,
Section~\ref{sec:SMPdesign:Data Locking} 에서 이야기된 데이터 락킹이나 다음
섹션에서 이야기될 병렬-fastpath 접근법과 같은 더 확장 가능한 접근법을
알아보는게 가치있을 겁니다.

\iffalse

Given these inefficiencies,
it is worthwhile to look into more-scalable approaches
such as the data locking described in
Section~\ref{sec:SMPdesign:Data Locking}
or the parallel-fastpath approach discussed in the next section.

\fi

\QuickQuiz{
	데이터 병렬 기법이 어떻게 행렬 곱셈을 도울 수 있죠?
	이건 \emph{이미} 데이터 병렬이라구요!!!

	\iffalse

	How are data-parallel techniques going to help with matrix
	multiply?
	It is \emph{already} data parallel!!!

	\fi

}\QuickQuizAnswer{
	주의를 기울여주신 것에 감사합니다!
	이 예제는 데이터 병렬성이 매우 좋은 것일 수 있긴 하지만, 그것이 모든
	비효율성의 원인을 자동으로 사라지게 하는 어떤 마법 지팡이는 아니라는
	것을 보이기 위함입니다.
	완전한 성능으로의 선형적 확장은 ``오직'' 64 쓰레드에 대해서라도 설계와
	구현의 모든 단계에 주의를 필요로 합니다.

	특히, 여러분은 각 조각의 크기에 주의를 기울여야 합니다.
	예를 들어, 여러분이 64-by-64 행렬 곱셈을 64 쓰레드로 쪼갠다면, 각
	쓰레드는 오직 64개의 부동소수점 곱셈만을 합니다.
	부동소수점 곱셈의 비용은 쓰레드 생성의 오버헤드에 비해 아주 작으며,
	캐쉬 미스 오버헤드 또한 이론적으로 완벽한 확장성을 망치는 (그리고 이
	선을 그렇게나 흔들리게 하는) 역할을 합니다.
	448 하드웨어 쓰레드의 완전한 사용의 경우에는 좋은 확장성을 얻기 위해
	수십만개의 열과 행을 갖는 행렬을 필요로 할텐데, 그 지점부터는 GPGPU 가
	상당히 매력적인 선택이 되는데, 특히 가격/성능의 관점에서 그렇습니다.

	여러분이 다양한 입력을 받는 병렬 프로그램을 가지고 있다면, 입력의
	크기가 병렬화를 하기에 가치가 있기엔 너무 작지 않은지에 대한 체크를
	항상 포함시키십시오.
	그리고 병렬화가 도움이 되지 않을 때에는, 쓰레드를 만들기 위한
	오버헤드를 일으키는건 도움이 되지 않습니다.

	\iffalse

	I am glad that you are paying attention!
	This example serves to show that although data parallelism can
	be a very good thing, it is not some magic wand that automatically
	wards off any and all sources of inefficiency.
	Linear scaling at full performance, even to ``only'' 64 threads,
	requires care at all phases of design and implementation.

	In particular, you need to pay careful attention to the
	size of the partitions.
	For example, if you split a 64-by-64 matrix multiply across
	64 threads, each thread gets only 64 floating-point multiplies.
	The cost of a floating-point multiply is minuscule compared to
	the overhead of thread creation, and cache-miss overhead
	also plays a role in spoiling the theoretically perfect scalability
	(and also in making the traces so jagged).
	The full 448~hardware threads would require a matrix with
	hundreds of thousands of rows and columns to attain good
	scalability, but by that point GPGPUs become quite attractive,
	especially from a price/performance viewpoint.

	Moral: If you have a parallel program with variable input,
	always include a check for the input size being too small to
	be worth parallelizing.
	And when it is not helpful to parallelize, it is not helpful
	to incur the overhead required to spawn a thread, now is it?

	\fi
}\QuickQuizEnd

\section{Parallel Fastpath}
\label{sec:SMPdesign:Parallel Fastpath}
%
\epigraph{There are two ways of meeting difficulties: You alter the
	  difficulties, or you alter yourself to meet them.}
	 {\emph{Phyllis Bottome}}

미세하게 조정된 (그리고 따라서 \emph{일반적으로} 성능이 높은) 설계들은 보통
거칠게 조정된 설계들에 비해 복잡합니다.
많은 경우, 대부분의 오버헤드는 코드의 작은 부분에서 기인합니다~\cite{Knuth73}.
그러니 그 작은 부분에 노력을 기울이지 않을 이유가 무엇일까요?

이것이 병렬-fastpath 설계패턴을 뒷받침하는 아이디어로, 적극적으로 전체
알고리즘을 병렬화 하려면 필요할 복잡성을 피하고 일반적인 경우의 코드 패쓰를
병렬화 하는 것입니다.
여러분은 여러분이 병렬화 하고자 하는 특정 알고리즘만이 아니라, 그 알고리즘이
사용되는 워크로드에 대해서도 이해해야 합니다.
병렬 fastpath 를 만드는 데에는 상당한 창의성과 설계 노력이 필요합니다.

병렬 fastpath 는 다른 패턴들을 조합하며 (하나는 fastpath 용, 다른 하나는 다른
곳을 위해) 따라서 템플릿 패턴입니다.
다음의 병렬 fastpath 의 예들은
Figure~\ref{fig:SMPdesign:Parallel-Fastpath Design Patterns} 에 그려진 것처럼
각자 하나씩의 전용 패턴을 갖습니다:

\iffalse

Fine-grained (and therefore \emph{usually} higher-performance)
designs are typically more complex than are coarser-grained designs.
In many cases, most of the overhead is incurred by a small fraction
of the code~\cite{Knuth73}.
So why not focus effort on that small fraction?

This is the idea behind the parallel-fastpath design pattern, to aggressively
parallelize the common-case code path without incurring the complexity
that would be required to aggressively parallelize the entire algorithm.
You must understand not only the specific algorithm you wish
to parallelize, but also the workload that the algorithm will
be subjected to.  Great creativity and design
effort is often required to construct a parallel fastpath.

Parallel fastpath combines different patterns (one for the
fastpath, one elsewhere) and is therefore a template pattern.
The following instances of parallel
fastpath occur often enough to warrant their own patterns,
as depicted in Figure~\ref{fig:SMPdesign:Parallel-Fastpath Design Patterns}:

\fi

\begin{figure}[tbp]
\centering
\resizebox{2.3in}{!}{\includegraphics{SMPdesign/ParallelFastpath}}
% \includegraphics{SMPdesign/ParallelFastpath}
\caption{Parallel-Fastpath Design Patterns}
\label{fig:SMPdesign:Parallel-Fastpath Design Patterns}
\end{figure}

\begin{enumerate}
\item	Reader/Writer Locking
	(Section~\ref{sec:SMPdesign:Reader/Writer Locking} 에서 설명됩니다).
\item	Reader/writer 락킹의 고성능 대체제로 사용될 수도 있는 Read-copy update (RCU) 는
	Section~\ref{sec:defer:Read-Copy Update (RCU)} 에서 설명됩니다.
	다른 대안으로는 해저드 포인터
	(\cref{sec:defer:Hazard Pointers})
	와 시퀀스 락킹 (\cref{sec:defer:Sequence Locks}) 등이 있습니다.
	이런 대안들은 이 챕터에서 더 이야기 되지 않습니다.
\item	Section~\ref{sec:SMPdesign:Hierarchical Locking} 에서 이야기 되는
	계층적 락킹~(\cite{McKenney95b}).
\item	리소스 할당 캐쉬~(\cite{McKenney95b,McKenney93}).
	더 자세한 내용을 위해선
	Section~\ref{sec:SMPdesign:Resource Allocator Caches} 을 보세요.
\end{enumerate}

\iffalse

\begin{enumerate}
\item	Reader/Writer Locking
	(described below in Section~\ref{sec:SMPdesign:Reader/Writer Locking}).
\item	Read-copy update (RCU), which may be used as a high-performance
	replacement for reader/writer locking, is introduced in
	Section~\ref{sec:defer:Read-Copy Update (RCU)}.
	Other alternatives include hazard pointers
	(\cref{sec:defer:Hazard Pointers})
	and sequence locking (\cref{sec:defer:Sequence Locks}).
	These alternatives will not be discussed further in this chapter.
\item   Hierarchical Locking~(\cite{McKenney95b}), which is touched upon
	in Section~\ref{sec:SMPdesign:Hierarchical Locking}.
\item	Resource Allocator Caches~(\cite{McKenney95b,McKenney93}).
	See Section~\ref{sec:SMPdesign:Resource Allocator Caches}
	for more detail.
\end{enumerate}

\fi

\subsection{Reader/Writer Locking}
\label{sec:SMPdesign:Reader/Writer Locking}

동기화 오버헤드가 크지 않다면 (예를 들어, 프로그램이 큰 크리티컬 섹션과 함께
거친 단위의 병렬성을 사용하고 있다면), 그리고 그 크리티컬 섹션의 작은 부분만이
데이터를 수정한다면, 여러 읽기 쓰레드가 병렬로 수행될 수 있게 하는 것은
확장성을 크게 향상시킬 수 있습니다.
쓰기 쓰레드는 읽기 쓰레드와 서로들을 모두 배제시켜야 합니다.
Reader-writer 락킹의 여러 구현이 존재하는데,
Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking} 에서 설명된 POSIX
구현이 포함됩니다.
Listing~\ref{lst:SMPdesign:Reader-Writer-Locking Hash Table Search}
은 해쉬 탐색이 reader-writer 락킹을 이용해 어떻게 구현될 수 있는지 보입니다.

\iffalse

If synchronization overhead is negligible (for example, if the program
uses coarse-grained parallelism with large critical sections), and if
only a small fraction of the critical sections modify data, then allowing
multiple readers to proceed in parallel can greatly increase scalability.
Writers exclude both readers and each other.
There are many implementations of reader-writer locking, including
the POSIX implementation described in
Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking}.
Listing~\ref{lst:SMPdesign:Reader-Writer-Locking Hash Table Search}
shows how the hash search might be implemented using reader-writer locking.

\fi

\begin{listing}[tbp]
\begin{VerbatimL}[commandchars=\\\[\]]
rwlock_t hash_lock;

struct hash_table
{
	long nbuckets;
	struct node **buckets;
};

typedef struct node {
	unsigned long key;
	struct node *next;
} node_t;

int hash_search(struct hash_table *h, long key)
{
	struct node *cur;
	int retval;

	read_lock(&hash_lock);
	cur = h->buckets[key % h->nbuckets];
	while (cur != NULL) {
		if (cur->key >= key) {
			retval = (cur->key == key);
			read_unlock(&hash_lock);
			return retval;
		}
		cur = cur->next;
	}
	read_unlock(&hash_lock);
	return 0;
}
\end{VerbatimL}
\caption{Reader-Writer-Locking Hash Table Search}
\label{lst:SMPdesign:Reader-Writer-Locking Hash Table Search}
\end{listing}

Reader/writer 락킹은 비대칭 락킹의 간단한 한가지 예입니다.
Snaman~\cite{Snaman87} 은 여러 클러스터 시스템에서 사용된 화려한 여섯개의
비대칭 락킹 설계모드를 설명합니다.
일반적인 락킹과 reader-writer 락킹이
Chapter~\ref{chp:Locking} 에서 자세하게 설명되어 있습니다.

\iffalse

Reader/writer locking is a simple instance of asymmetric locking.
Snaman~\cite{Snaman87} describes a more ornate six-mode
asymmetric locking design used in several clustered systems.
Locking in general and reader-writer locking in particular is described
extensively in
Chapter~\ref{chp:Locking}.

\fi

\subsection{Hierarchical Locking}
\label{sec:SMPdesign:Hierarchical Locking}

계층적 (hierarchical) 락킹의 아이디어는 어떤 미세 단위 락을 획득할지 선택하는
동안만 거대 단위 락을 갖자는 것입니다.
Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
은 우리의 해쉬 테이블 탐색이 어떻게 계층적 락킹을 적용하도록 될 수 있는지
보입니다만, 또한 이 방법의 큰 약점도 보입니다:
우린 두번째 락을 획득하기 위한 오버헤드를 지불했습니다만, 우린 이를 짧은
시간동안만 쥐고 있습니다.
이 경우, 더욱 간단한 데이터 락킹 접근법은 더 간단할 것이고 성능도 높을 가능성이
큽니다.

\iffalse

The idea behind hierarchical locking is to have a coarse-grained lock
that is held only long enough to work out which fine-grained lock
to acquire.
Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
shows how our hash-table search might be adapted to do hierarchical
locking, but also shows the great weakness of this approach:
we have paid the overhead of acquiring a second lock, but we only
hold it for a short time.
In this case, the simpler data-locking approach would be simpler
and likely perform better.

\fi

\begin{listing}[tb]
\begin{fcvlabel}[ln:SMPdesign:Hierarchical-Locking Hash Table Search]
\begin{VerbatimL}[commandchars=\\\[\]]
struct hash_table
{
	long nbuckets;
	struct bucket **buckets;
};

struct bucket {
	spinlock_t bucket_lock;
	node_t *list_head;
};

typedef struct node {
	spinlock_t node_lock;
	unsigned long key;
	struct node *next;
} node_t;

int hash_search(struct hash_table *h, long key)
{
	struct bucket *bp;
	struct node *cur;
	int retval;

	bp = h->buckets[key % h->nbuckets];
	spin_lock(&bp->bucket_lock);
	cur = bp->list_head;
	while (cur != NULL) {
		if (cur->key >= key) {
			spin_lock(&cur->node_lock);
			spin_unlock(&bp->bucket_lock);
			retval = (cur->key == key);\lnlbl[retval]
			spin_unlock(&cur->node_lock);
			return retval;
		}
		cur = cur->next;
	}
	spin_unlock(&bp->bucket_lock);
	return 0;
}
\end{VerbatimL}
\end{fcvlabel}
\caption{Hierarchical-Locking Hash Table Search}
\label{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
\end{listing}

\QuickQuiz{
	어떤 경우에 계층적 락킹이 잘 동작하나요?

	\iffalse

	In what situation would hierarchical locking work well?

	\fi

}\QuickQuizAnswer{
	Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search} 의
	line~\ref{ln:SMPdesign:Hierarchical-Locking Hash Table Search:retval}
	에서의 비교가 훨씬 무거운 오퍼레이션으로 대체되었다면,
	\co{bp->bucket_lock} 의 해제는 \emph{어쩌면} \co{cur->node_lock} 의
	추가적인 획득과 해제의 오버헤드를 넘어설 정도로 락 컨텐션을 줄였을 수도
	있습니다.

	\iffalse

	If the comparison on
        line~\ref{ln:SMPdesign:Hierarchical-Locking Hash Table Search:retval} of
	Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
	were replaced by a much heavier-weight operation,
	then releasing \co{bp->bucket_lock} \emph{might} reduce lock
	contention enough to outweigh the overhead of the extra
	acquisition and release of \co{cur->node_lock}.

	\fi

}\QuickQuizEnd

\subsection{Resource Allocator Caches}
\label{sec:SMPdesign:Resource Allocator Caches}

이 섹션은 병렬 고정 클록 크기 메모리 할당자의 간략화된 설명을 제공합니다.
더 자세한 설명은 리눅스 커널~\cite{Torvalds2.6kernel} 의
문서에서~\cite{McKenney92a,McKenney93,Bonwick01slab,McKenney01e,JasonEvans2011jemalloc,ChrisKennelly2020tcmalloc}
찾을 수 있습니다.

\iffalse

This section presents a simplified schematic of a parallel fixed-block-size
memory allocator.
More detailed descriptions may be found in the
literature~\cite{McKenney92a,McKenney93,Bonwick01slab,McKenney01e,JasonEvans2011jemalloc,ChrisKennelly2020tcmalloc}
or in the Linux kernel~\cite{Torvalds2.6kernel}.

\fi

\subsubsection{Parallel Resource Allocation Problem}

The basic problem facing a parallel memory allocator is the tension
between the need to provide extremely fast memory allocation and
freeing in the common case and the need to efficiently distribute
memory in face of unfavorable allocation and freeing patterns.

To see this tension, consider a straightforward application of
data ownership to this problem---simply carve up memory so that
each CPU owns its share.
For example, suppose that a system with 12~CPUs has 64~gigabytes
of memory, for example, the laptop I am using right now.
We could simply assign each CPU a five-gigabyte region of memory,
and allow each CPU to allocate from its own region, without the need
for locking and its complexities and overheads.
Unfortunately, this scheme fails when CPU~0 only allocates memory and
CPU~1 only frees it, as happens in simple producer-consumer workloads.

The other extreme, code locking, suffers from excessive lock contention
and overhead~\cite{McKenney93}.

\subsubsection{Parallel Fastpath for Resource Allocation}
\label{sec:SMPdesign:Parallel Fastpath for Resource Allocation}

The commonly used solution uses parallel fastpath with each CPU
owning a modest cache of blocks, and with a large code-locked
shared pool for additional blocks.
To prevent any given CPU from monopolizing the memory blocks,
we place a limit on the number of blocks that can be in each CPU's
cache.
In a two-CPU system, the flow of memory blocks will be as shown
in Figure~\ref{fig:SMPdesign:Allocator Cache Schematic}:
when a given CPU is trying to free a block when its pool is full,
it sends blocks to the global pool, and, similarly, when that CPU
is trying to allocate a block when its pool is empty, it retrieves
blocks from the global pool.

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/allocatorcache}}
\caption{Allocator Cache Schematic}
\label{fig:SMPdesign:Allocator Cache Schematic}
\end{figure}

\subsubsection{Data Structures}

The actual data structures for a ``toy'' implementation of allocator
caches are shown in
Listing~\ref{lst:SMPdesign:Allocator-Cache Data Structures}.
The ``Global Pool'' of Figure~\ref{fig:SMPdesign:Allocator Cache Schematic}
is implemented by \co{globalmem} of type \co{struct globalmempool},
and the two CPU pools by the per-thread variable \co{perthreadmem} of
type \co{struct perthreadmempool}.
Both of these data structures have arrays of pointers to blocks
in their \co{pool} fields, which are filled from index zero upwards.
Thus, if \co{globalmem.pool[3]} is \co{NULL}, then the remainder of
the array from index 4 up must also be \co{NULL}.
The \co{cur} fields contain the index of the highest-numbered full
element of the \co{pool} array, or $-1$ if all elements are empty.
All elements from \co{globalmem.pool[0]} through
\co{globalmem.pool[globalmem.cur]} must be full, and all the rest
must be empty.\footnote{
	Both pool sizes (\co{TARGET_POOL_SIZE} and
	\co{GLOBAL_POOL_SIZE}) are unrealistically small, but this small
	size makes it easier to single-step the program in order to get
	a feel for its operation.}

\begin{listing}[tbp]
\input{CodeSamples/SMPdesign/smpalloc@data_struct.fcv}
\caption{Allocator-Cache Data Structures}
\label{lst:SMPdesign:Allocator-Cache Data Structures}
\end{listing}

The operation of the pool data structures is illustrated by
Figure~\ref{fig:SMPdesign:Allocator Pool Schematic},
with the six boxes representing the array of pointers making up
the \co{pool} field, and the number preceding them representing
the \co{cur} field.
The shaded boxes represent non-\co{NULL} pointers, while the empty
boxes represent \co{NULL} pointers.
An important, though potentially confusing, invariant of this
data structure is that the \co{cur} field is always one
smaller than the number of non-\co{NULL} pointers.

\begin{figure}[tbp]
\centering
\resizebox{2.6in}{!}{\includegraphics{SMPdesign/AllocatorPool}}
\caption{Allocator Pool Schematic}
\label{fig:SMPdesign:Allocator Pool Schematic}
\end{figure}

\subsubsection{Allocation Function}

\begin{fcvref}[ln:SMPdesign:smpalloc:alloc]
The allocation function \co{memblock_alloc()} may be seen in
Listing~\ref{lst:SMPdesign:Allocator-Cache Allocator Function}.
Line~\lnref{pick} picks up the current thread's per-thread pool,
and line~\lnref{chk:empty} checks to see if it is empty.

If so, \clnrefrange{ack}{rel} attempt to refill it
from the global pool
under the spinlock acquired on line~\lnref{ack} and released on line~\lnref{rel}.
\Clnrefrange{loop:b}{loop:e} move blocks from the global
to the per-thread pool until
either the local pool reaches its target size (half full) or
the global pool is exhausted, and line~\lnref{set} sets the per-thread pool's
count to the proper value.

In either case, line~\lnref{chk:notempty} checks for the per-thread
pool still being
empty, and if not, \clnrefrange{rem:b}{rem:e} remove a block and return it.
Otherwise, line~\lnref{ret:NULL} tells the sad tale of memory exhaustion.
\end{fcvref}

\begin{listing}[tbp]
\input{CodeSamples/SMPdesign/smpalloc@alloc.fcv}
\caption{Allocator-Cache Allocator Function}
\label{lst:SMPdesign:Allocator-Cache Allocator Function}
\end{listing}

\subsubsection{Free Function}

\begin{fcvref}[ln:SMPdesign:smpalloc:free]
Listing~\ref{lst:SMPdesign:Allocator-Cache Free Function} shows
the memory-block free function.
Line~\lnref{get} gets a pointer to this thread's pool, and
line~\lnref{chk:full} checks to see if this per-thread pool is full.

If so, \clnrefrange{acq}{empty:e} empty half of the per-thread pool
into the global pool,
with lines~\lnref{acq} and~\lnref{rel} acquiring and releasing the spinlock.
\Clnrefrange{loop:b}{loop:e} implement the loop moving blocks
from the local to the
global pool, and line~\lnref{set} sets the per-thread pool's count to the proper
value.

In either case, line~\lnref{place} then places the newly freed block into the
per-thread pool.
\end{fcvref}

\begin{listing}[tbp]
\input{CodeSamples/SMPdesign/smpalloc@free.fcv}
\caption{Allocator-Cache Free Function}
\label{lst:SMPdesign:Allocator-Cache Free Function}
\end{listing}

\QuickQuiz{
	Doesn't this resource-allocator design resemble that of
	the approximate limit counters covered in
	Section~\ref{sec:count:Approximate Limit Counters}?
}\QuickQuizAnswer{
	Indeed it does!
	We are used to thinking of allocating and freeing memory,
	but the algorithms in
	Section~\ref{sec:count:Approximate Limit Counters}
	are taking very similar actions to allocate and free
	``count''.
}\QuickQuizEnd

\subsubsection{Performance}

Rough performance results\footnote{
	This data was not collected in a statistically meaningful way,
	and therefore should be viewed with great skepticism and suspicion.
	Good data-collection and -reduction practice is discussed
	in Chapter~\ref{chp:Validation}.
	That said, repeated runs gave similar results, and these results
	match more careful evaluations of similar algorithms.}
are shown in
Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
running on a dual-core Intel x86 running at 1\,GHz (4300 bogomips per CPU)
with at most six blocks allowed in each CPU's cache.
In this micro-benchmark,
each thread repeatedly allocates a group of blocks and then frees all
the blocks in that group, with
the number of blocks in the group being the ``allocation run length''
displayed on the x-axis.
The y-axis shows the number of successful allocation/free pairs per
microsecond---failed allocations are not counted.
The ``X''s are from a two-thread run, while the ``+''s are from a
single-threaded run.

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/smpalloc}}
\caption{Allocator Cache Performance}
\label{fig:SMPdesign:Allocator Cache Performance}
\end{figure}

Note that run lengths up to six scale linearly and give excellent performance,
while run lengths greater than six show poor performance and almost always
also show \emph{negative} scaling.
It is therefore quite important to size \co{TARGET_POOL_SIZE}
sufficiently large,
which fortunately is usually quite easy to do in actual
practice~\cite{McKenney01e}, especially given today's large memories.
For example, in most systems, it is quite reasonable to set
\co{TARGET_POOL_SIZE} to 100, in which case allocations and frees
are guaranteed to be confined to per-thread pools at least 99\,\% of
the time.

As can be seen from the figure, the situations where the common-case
data-ownership applies (run lengths up to six) provide greatly improved
performance compared to the cases where locks must be acquired.
Avoiding synchronization in the common case will be a recurring theme through
this book.

\QuickQuizSeries{%
\QuickQuizB{
	In Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
	there is a pattern of performance rising with increasing run
	length in groups of three samples, for example, for run lengths
	10, 11, and 12.
	Why?
}\QuickQuizAnswerB{
	This is due to the per-CPU target value being three.
	A run length of 12 must acquire the global-pool lock twice,
	while a run length of 13 must acquire the global-pool lock
	three times.
}\QuickQuizEndB
%
\QuickQuizE{
	Allocation failures were observed in the two-thread
	tests at run lengths of 19 and greater.
	Given the global-pool size of 40 and the per-thread target
	pool size $s$ of three, number of threads $n$ equal to two,
	and assuming that the per-thread pools are initially
	empty with none of the memory in use, what is the smallest allocation
	run length $m$ at which failures can occur?
	(Recall that each thread repeatedly allocates $m$ block of memory,
	and then frees the $m$ blocks of memory.)
	Alternatively, given $n$ threads each with pool size $s$, and
	where each thread repeatedly first allocates $m$ blocks of memory
	and then frees those $m$ blocks, how large must the global pool
	size be?
	\emph{Note:} Obtaining the correct answer will require you to
	examine the \path{smpalloc.c} source code, and very likely
	single-step it as well.
	You have been warned!
}\QuickQuizAnswerE{
	This solution is adapted from one put forward by Alexey Roytman.
	It is based on the following definitions:

	\begin{description}
	\item[$g$]	Number of blocks globally available.
	\item[$i$]	Number of blocks left in the initializing thread's
			per-thread pool.  (This is one reason you needed
			to look at the code!)
	\item[$m$]	Allocation/free run length.
	\item[$n$]	Number of threads, excluding the initialization thread.
	\item[$p$]	Per-thread maximum block consumption, including
			both the blocks actually allocated and the blocks
			remaining in the per-thread pool.
	\end{description}

	The values $g$, $m$, and $n$ are given.  The value for $p$ is
	$m$ rounded up to the next multiple of $s$, as follows:

	\begin{equation}
		p = s \left \lceil \frac{m}{s} \right \rceil
	\label{sec:SMPdesign:p}
	\end{equation}

	The value for $i$ is as follows:

	\begin{equation}
		i = \left \{
			\begin{array}{l}
				g \pmod{2 s} = 0: 2 s \\
				g \pmod{2 s} \ne 0: g \pmod{2 s}
			\end{array}
		    \right .
	\label{sec:SMPdesign:i}
	\end{equation}

	\begin{figure}[tb]
	\centering
	\resizebox{3in}{!}{\includegraphics{SMPdesign/smpalloclim}}
	\caption{Allocator Cache Run-Length Analysis}
	\label{fig:SMPdesign:Allocator Cache Run-Length Analysis}
	\end{figure}

	The relationships between these quantities are shown in
	Figure~\ref{fig:SMPdesign:Allocator Cache Run-Length Analysis}.
	The global pool is shown on the top of this figure, and
	the ``extra'' initializer thread's per-thread pool and
	per-thread allocations are the left-most pair of boxes.
	The initializer thread has no blocks allocated, but has
	$i$ blocks stranded in its per-thread pool.
	The rightmost two pairs of boxes are the per-thread pools and
	per-thread allocations of threads holding the maximum possible
	number of blocks, while the second-from-left pair of boxes
	represents the thread currently trying to allocate.

	The total number of blocks is $g$, and adding up the per-thread
	allocations and per-thread pools, we see that the global pool
	contains $g-i-p(n-1)$ blocks.
	If the allocating thread is to be successful, it needs at least
	$m$ blocks in the global pool, in other words:

	\begin{equation}
		g - i - p(n - 1) \ge m
	\label{sec:SMPdesign:g-vs-m}
	\end{equation}

	The question has $g=40$, $s=3$, and $n=2$.
	Equation~\ref{sec:SMPdesign:i} gives $i=4$, and
	Equation~\ref{sec:SMPdesign:p} gives $p=18$ for $m=18$
	and $p=21$ for $m=19$.
	Plugging these into Equation~\ref{sec:SMPdesign:g-vs-m}
	shows that $m=18$ will not overflow, but that $m=19$ might
	well do so.

	The presence of $i$ could be considered to be a bug.
	After all, why allocate memory only to have it stranded in
	the initialization thread's cache?
	One way of fixing this would be to provide a \co{memblock_flush()}
	function that flushed the current thread's pool into the
	global pool.
	The initialization thread could then invoke this function
	after freeing all of the blocks.
}\QuickQuizEndE
}

\subsubsection{Real-World Design}

The toy parallel resource allocator was quite simple, but real-world
designs expand on this approach in a number of ways.

First, real-world allocators are required to handle a wide range
of allocation sizes, as opposed to the single size shown in this
toy example.
One popular way to do this is to offer a fixed set of sizes, spaced
so as to balance external and internal fragmentation, such as in
the late-1980s BSD memory allocator~\cite{McKusick88}.
Doing this would mean that the ``globalmem'' variable would need
to be replicated on a per-size basis, and that the associated
lock would similarly be replicated, resulting in data locking
rather than the toy program's code locking.

Second, production-quality systems must be able to repurpose memory,
meaning that they must be able to coalesce blocks into larger structures,
such as pages~\cite{McKenney93}.
This coalescing will also need to be protected by a lock, which again
could be replicated on a per-size basis.

Third, coalesced memory must be returned to the underlying memory
system, and pages of memory must also be allocated from the underlying
memory system.
The locking required at this level will depend on that of the underlying
memory system, but could well be code locking.
Code locking can often be tolerated at this level, because this
level is so infrequently reached in well-designed systems~\cite{McKenney01e}.

Despite this real-world design's greater complexity, the underlying
idea is the same---repeated application of parallel fastpath,
as shown in
Table~\ref{fig:app:questions:Schematic of Real-World Parallel Allocator}.

\begin{table}[tbp]
\rowcolors{1}{}{lightgray}
\small
\centering
\renewcommand*{\arraystretch}{1.25}
\begin{tabularx}{\twocolumnwidth}{ll>{\raggedright\arraybackslash}X}
\toprule
Level	& Locking & Purpose \\
\midrule
Per-thread pool	  & Data ownership & High-speed allocation \\
Global block pool & Data locking   & Distributing blocks among threads \\
Coalescing	  & Data locking   & Combining blocks into pages \\
System memory	  & Code locking   & Memory from/to system \\
\bottomrule
\end{tabularx}
\caption{Schematic of Real-World Parallel Allocator}
\label{fig:app:questions:Schematic of Real-World Parallel Allocator}
\end{table}

\input{SMPdesign/beyond}

\QuickQuizAnswersChp{qqzSMPdesign}
