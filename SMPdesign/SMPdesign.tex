% SMPdesign/SMPdesign.tex

\QuickQuizChapter{cha:Partitioning and Synchronization Design}{Partitioning and Synchronization Design}
%
\Epigraph{Divide and rule.}{\emph{Philip II of Macedon}}

이 챕터는 상용 시스템에서 지속적으로 나타나는 멀티 CPU 로부터의 이점을 얻기
위해서 소프트웨어를 어떻게 설계해야 하는지 설명합니다.
이를 위해 성능, 확장성, 그리고 반응시간 사이의 균형을 잡는데 도움을 줄 수 있을,
여러개의 관용구, 또는
``디자인 패턴''~\cite{Alexander79,GOF95,SchmidtStalRohnertBuschmann2000v2Textbook}
들을 소개합니다.
앞의 챕터에서 이야기 했듯, 병렬 소프트웨어를 만들 때 하게 되는 가장 중요한
결정은 파티셔닝을 어떻게 할것인지 입니다.
잘 분할된 문제들은 간단하고, 확장성 있으며, 높은 성능을 갖는 해결책을
이끌어냅니다만, 잘못 분할된 문제들은 느리고 복잡한 해결책을 만듭니다.
이 챕터는 몰아서 처리하기 (batching) 와 약화시키기 (weakening) 에 대한 토론과
함께 파티셔닝을 코드로 설계하는 것을 도울 것입니다.
``설계'' 란 단어는 매우 중요합니다: 파티셔닝이 첫번째, 몰아 처리하기가 두번째,
약화시키기가 세번째이며, 코딩은 네번째입니다.
이 순서를 바꾸는 행위는 낮은 성능과 좌절스러운 확장성을 이끌어 낼 겁니다.
\iffalse

This chapter describes how to design software to take advantage of
the multiple CPUs that are increasingly appearing in commodity systems.
It does this by presenting a number of idioms, or
``design patterns''~\cite{Alexander79,GOF95,SchmidtStalRohnertBuschmann2000v2Textbook}
that can help you balance performance, scalability, and response time.
As noted in earlier chapters, the most important decision you will make
when creating parallel software is how to carry out the partitioning.
Correctly partitioned problems lead to simple, scalable, and
high-performance solutions, while poorly partitioned problems result
in slow and complex solutions.
This chapter will help you design partitioning into your code, with
some discussion of batching and weakening as well.
The word ``design'' is very important: You should partition first,
batch second, weaken third, and code fourth.
Changing this order often leads to poor performance and scalability
along with great frustration.
\fi

이를 위해, Section~\ref{sec:SMPdesign:Partitioning Exercises} 에서는 파티셔닝
연습문제들을 소개하고,
Section~\ref{sec:SMPdesign:Design Criteria} 에서 분할가능성 설계 기준을
알아보고,
Section~\ref{sec:SMPdesign:Synchronization Granularity} 에서 적절한 동기화
정도의 선택에 대해 이야기 하고,
Section~\ref{sec:SMPdesign:Parallel Fastpath} 에서 일반적인 경우에는 속도와
확장성을 제공하는 중요한 병렬적 빠른 수행 경로를 사용하고 일반적이지 않은
상황에는 확장성이 덜한 대안인 ``슬로우 패스'' 사용 설계의 개요를 알아본 후,
마지막으로
Section~\ref{sec:SMPdesign:Beyond Partitioning} 에서 파티셔닝의 다음을 간략히
봅니다.
\iffalse

To this end, Section~\ref{sec:SMPdesign:Partitioning Exercises}
presents partitioning exercises,
Section~\ref{sec:SMPdesign:Design Criteria} reviews partitionability
design criteria,
Section~\ref{sec:SMPdesign:Synchronization Granularity}
discusses selecting an appropriate synchronization granularity,
Section~\ref{sec:SMPdesign:Parallel Fastpath}
gives an overview of important parallel-fastpath designs
that provide speed and scalability in the common case with
a simpler but less-scalable fallback ``slow path'' for unusual
situations,
and finally
Section~\ref{sec:SMPdesign:Beyond Partitioning}
takes a brief look beyond partitioning.
\fi

\input{SMPdesign/partexercises}

\input{SMPdesign/criteria}

\section{Synchronization Granularity}
\label{sec:SMPdesign:Synchronization Granularity}

\begin{figure}[tb]
\centering
\resizebox{1.2in}{!}{\includegraphics{SMPdesign/LockGranularity}}
\caption{Design Patterns and Lock Granularity}
\label{fig:SMPdesign:Design Patterns and Lock Granularity}
\end{figure}

Figure~\ref{fig:SMPdesign:Design Patterns and Lock Granularity} 는 서로 다른
동기화 단위 크기의 단계를 그림으로 보여주는데, 각각의 단계는 뒤의 섹션에서
다루어질 것입니다.
여기서의 섹션들은 주로 락킹에 중점을 맞춥니다만, 비슷한 단위 크기 문제가 모든
동기화 문제에 존재합니다.
\iffalse

Figure~\ref{fig:SMPdesign:Design Patterns and Lock Granularity}
gives a pictorial view of different levels of synchronization granularity,
each of which is described in one of the following sections.
These sections focus primarily on locking, but similar granularity
issues arise with all forms of synchronization.
\fi

\subsection{Sequential Program}
\label{sec:SMPdesign:Sequential Program}

프로그램이 싱글 프로세서 위에서 충분히 빨리 돌아간다면, 그리고 다른
프로세스들이나 쓰레드, 또는 인터럽트 핸들러들과의 상호작용이 없다면, 동기화
기능들을 모두 제거해서 그것들의 오버헤드와 복잡성을 당신으로부터 떼어놓아야
합니다.
몇년 전에는, Moore 의 법칙이 결국은 모든 프로그램을 이 카테고리로 만들 것이라
주장하던 사람들도 있었습니다.
하지만,
% @@@ Intel Trademark
Figure~\ref{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs} 에서 볼 수
있듯이, 싱글 쓰레드의 성능의 기하급수적인 증가는 2003년 정도에 멈췄습니다.
따라서, 성능을 늘리기 위해선 병렬성이 필요합니다.\footnote{
	이 그림은 이론적으로 클락당 하나 이상의 인스트럭션들을 처리할 수 있는
	최신 CPU 들의 경우 클락 주파수를, 그리고 하나의 간단한 인스트럭션을
	처리하는데 여러 클락을 필요로 하는 오래된 CPU 들의 경우 MIPS 를
	보입니다.
	이런 접근을 취하는 이유는 클락당 여러 인스트럭션들을 처리하는 최신 CPU
	들의 기능들은 일반적으로 메모리 시스템 성능에 제한되기 때문입니다.}
이 새로운 트렌드가 수천개의 CPU 들을 갖는 하나의 칩을 나타나게 할 것인지에 대한
논쟁은 금방 끝나지 않겠지만, Paul 이 이 문장을 듀얼코어 랩탑으로 쓰고 있다는
점을 볼 때, SMP 의 시대는 우리에게 와 있는 것으로 보입니다.
Figure~\ref{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}
에서 볼 수 있듯이 이더넷 대역폭이 지속적으로 성장하고 있음을 알아두는 것 역시
중요합니다.
이런 추세는 커뮤니케이션 부하를 처리하기 위해 멀티쓰레드 서버들이 나타나도록
촉진하는 역할을 할 것입니다.
\iffalse

If the program runs fast enough on a single processor, and
has no interactions with other processes, threads, or interrupt
handlers, you should
remove the synchronization primitives and spare yourself their
overhead and complexity.
Some years back, there were those who would argue that Moore's Law
would eventually force all programs into this category.
However, as can be seen in
Figure~\ref{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs},
the exponential increase in single-threaded performance halted in
about 2003.
Therefore,
increasing performance will increasingly require parallelism.\footnote{
	This plot shows clock frequencies for newer CPUs theoretically
	capable of retiring one or more instructions per clock, and MIPS for
	older CPUs requiring multiple clocks to execute even the
	simplest instruction.
	The reason for taking this approach is that the newer CPUs'
	ability to retire multiple instructions per clock is typically
	limited by memory-system performance.}
The debate as to whether this new trend will result in single chips
with thousands
of CPUs will not be settled soon, but given that Paul is typing this
sentence on a dual-core laptop, the age of SMP does seem to be upon us.
It is also important to note that Ethernet bandwidth is continuing
to grow, as shown in
Figure~\ref{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}.
This growth will motivate multithreaded servers in order to handle
the communications load.
\fi

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/clockfreq}}
\caption{MIPS/Clock-Frequency Trend for Intel CPUs}
\label{fig:SMPdesign:Clock-Frequency Trend for Intel CPUs}
\end{figure}

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/CPUvsEnet}}
\caption{Ethernet Bandwidth vs. Intel x86 CPU Performance}
\label{fig:SMPdesign:Ethernet Bandwidth vs. Intel x86 CPU Performance}
\end{figure}

이는 당신이 모든 프로그램을 멀티쓰레드 방식으로 코딩해야 한다는 의미가
\emph{아닙니다}.
다시 말하지만, 프로그램이 싱글 프로세서에서 충분히 빨리 동작한다면, SMP 동기화
기능들의 오버헤드와 복잡성으로부터 당신을 멀리 하십시오.
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search} 에 나와 있는
해시 테이블 탐색 코드의 단순성이 이 점을 강조합니다.\footnote{
이 섹션의 예들은 Hart 등~\cite{ThomasEHart2006a} 으로부터 얻어졌으며, 여러
	파일들로부터 관련된 코드를 모음으로써 경쾌함을 위해 적용되었습니다.}
핵심은 병렬성으로 인한 속도향상이 CPU 들의 갯수에 제한된다는 것입니다.
반면, 예컨대 조심스럽게 선택된 데이터 구조와 같은 순차적 최적화를 통한
속도향상은 얼마든지 클 수 있습니다.
\iffalse

Please note that this does \emph{not} mean that you should code each
and every program in a multi-threaded manner.
Again, if a program runs quickly enough on a single processor,
spare yourself the overhead and complexity of SMP synchronization
primitives.
The simplicity of the hash-table lookup code in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
underscores this point.\footnote{
	The examples in this section are taken from Hart et
	al.~\cite{ThomasEHart2006a}, adapted for clarity
	by gathering related code from multiple files.}
A key point is that speedups due to parallelism are normally
limited to the number of CPUs.
In contrast, speedups due to sequential optimizations, for example,
careful choice of data structure, can be arbitrarily large.
\fi

\begin{listing}[tbhp]
{ \scriptsize
\begin{verbbox}
  1 struct hash_table
  2 {
  3   long nbuckets;
  4   struct node **buckets;
  5 };
  6
  7 typedef struct node {
  8   unsigned long key;
  9   struct node *next;
 10 } node_t;
 11
 12 int hash_search(struct hash_table *h, long key)
 13 {
 14   struct node *cur;
 15
 16   cur = h->buckets[key % h->nbuckets];
 17   while (cur != NULL) {
 18     if (cur->key >= key) {
 19       return (cur->key == key);
 20     }
 21     cur = cur->next;
 22   }
 23   return 0;
 24 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Sequential-Program Hash Table Search}
\label{lst:SMPdesign:Sequential-Program Hash Table Search}
\end{listing}

% ./test_hash_null.exe 1000 0/100 1 1024 1
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 96.2913  max = 98.2337  min = 90.4095  std = 2.95314
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 91.5592  max = 97.3315  min = 89.9885  std = 2.88925
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 93.3568  max = 106.162  min = 89.8828  std = 6.40418

반면, 이런 행복한 상황에 처해 있는 게 아니라면, 계속 읽으세요!
\iffalse

On the other hand, if you are not in this happy situation, read on!
\fi

\subsection{Code Locking}
\label{sec:SMPdesign:Code Locking}

코드 락킹은 글로벌 락들만을 사용하기 때문에 상당히 간단합니다.\footnote{
	여러분의 프로그램이 데이터 구조체 안에 락들을 가지고 있거나, 자바의
	경우, synchronized 인스턴스로 클래스들을 사용한다면,
	Section~\ref{sec:SMPdesign:Data Locking} 에 설명된 ``데이터 락킹'' 을
	사용하고 있는 겁니다.}
기존의 프로그램을 멀티 프로세서 위에서 동작하도록 하기 위해 코드 락킹을
사용하도록 수정하는 것은 특히 쉽습니다.
만약 프로그램이 하나의 공유자원만을 가지고 있다면, 코드 락킹은 최적의 성능을
제공할 것입니다.
하지만, 많은 거대하고 복잡한 프로그램들은 크리티컬 세견에서 많은 작업이
수행되어야 할 것을 필요로 하고, 이는 곧 코드 락킹이 그 프로그램의 확장성을 크게
제한하게 합니다.
\iffalse

Code locking is quite simple due to the fact that is uses only
global locks.\footnote{
	If your program instead has locks in data structures,
	or, in the case of Java, uses classes with synchronized
	instances, you are instead using ``data locking'', described
	in Section~\ref{sec:SMPdesign:Data Locking}.}
It is especially
easy to retrofit an existing program to use code locking in
order to run it on a multiprocessor.  If the program has
only a single shared resource, code locking will even give
optimal performance.
However, many of the larger and more complex programs
require much of the execution to
occur in critical sections, which in turn causes code locking
to sharply limits their scalability.
\fi

따라서, 전체 실행시간 중 작은 부분만을 크리티컬 섹션에서 수행하거나 작은
확장성만이 필요한 프로그램들에 대해서만 코드 락킹을 사용해야 합니다.
이런 경우, 코드 락킹은
Listing~\ref{lst:SMPdesign:Code-Locking Hash Table Search} 에서 볼 수 있듯,
순차적인 버전과 매우 유사하고 상대적으로 간단한 프로그램을 제공할 것입니다.
하지만, Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search} 의
\co{hash_search()} 에서의 단순한 비교값 리턴은 이제 리턴 전에 락을 풀어야 하기
때문에 세개의 문장이 되었음을 알아 두시기 바랍니다.
\iffalse

Therefore, you should use code locking on programs that spend
only a small fraction of their execution time in critical sections or
from which only modest scaling is required.  In these cases,
code locking will provide a relatively simple program that is
very similar to its sequential counterpart,
as can be seen in
Listing~\ref{lst:SMPdesign:Code-Locking Hash Table Search}.
However, note that the simple return of the comparison in
\co{hash_search()} in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}
has now become three statements due to the need to release the
lock before returning.
\fi

\begin{listing}[tbhp]
{ \scriptsize
\begin{verbbox}
  1 spinlock_t hash_lock;
  2
  3 struct hash_table
  4 {
  5   long nbuckets;
  6   struct node **buckets;
  7 };
  8
  9 typedef struct node {
 10   unsigned long key;
 11   struct node *next;
 12 } node_t;
 13
 14 int hash_search(struct hash_table *h, long key)
 15 {
 16   struct node *cur;
 17   int retval;
 18
 19   spin_lock(&hash_lock);
 20   cur = h->buckets[key % h->nbuckets];
 21   while (cur != NULL) {
 22     if (cur->key >= key) {
 23       retval = (cur->key == key);
 24       spin_unlock(&hash_lock);
 25       return retval;
 26     }
 27     cur = cur->next;
 28   }
 29   spin_unlock(&hash_lock);
 30   return 0;
 31 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Code-Locking Hash Table Search}
\label{lst:SMPdesign:Code-Locking Hash Table Search}
\end{listing}

안타깝게도, 코드 락킹은 여러 CPU 들이 락을 동시에 획득하려 하는, ``락 경쟁''
상황에 빠지기 쉬운 경향이 있습니다.
한 무리의 어린 아이들 (또는 아이처럼 행동하는 어른들의 무리들) 을 보살펴 본 SMP
프로그래머라면 Figure~\ref{fig:SMPdesign:Lock Contention} 에 그려진 것처럼 뭔가
단 하나를 사용하는 것의 위험성을 깨달을 겁니다.
\iffalse

Unfortunately, code locking is particularly prone to ``lock contention'',
where multiple CPUs need to acquire the lock concurrently.
SMP programmers who have taken care of groups of small children
(or groups of older people who are acting like children) will immediately
recognize the danger of having only one of something,
as illustrated in Figure~\ref{fig:SMPdesign:Lock Contention}.
\fi

% ./test_hash_codelock.exe 1000 0/100 1 1024 1
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 164.115  max = 170.388  min = 161.659  std = 3.21857
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 181.17  max = 198.4  min = 162.459  std = 15.8585
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_codelock.exe: avg = 167.651  max = 189.014  min = 162.144  std = 10.6819

% ./test_hash_codelock.exe 1000 0/100 1 1024 2
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 378.481  max = 385.971  min = 374.235  std = 4.05934
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 753.414  max = 1015.28  min = 377.734  std = 294.942
% ./test_hash_codelock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_codelock.exe: avg = 502.737  max = 980.924  min = 374.406  std = 239.383

이 문제에 대한 해결책인 ``데이터 락킹'' 이 다음 섹션에 설명됩니다.
\iffalse

One solution to this problem, named ``data locking'', is described
in the next section.
\fi

\begin{figure}[tbh]
\centering
\resizebox{2.5in}{!}{\includegraphics{cartoons/r-2014-Data-one-fighting}}
\caption{Lock Contention}
\ContributedBy{Figure}{fig:SMPdesign:Lock Contention}{Melissa Broussard}
\end{figure}

\subsection{Data Locking}
\label{sec:SMPdesign:Data Locking}

많은 데이터 구조체들이 각자 자기의 락을 갖는 조각들로 분할될 수 있습니다.
이렇게 되면 데이터 구조체의 각 조각들의 크리티컬 섹션들은 병렬적으로 실행될 수
있습니다, 각 조각의 크리티컬 섹션의 인스턴스는 한번에 하나씩만 수행될 수 있긴
하지만요.
경쟁상황이 줄어야만 하고, 동기화 오버헤드가 속도향상을 제한하지 않는 경우에는
데이터 락킹을 사용해야 합니다.
데이터 락킹은 여러 데이터 구조체 사이에 존재하는 너무 큰 크리티컬 섹션
인스턴스들을 분산시킴으로써 경쟁상황을 줄여주는데, 예를 들어
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search} 처럼 해시 테이블에서
해시 버킷 별로 크리티컬 섹션을 두는 식입니다.
향상된 확장성은 \co{struct bucket} 와 같이 추가된 데이터 구조체의 형태로
복잡도를 약간 증가시킵니다.
\iffalse

Many data structures may be partitioned,
with each partition of the data structure having its own lock.
Then the critical sections for each part of the data structure
can execute in parallel,
although only one instance of the critical section for a given
part could be executing at a given time.
You should use data locking when contention must
be reduced, and where synchronization overhead is not
limiting speedups.
Data locking reduces contention by distributing the instances
of the overly-large critical section across multiple data structures,
for example, maintaining per-hash-bucket critical sections in a
hash table, as shown in
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}.
The increased scalability again results in a slight increase in complexity
in the form of an additional data structure, the \co{struct bucket}.
\fi

\begin{listing}[tb]
{ \scriptsize
\begin{verbbox}
  1 struct hash_table
  2 {
  3   long nbuckets;
  4   struct bucket **buckets;
  5 };
  6
  7 struct bucket {
  8   spinlock_t bucket_lock;
  9   node_t *list_head;
 10 };
 11
 12 typedef struct node {
 13   unsigned long key;
 14   struct node *next;
 15 } node_t;
 16
 17 int hash_search(struct hash_table *h, long key)
 18 {
 19   struct bucket *bp;
 20   struct node *cur;
 21   int retval;
 22
 23   bp = h->buckets[key % h->nbuckets];
 24   spin_lock(&bp->bucket_lock);
 25   cur = bp->list_head;
 26   while (cur != NULL) {
 27     if (cur->key >= key) {
 28       retval = (cur->key == key);
 29       spin_unlock(&bp->bucket_lock);
 30       return retval;
 31     }
 32     cur = cur->next;
 33   }
 34   spin_unlock(&bp->bucket_lock);
 35   return 0;
 36 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Data-Locking Hash Table Search}
\label{lst:SMPdesign:Data-Locking Hash Table Search}
\end{listing}

Figure~\ref{fig:SMPdesign:Lock Contention} 에서 보였던 경쟁적인 상황과 달리,
데이터 락킹은 Figure~\ref{fig:SMPdesign:Data Locking} 에 그려진 것처럼 조화를
향상시킵니다 --- 그리고 병렬 프로그램에서, 이는 \emph{거의} 항상 향상된 성능과
확장성으로 이어집니다.
이런 이유로, 데이터 락킹은 DYNIX 와 DYNIX/ptx 운영체제의 Sequent 에서 매우 많이
사용되었습니다~\cite{Beck85,Inman85,Garg90,Dove90,McKenney92b,McKenney92a,McKenney93}.
\iffalse

In contrast with the contentious situation
shown in Figure~\ref{fig:SMPdesign:Lock Contention},
data locking helps promote harmony, as illustrated by
Figure~\ref{fig:SMPdesign:Data Locking}---and in parallel programs,
this \emph{almost} always translates into increased performance and
scalability.
For this reason, data locking was heavily used by Sequent in
both its DYNIX and DYNIX/ptx operating
systems~\cite{Beck85,Inman85,Garg90,Dove90,McKenney92b,McKenney92a,McKenney93}.
\fi

\begin{figure}[tbh]
\centering
\resizebox{2.4in}{!}{\includegraphics{cartoons/r-2014-Data-many-happy}}
\caption{Data Locking}
\ContributedBy{Figure}{fig:SMPdesign:Data Locking}{Melissa Broussard}
\end{figure}

% ./test_hash_spinlock.exe 1000 0/100 1 1024 1
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 158.118  max = 162.404  min = 156.199  std = 2.19391
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 157.717  max = 162.446  min = 156.415  std = 2.36662
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_spinlock.exe: avg = 158.369  max = 164.75  min = 156.501  std = 3.19454

% ./test_hash_spinlock.exe 1000 0/100 1 1024 2
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 223.426  max = 422.948  min = 167.858  std = 100.136
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 235.462  max = 507.134  min = 167.466  std = 135.836
% ./test_hash_spinlock.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_spinlock.exe: avg = 305.807  max = 481.685  min = 167.939  std = 132.589

하지만, 어린 아이들을 돌봐 본 사람이라면 이야기 할 수 있겠지만, 충분히 놀 수
있는 공간을 주는 것이 평안을 보장하지는 않습니다.
예를 들어, 리눅스 커널은 파일과 디렉토리들의 캐시 (``dcache'' 라 불립니다) 를
갖습니다.
이 캐시의 각 원소들은 자신의 락을 갖습니다만, 루트 디렉토리에 해당하는 원소들과
그것의 직접적인 자식들은 다른 원소들에 비해 훨씬 많이 순회됩니다.
이는 많은 CPU 들이 이 자주 접근되는 원소들의 락에 경쟁을 하게 되는 결과를
초래해서, Figure~\ref{fig:SMPdesign:Data and Skew} 에 보여진 것과 같은 상황을
초래하고 맙니다.
\iffalse

However, as those who have taken care of small children can again attest,
even providing enough to go around is no guarantee of tranquillity.
The analogous situation can arise in SMP programs.
For example, the Linux kernel maintains a cache of files and directories
(called ``dcache'').
Each entry in this cache has its own lock, but the entries corresponding
to the root directory and its direct descendants are much more likely to
be traversed than are more obscure entries.
This can result in many CPUs contending for the locks of these popular
entries, resulting in a situation not unlike that
shown in Figure~\ref{fig:SMPdesign:Data and Skew}.
\fi

\begin{figure}[tbh]
\centering
\resizebox{\twocolumnwidth}{!}{\includegraphics{cartoons/r-2014-Data-many-fighting}}
\caption{Data Locking and Skew}
\ContributedBy{Figure}{fig:SMPdesign:Data and Skew}{Melissa Broussard}
\end{figure}

많은 경우, 알고리즘들은 데이터 쏠림을 줄이도록 설계될 수 있고, 어떤 경우에는
아예 그것들을 없애버릴 수도 있습니다 (리눅스 커널의 dcache 에서도 가능한 것으로
나타난 것처럼요~\cite{McKenney04a}).
데이터 락킹은 종종 해시 테이블처럼 분할될 수 있는 데이터 구조체에 사용됩니다만,
여러 존재가 각각 어떤 데이터 구조체의 인스턴스로 표현될 수 있는 경우에도
사용됩니다.
리눅스 커널 2.6.17 버전의 태스크 리스트는 후자의 한 예로, 각 태스크 구조체는
자신의 \co{proc_lock} 을 갖습니다.

다이나믹하게 할당되는 구조체들에서의 데이터 락킹의 핵심 문제는 해당 구조체가
해당 락이 잡혀있는 동안은 존재하는 상태를 유지해야 한다는 것입니다.
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search} 의 코드는 이 문제를
락들을 정적으로 할당된 해시 버킷들에 넣어두고, 절대 메모리에서 해제시키지 않는
것으로 해결합니다.
하지만, 이 트릭은 해시 테이블의 크기가 바뀔 수 있다면 락들이 다이나믹하게
할당되어야 하므로 제대로 동작하지 않을 것입니다.
이 경우, 해시 버킷들을 그 락들이 잡혀있는 동안은 메모리 해제되지 않도록 하는
어떤 수단이 필요할 것입니다.
\iffalse

In many cases, algorithms can be designed to reduce the instance of
data skew, and in some cases eliminate it entirely
(as appears to be possible with the Linux kernel's dcache~\cite{McKenney04a}).
Data locking is often used for partitionable data structures such as
hash tables, as well as in situations where multiple entities are each
represented by an instance of a given data structure.
The task list in version 2.6.17 of the Linux kernel is an example of the
latter, each task structure having its own \co{proc_lock}.

A key challenge with data locking on dynamically allocated structures
is ensuring that the structure remains in existence while the lock is
being acquired.
The code in
Listing~\ref{lst:SMPdesign:Data-Locking Hash Table Search}
finesses this challenge by placing the locks in the statically allocated
hash buckets, which are never freed.
However, this trick would not work if the hash table were resizeable,
so that the locks were now dynamically allocated.
In this case, there would need to be some means to prevent the hash
bucket from being freed during the time that its lock was being acquired.
\fi

\QuickQuiz{}
	자신의 락이 잡혀 있는 동안은 구조체가 메모리 해제 되지 않도록 할 수
	있는 방법들은 어떤 것들이 있을까요?
	\iffalse

	What are some ways of preventing a structure from being freed while
	its lock is being acquired?
	\fi
\QuickQuizAnswer{
	여기 \emph{존재 보장} 문제를 위한 해결책 몇가지가 있습니다:
	\iffalse

	Here are a few possible solutions to this \emph{existence guarantee}
	problem:
	\fi

	\begin{enumerate}
	\item	구조체별 락이 잡혀 있는 동안 유지되는 정적으로 할당된 락을 두는
		것으로, 계층적 락킹의 한 예입니다
		(Section~\ref{sec:SMPdesign:Hierarchical Locking} 을
		참고하세요).
		물론, 이런 목적으로 하나의 글로벌 락을 사용하는 것은
		허용불가능할 정도로 높은 락 경쟁 상황을 가져와서 성능과
		확장성의 드라마틱한 하락을 가져올 수 있습니다.
	\item	정적으로 할당된 락들의 배열을 두어서, 구조체의 어드레스를
		해싱해서 잡아야 할 락을 고르는 것으로,
		Chapter~\ref{chp:Locking} 에서 설명된 방식입니다.
		주어진 해시 함수가 충분히 높은 성능을 갖는다면, 이 방법은
		하나의 글로벌 락이 갖는 확장성의 한계를 해결할 수 있습니다만,
		읽기가 대부분인 상황에서는 락 획득 오버헤드가 수용불가할 정도로
		성능을 떨어뜨릴 수 있습니다.
	\item	가비지 콜렉터를 제공하는 소프트웨어 환경이라면 가비지 콜렉터를
		사용해서, 구조체가 참조되어 있는 동안은 메모리 해제되지 않도록
		하는 것입니다.
		이 방법은 잘 동작하고, 존재 보장의 짐을 (그리고 그외의 것들을)
		개발자의 어깨에서 내려놓게 하지만, 프로그램의 가비지 콜렉션
		오버헤드를 갖습니다.
		가비지 콜렉션 기술은 지난 수십년간 상당히 진보했지만, 그
		오버헤드는 일부 어플리케이션에서는 수용하기 어려울 만큼 높을 수
		있습니다.
		또한, 일부 어플리케이션들은 데이터 구조체의 배치와 위치 조정에
		대해 대부분의 가비지 콜렉션 환경에 비해 개발자에게 많은 연습을
		필요로 할수도 있습니다.
	\iffalse

	\item	Provide a statically allocated lock that is held while
		the per-structure lock is being acquired, which is an
		example of hierarchical locking (see
		Section~\ref{sec:SMPdesign:Hierarchical Locking}).
		Of course, using a single global lock for this purpose
		can result in unacceptably high levels of lock contention,
		dramatically reducing performance and scalability.
	\item	Provide an array of statically allocated locks, hashing
		the structure's address to select the lock to be acquired,
		as described in Chapter~\ref{chp:Locking}.
		Given a hash function of sufficiently high quality, this
		avoids the scalability limitations of the single global
		lock, but in read-mostly situations, the lock-acquisition
		overhead can result in unacceptably degraded performance.
	\item	Use a garbage collector, in software environments providing
		them, so that a structure cannot be deallocated while being
		referenced.
		This works very well, removing the existence-guarantee
		burden (and much else besides) from the developer's
		shoulders, but imposes the overhead of garbage collection
		on the program.
		Although garbage-collection technology has advanced
		considerably in the past few decades, its overhead
		may be unacceptably high for some applications.
		In addition, some applications require that the developer
		exercise more control over the layout and placement of
		data structures than is permitted by most garbage collected
		environments.
	\fi
	\item	가비지 콜렉터의 특수한 경우로, 글로벌 레퍼런스 카운터를 두거나
		레퍼런스 카운터들의 글로벌한 배열을 두는 방법이 있습니다.
	\item	안에서-밖으로의 참조 카운트라 생각할 수 있는,
		\emph{해저드 포인터}~\cite{MagedMichael04a} 들을 사용하는
		방법이 있습니다.
		해저드 포인터 기반의 알고리즘들은 쓰레드별 포인터들의 리스트를
		두어서, 이 리스트들에 있는 포인터의 존재가 연관된 구조체로의
		참조처럼 동작하게 합니다.
		해저드 포인터들은 흥미로운 연구 방향입니다만, 상품화 단계
		(2008년도 시점에선) 에선 아직 잘 쓰여지지 않고 있습니다.
	\item	트랜잭셔널
		메모리(TM)~\cite{Herlihy93a,DBLomet1977SIGSOFT,Shavit95} 를
		사용해서 데이터 구조체로 요청되는 각각의 참조와 수정이
		어토믹하게 수행되도록 하는 방법이 있습니다.
		TM 은 최근의 수년간 대단한 흥분을 일으켰고 상품 단계
		소프트웨어에서 일부 사용될 것처럼 보였지만, 개발자들은 몇가지
		주의를 기울여야
		하며~\cite{Blundell2005DebunkTM,Blundell2006TMdeadlock,McKenney2007PLOSTM}
		이는 특히 성능에 영향을 주는 코드에선 더더욱 그렇습니다.
		특히, 존재 보장은 트랜잭션이 글로벌 레퍼런스부터 업데이트되는
		데이터 원소들에 이르기까지 전체를 감쌀 것을 필요로 합니다.
	\item	극단적으로 가벼운 가비지 콜렉터의 추상화로 생각될 수 있는, RCU
		를 사용하는 것입니다.
		업데이트를 하는 쪽은 RCU 로 보호되는 데이터 구조체를 RCU 로
		보호되는 데이터를 읽는쪽이 여전히 참조를 하고 있는 동안은
		메모리에서 해제시킬 수 없습니다.
		RCU 는 읽기가 대부분인 데이터 구조체에서 상당히 많이 사용되고
		있고, Chapter~\ref{chp:Deferred Processing} 에서 이야기될
		것입니다.
	\iffalse

	\item	As a special case of a garbage collector, use a global
		reference counter, or a global array of reference counters.
	\item	Use \emph{hazard pointers}~\cite{MagedMichael04a}, which
		can be thought of as an inside-out reference count.
		Hazard-pointer-based algorithms maintain a per-thread list of
		pointers, so that the appearance of a given pointer on
		any of these lists acts as a reference to the corresponding
		structure.
		Hazard pointers are an interesting research direction, but
		have not yet seen much use in production (written in 2008).
	\item	Use transactional memory
		(TM)~\cite{Herlihy93a,DBLomet1977SIGSOFT,Shavit95},
		so that each reference and
		modification to the data structure in question is
		performed atomically.
		Although TM has engendered much excitement in recent years,
		and seems likely to be of some use in production software,
		developers should exercise some
		caution~\cite{Blundell2005DebunkTM,Blundell2006TMdeadlock,McKenney2007PLOSTM},
		particularly in performance-critical code.
		In particular, existence guarantees require that the
		transaction cover the full path from a global reference
		to the data elements being updated.
	\item	Use RCU, which can be thought of as an extremely lightweight
		approximation to a garbage collector.
		Updaters are not permitted to free RCU-protected
		data structures that RCU readers might still be referencing.
		RCU is most heavily used for read-mostly data structures,
		and is discussed at length in
		Chapter~\ref{chp:Deferred Processing}.
	\fi
	\end{enumerate}

	존재 보장에 대해 더 많은 내용을 위해선 Chapter~\ref{chp:Locking} 과
	\ref{chp:Deferred Processing} 을 참고하세요.
	\iffalse

	For more on providing existence guarantees, see
	Chapters~\ref{chp:Locking} and \ref{chp:Deferred Processing}.
	\fi
} \QuickQuizEnd

\subsection{Data Ownership}
\label{sec:SMPdesign:Data Ownership}

데이터 소유권은 각 쓰레드/CPU 가 자신에게 할당된 데이터의 부분집합을 어떤
동기화 오버헤드 없이 접근할 수 있게끔 데이터 구조를 쓰레드나 CPU 단위로
쪼갭니다.
하지만, 어떤 쓰레드가 다른 쓰레드의 데이터에 접근하길 원한다면, 그냥 할 수
없습니다.
대신, 이 쓰레드는 다른 쓰레드와 먼저 통신을 해서 다른 쓰레드가 그 일을 대신
해주거나 그 데이터의 소유권을 이전해 주도록 해야 합니다.

데이터 소유권은 불가사의해 보일 수도 있지만, 매우 자주 사용됩니다:
\iffalse

Data ownership partitions a given data structure over the threads
or CPUs, so that each thread/CPU accesses its subset of the data
structure without any synchronization overhead whatsoever.
However, if one thread wishes to access some other thread's data,
the first thread is unable to do so directly.
Instead, the first thread must communicate with the second thread,
so that the second thread performs the operation on behalf of the
first, or, alternatively, migrates the data to the first thread.

Data ownership might seem arcane, but it is used very frequently:
\fi
\begin{enumerate}
\item	한 CPU 나 쓰레드에 의해서만 접근될 수 있는 변수 (C 와 C++ 에서의 {\tt
	auto} 변수와 같은) 들은 모두 해당 CPU 나 프로세스에게 소유되어
	있습니다.
\item	사용자 인터페이스의 한 인스턴스는 해당 사용자의 컨텍스트를 소유합니다.
	병렬 데이터베이스 엔진과 상호작용하는 어플리케이션들은 그것들이 순차적
	프로그램인 것마냥 작성되는 것이 매우 흔한 일입니다.
	그런 어플리케이션들은 사용자 인터페이스와 그/그녀의 현재 동작을
	소유합니다.
	따라서 명시적인 병렬성은 데이터베이스 엔진 그 자체에 국한되어 있습니다.
\item	파라미터를 사용하는 시뮬레이션들은 종종 각 쓰레드가 파라미터 공간의
	특정 영역에 소유권을 갖게 하는 방법으로 병렬화 되곤 합니다.
	이런 타입의 문제를 위한 컴퓨팅 프레임웍도 존재합니다~\cite{BOINC2008}.
\iffalse

\item	Any variables accessible by only one CPU or thread
	(such as {\tt auto} variables in C
	and C++) are owned by that CPU or process.
\item	An instance of a user interface owns the corresponding
	user's context.  It is very common for applications
	interacting with parallel database engines to be
	written as if they were entirely sequential programs.
	Such applications own the user interface and his current
	action.  Explicit parallelism is thus confined to the
	database engine itself.
\item	Parametric simulations are often trivially parallelized
	by granting each thread ownership of a particular region
	of the parameter space.
	There are also computing frameworks designed for this
	type of problem~\cite{BOINC2008}.
\fi
\end{enumerate}

상당히 많은 공유가 존재한다면, 쓰레드들이나 CPU 들 사이의 통신은 상당한
복잡도와 오버헤드를 만들어낼 것입니다.
더 나아가서, 가장 많이 사용되는 데이터가 단일 CPU 에게 소유되어 있게 된다면, 이
CPU 는 ``핫스팟'' 이 될 것이고, Figure~\ref{fig:SMPdesign:Data and Skew} 에
그려진 것과 같은 결과를 낼 것입니다.
하지만, 공유가 필요하지 않은 상황이라면,
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search} 에 보여진
것처럼 데이터 소유권은 이상적인 성능을 내고, 순차적 프로그램처럼 간단해질 수
있을 것입니다.
그런 상황은 종종 ``당황스러울 정도로 병렬적'' 이라 불리곤 하고,
Figure~\ref{fig:SMPdesign:Data Locking} 에서 앞서 본것과 같은 상황과 닮아
있습니다.
\iffalse

If there is significant sharing, communication between the threads
or CPUs can result in significant complexity and overhead.
Furthermore, if the most-heavily used data happens to be that owned
by a single CPU, that CPU will be a ``hot spot'', sometimes with
results resembling that shown in Figure~\ref{fig:SMPdesign:Data and Skew}.
However, in situations where no sharing is required, data ownership
achieves ideal performance, and with code that can be as simple
as the sequential-program case shown in
Listing~\ref{lst:SMPdesign:Sequential-Program Hash Table Search}.
Such situations are often referred to as ``embarrassingly
parallel'', and, in the best case, resemble the situation
previously shown in Figure~\ref{fig:SMPdesign:Data Locking}.
\fi

% ./test_hash_null.exe 1000 0/100 1 1024 1
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 96.2913  max = 98.2337  min = 90.4095  std = 2.95314
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 91.5592  max = 97.3315  min = 89.9885  std = 2.88925
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 1
% ./test_hash_null.exe: avg = 93.3568  max = 106.162  min = 89.8828  std = 6.40418

% ./test_hash_null.exe 1000 0/100 1 1024 2
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 45.4526  max = 46.4281  min = 45.1954  std = 0.487791
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 46.0238  max = 49.2861  min = 45.1852  std = 1.63127
% ./test_hash_null.exe: nmilli: 1000 update/total: 0/100 nelements: 1 nbuckets: 1024 nthreads: 2
% ./test_hash_null.exe: avg = 46.6858  max = 52.6278  min = 45.1761  std = 2.97102

또다른 중요한 데이터 소유권 상황은 데이터가 읽기 전용이어서 모든 쓰레드가
복사본을 통해 그것을 ``소유'' 할 수 있는 경우입니다.

데이터 소유권은 Chapter~\ref{chp:Data Ownership} 에서 좀 더 자세히 다뤄질
겁니다.
\iffalse

Another important instance of data ownership occurs when the data
is read-only, in which case,
all threads can ``own'' it via replication.

Data ownership will be presented in more detail in
Chapter~\ref{chp:Data Ownership}.
\fi

\subsection{Locking Granularity and Performance}
\label{sec:SMPdesign:Locking Granularity and Performance}

이 섹션은 락킹 빈도와 성능 사이의 관계를 수학적인 동기화 효율성 관점에서
살펴봅니다.
수학이 지루한 독자분들은 이 섹션을 건너뛰셔도 됩니다.

사용할 방법은 하나의 공유되는 글로벌 변수에 대해 동작하는 동기화 메커니즘의
효율성을 위한 간단한 큐잉 모델을 M/M/1 큐를 기반으로 사용해 보는 것입니다.
M/M/1 큐잉 모델은 지수적으로 분포되는 ``도착간 비율 (inter-arrival rate)''
$\lambda$ 와 지수적으로 분포되는 ``서비스 비율 (service rate)'' $\mu$ 에
기반합니다.
도착간 비율 $\lambda$ 는 만약 동기화 비용이 전혀 들지 않는다면 시스템이 처리할
수 있는, 초당 동기화 오퍼레이션들의 평균 숫자로 생각될 수 있는데, 달리 말하자면
$\lambda$ 는 작업의 비동기 유닛의 오버헤드의 역수입니다.
예를 들어, 일의 각 유닛이 트랜잭션이고, 각 트랜잭션이 처리되는데 동기화
오버헤드를 제외하고 1 밀리세컨드가 걸린다면, $\lambda$ 는 초당 1,000 트랜잭션이
될 것입니다.
\iffalse

This section looks at locking granularity and performance from
a mathematical synchronization\-/efficiency viewpoint.
Readers who are uninspired by mathematics might choose to skip
this section.

The approach is to use a crude queueing model for the efficiency of
synchronization mechanism that operate on a single shared global
variable, based on an M/M/1 queue.
M/M/1 queuing models are based on an exponentially distributed
``inter-arrival rate'' $\lambda$ and an exponentially distributed
``service rate'' $\mu$.
The inter-arrival rate $\lambda$ can be thought of as the average
number of synchronization operations per second that the system
would process if the synchronization were free, in other words,
$\lambda$ is an inverse measure of the overhead of each non-synchronization
unit of work.
For example, if each unit of work was a transaction, and if each transaction
took one millisecond to process, excluding synchronization overhead,
then $\lambda$ would be 1,000 transactions per second.
\fi

서비스 비율 $\mu$ 는 비슷하게 정의됩니다만, CPU 들이 각자의 동기화
오퍼레이션들을 완료하기를 기다려야 한다는 사실을 무시하고 각 트랜잭션의
오버헤드가 존재하지 않는다면 시스템이 1초 안에 처리할 수 있는 동기화
오퍼레이션들의 수의 평균으로, 달리 말하자면, $\mu$ 는 컨텐션이 없을 때의 동기화
오버헤드라고 생각될 수 있겠습니다.
예를 들어, 각 동기화 오퍼레이션이 어토믹 값 증가 인스트럭션을 내포하고 있고,
컴퓨터 시스템은 각 CPU 에서 각자의 변수에 25 나노세컨드마다 어토믹 값 증가
인스트럭션을 수행할 수 있다고 해 봅시다.\footnote{
	물론, 같은 공유 변수의 값을 증가시키는 8 개의 CPU 가 존재한다면, 각각의
	CPU 는 자신의 값 증가 작업을 위해 25 나노세컨드를 소모하기 전에 다른
	CPU 들이 각자의 값 증가 작업을 마무리 할 때까지 175 나노세컨드를
	기다려야 할 것입니다.
	실제로는, 이 대기는 더 길어질텐데 그 변수를 한 CPU 에서 다른 CPU 로
	옮기는 시간도 걸리기 때문입니다.}
따라서 $\mu$ 의 값은 초당 40,000,000 어토믹 값 증가 일 것입니다.

물론, $\lambda$ 의 값은 CPU 수가 늘어나면 함께 늘어날텐데, 각 CPU 가 독립적으로
트랜잭션들을 처리할 수 있기 때문입니다 (다시 말하지만, 동기화를 무시합니다):
\iffalse

The service rate $\mu$ is defined similarly, but for the average
number of synchronization operations per second that the system
would process if the overhead of each transaction was zero, and
ignoring the fact that CPUs must wait on each other to complete
their synchronization operations, in other words, $\mu$ can be roughly
thought of as the synchronization overhead in absence of contention.
For example, suppose that each synchronization operation involves an atomic
increment instruction, and that a computer system is able to do
an atomic increment every 25 nanoseconds on each CPU
to a private variable.\footnote{
	Of course, if there are 8 CPUs all incrementing the same
	shared variable, then each CPU must wait at least 175 nanoseconds
	for each of the other CPUs to do its increment before consuming
	an additional 25 nanoseconds doing its own increment.
	In actual fact, the wait will be longer due to the need
	to move the variable from one CPU to another.}
The value of $\mu$ is therefore about 40,000,000 atomic increments
per second.

Of course, the value of $\lambda$ increases with increasing numbers of
CPUs, as each CPU is capable of processing transactions independently
(again, ignoring synchronization):
\fi

\begin{equation}
	\lambda = n \lambda_0
\end{equation}

$n$ 은 CPU 의 갯수이고 $\lambda_0$ 는 단일 CPU 의 트랜잭션 처리 가능량입니다.
단일 CPU 가 하나의 트랜잭션을 처리하는데 걸릴 것으로 기대되는 시간은
$1 / \lambda_0$ 임을 기억해 두십시오.

이 CPU 들은 다른 CPU 들이 각각 하나의 공유 변수의 값을 증가시킬 동안
``줄을 서서 기다려야'' 하기 때문에, 기대되는 전체 대기 시간을 표현하는데 M/M/1
큐잉 모델을 다음과 같이 사용할 수 있습니다:
\iffalse

where $n$ is the number of CPUs and $\lambda_0$ is the transaction-processing
capability of a single CPU.
Note that the expected time for a single CPU to execute a single transaction
is $1 / \lambda_0$.

Because the CPUs have to ``wait in line'' behind each other to get their
chance to increment the single shared variable, we can use the M/M/1
queueing-model expression for the expected total waiting time:
\fi

\begin{equation}
	T = \frac{1}{\mu - \lambda}
\end{equation}

앞의 $\lambda$ 값을 대입하면:
\iffalse

Substituting the above value of $\lambda$:
\fi

\begin{equation}
	T = \frac{1}{\mu - n \lambda_0}
\end{equation}

이제, 효율성은 동기화 없을 때 트랜잭션 하나를 처리하는데 필요한 시간 ($1 /
\lambda_0$) 과 동기화를 포함해서 필요한 시간 ($T + 1 / \lambda_0$) 사이의
비율입니다:
\iffalse

Now, the efficiency is just the ratio of the time required to process
a transaction in absence of synchronization ($1 / \lambda_0$)
to the time required including synchronization ($T + 1 / \lambda_0$):
\fi

\begin{equation}
	e = \frac{1 / \lambda_0}{T + 1 / \lambda_0}
\end{equation}

앞의 $T$ 값을 대입하고 간략화 하면:
\iffalse

Substituting the above value for $T$ and simplifying:
\fi

\begin{equation}
	e = \frac{\frac{\mu}{\lambda_0} - n}{\frac{\mu}{\lambda_0} - (n - 1)}
\end{equation}

하지만 $\mu / \lambda_0$ 의 값은 그저 트랜잭션을 처리하는데 필요한 시간과
(경쟁상황이 없는 상태에서의) 동기화 자체 오버헤드 간의 비율입니다.
이 비율을 $f$ 라고 하면, 이렇게 됩니다:
\iffalse

But the value of $\mu / \lambda_0$ is just the ratio of the time required
to process the transaction (absent synchronization overhead) to that of 
the synchronization overhead itself (absent contention).
If we call this ratio $f$, we have:
\fi

\begin{equation}
	e = \frac{f - n}{f - (n - 1)}
\end{equation}

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/synceff}}
\caption{Synchronization Efficiency}
\label{fig:SMPdesign:Synchronization Efficiency}
\end{figure}

Figure~\ref{fig:SMPdesign:Synchronization Efficiency} 는 동기화 효율성 $e$ 가
CPU/쓰레드 수 $n$ 에 의해 변화되는 모습을 오버헤드 비율 $f$ 몇개의 값과 함께
보여줍니다.
예를 들어, 25 나노세컨드 걸리는 어토믹 값 증가 연산을 사용하면 $f=10$ 선은 각
CPU 가 매 250 나노세컨드마다 어토믹 값 증가 연산을 시도하고, $f=100$ 라인은 각
CPU 가 수천개의 인스트럭션이 처리될 수 있는 시간인 2.5 마이크로세컨드마다
어토믹 값 증가 연산을 시도합니다.
각 조합의 결과가 CPU 나 쓰레드의 수가 늘어남에 따라 급격하게 떨어지는 것으로
보아, 하나의 글로벌 공유 변수에의 어토믹 조정을 통한 동기화 메커니즘은 현재의
하드웨어에서 많이 사용되면 잘 확장되지 못할 것이라 결론 내릴 수 있습니다.
이건 이 규칙들을 수학적으로 그려본 것으로, Chapter~\ref{chp:Counting} 에서
이야기한 병렬 카운팅 알고리즘을 이끌어내게 합니다.
\iffalse

Figure~\ref{fig:SMPdesign:Synchronization Efficiency} plots the synchronization
efficiency $e$ as a function of the number of CPUs/threads $n$ for
a few values of the overhead ratio $f$.
For example, again using the 25-nanosecond atomic increment, the
$f=10$ line corresponds to each CPU attempting an atomic increment
every 250 nanoseconds, and the $f=100$ line corresponds to each
CPU attempting an atomic increment every 2.5 microseconds,
which in turn corresponds to several thousand instructions.
Given that each trace drops off sharply with increasing numbers of
CPUs or threads, we can conclude that
synchronization mechanisms based on
atomic manipulation of a single global shared variable will not
scale well if used heavily on current commodity hardware.
This is a mathematical depiction of the forces leading to the parallel
counting algorithms that were discussed in Chapter~\ref{chp:Counting}.
\fi

이 효율성 컨셉은 정규적인 동기화가 적거나 아예 없을 때에도 효과적입니다.
예를 들어, 한 행렬의 행을 (``dot product'' 로) 다른 행렬의 열과 곱해서 세번째
행렬을 만들어내는 행렬 곱셈을 생각해 봅시다.
이 오퍼레이션들은 서로 겹치지 않기 대문에, 첫번째 행렬의 행들을 쓰레드들에
분할시켜서 각 쓰레드가 결과 행렬의 연관된 행을 계산하는 것이 가능합니다.
따라서 이 쓰레드들은 \path{matmul.c} 에서와 같이 아무런 동기화 오버헤드 없이
완전히 독립적으로 동작할수 있습니다.
따라서 병렬적 행렬 곱셈은 완벽한 효율성, 1.0을 가질 것이라 예상할 수도
있습니다.
\iffalse

The concept of efficiency is useful even in cases having little or
no formal synchronization.
Consider for example a matrix multiply, in which the columns of one
matrix are multiplied (via ``dot product'') by the rows of another,
resulting in an entry in a third matrix.
Because none of these operations conflict, it is possible to partition
the columns of the first matrix among a group of threads, with each thread
computing the corresponding columns of the result matrix.
The threads can therefore operate entirely independently, with no
synchronization overhead whatsoever, as is done in
\path{matmul.c}.
One might therefore expect a parallel matrix multiply to have a
perfect efficiency of 1.0.
\fi

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/matmuleff}}
\caption{Matrix Multiply Efficiency}
\label{fig:SMPdesign:Matrix Multiply Efficiency}
\end{figure}

하지만,
Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency} 는 다르게 이야기하는데,
특히 64 행 64 열 행렬 곱셈에서는 0.7 보다 나은 효율성을 절대 갖지 못합니다,
싱글쓰레드로 동작하는데도 불구하고 말이지요.
512 행 512 열 행렬 곱셈의 효율성은 10 쓰레드 아래에서는 1.0보다 조금 작은
것으로 측정되며 심지어 1024 행 1024 열 행렬 곱셈조차도 수십 쓰레드에서는 완벽한
효율성을 벗어나게 되고 맙니다.
더도 아니고 덜도 아니고, 이 그림은 몰아서 처리하기의 성능과 확장성에서의 이점을
분명하게 보여주고 있으며, 이로 인해 당신의 돈의 가치도 알 수 있게 해줍니다.
\iffalse

However,
Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
tells a different story, especially for a 64-by-64 matrix multiply,
which never gets above an efficiency of about 0.7, even when running
single-threaded.
The 512-by-512 matrix multiply's efficiency is measurably less
than 1.0 on as few as 10 threads, and even the 1024-by-1024 matrix
multiply deviates noticeably from perfection at a few tens of threads.
Nevertheless, this figure clearly demonstrates the performance and
scalability benefits of batching: If you must incur synchronization
overhead, you may as well get your money's worth.
\fi

\QuickQuiz{}
	싱글쓰레드로 동작하는 64 행 64 열 행렬 곱셈이 어떻게 1.0보다 낮은
	효율성을 가질 수 있죠?
	Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency} 의 모든
	조합에서의 결과들이 한 쓰레드에서만 돌아갈 때에는 정확히 1.0 의
	효율성을 보여야 하는 거 아닌가요?
	\iffalse

	How can a single-threaded 64-by-64 matrix multiple possibly
	have an efficiency of less than 1.0?
	Shouldn't all of the traces in
	Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
	have efficiency of exactly 1.0 when running on only one thread?
	\fi
\QuickQuizAnswer{
	\path{matmul.c} 프로그램은 명시된 수의 워커 쓰레드들을 생성하므로,
	하나의 워커 쓰레드만을 생성한 경우에도 쓰레드 생성 오버헤드는 발생할 수
	있습니다.
	하나의 워커 쓰레드의 경우에 쓰레드 생성 오버헤드를 없애기 위한 수정은
	독자 여러분의 연습문제로 남겨두겠습니다.
	\iffalse

	The \path{matmul.c} program creates the specified number of
	worker threads, so even the single-worker-thread case incurs
	thread-creation overhead.
	Making the changes required to optimize away thread-creation
	overhead in the single-worker-thread case is left as an
	exercise to the reader.
	\fi
} \QuickQuizEnd

이런 비효율성 아래, Section~\ref{sec:SMPdesign:Data Locking} 에서 이야기한
데이터 락킹과 같이 더 확장성 있는 방법을 생각해 보거나 다음 섹션에서 다룰 병렬
상황 빠른 수행 경로 해결책을 고려해 보는것도 가치가 있을 것입니다.
\iffalse

Given these inefficiencies,
it is worthwhile to look into more-scalable approaches
such as the data locking described in
Section~\ref{sec:SMPdesign:Data Locking}
or the parallel-fastpath approach discussed in the next section.
\fi

\QuickQuiz{}
	행렬 곱셈에서 데이터 병렬화 기법이 어떻게 도움이 될 수 있나요?
	그건 \emph{이미} 병렬적인 데이터잖아요!!!
	\iffalse

	How are data-parallel techniques going to help with matrix
	multiply?
	It is \emph{already} data parallel!!!
	\fi
\QuickQuizAnswer{
	주의를 기울이고 있으신 것 같아 기쁩니다!
	이 예는 데이터 병렬성은 매우 좋은 것이긴 하지만, 모든 비효율성의 원인을
	자동으로 제거하는 마술봉은 아니란 것을 보이기 위한 것임을 보이기 위한
	것입니다.
	64 쓰레드에게 ``한정적인'' 상황에서조차도 전체 성능을 가지고 선형적으로
	성능을 확장하는 데에는 설계와 구현의 모든 단계에서의 세심한 주의가
	필요합니다.
	\iffalse

	I am glad that you are paying attention!
	This example serves to show that although data parallelism can
	be a very good thing, it is not some magic wand that automatically
	wards off any and all sources of inefficiency.
	Linear scaling at full performance, even to ``only'' 64 threads,
	requires care at all phases of design and implementation.
	\fi

	특히, 분할된 조각의 크기에 매우 세심한 관심을 기울여야만 합니다.
	예를 들어, 64 행 64 열 행렬 곱셈 문제를 64 쓰레드에게 쪼갠다면, 각
	쓰레드는 64 개의 부동소수점 곱셈 연산만을 하게 될 것입니다.
	부동소수점 곱셈 연산의 비용은 쓰레드 생성의 오버헤드에 비하면 매우
	작습니다.

	교훈: 변화무쌍한 입력을 받을 수 있는 병렬 프로그램을 가지고 있다면,
	항상 입력의 크기를 체크하는 과정을 포함시켜서 병렬화 시킬 가치가 없을
	정도로 너무 작은 입력 사이즈에 대응하도록 하십시오.
	그리고 병렬화에 도움이 되지 않을 정도라면, 쓰레드를 생성하는데 필요한
	오버헤드를 감내하기에도 도움이 되지 않을 정도입니다, 그렇죠?
	\iffalse

	In particular, you need to pay careful attention to the
	size of the partitions.
	For example, if you split a 64-by-64 matrix multiply across
	64 threads, each thread gets only 64 floating-point multiplies.
	The cost of a floating-point multiply is minuscule compared to
	the overhead of thread creation.

	Moral: If you have a parallel program with variable input,
	always include a check for the input size being too small to
	be worth parallelizing.
	And when it is not helpful to parallelize, it is not helpful
	to incur the overhead required to spawn a thread, now is it?
	\fi
} \QuickQuizEnd

\section{Parallel Fastpath}
\label{sec:SMPdesign:Parallel Fastpath}

잘게 쪼개진 (그리고 따라서 \emph{일반적으로} 높은 성능을 갖는) 설계들은 굵게
쪼개진 설계들에 비해 일반적으로 더 복잡합니다.
많은 경우, 대부분의 오버헤드는 코드의 작은 부분에서 발생합니다~\cite{Knuth73}.
그러니 그 작은 부분에 집중하는 노력을 가져보는게 어떨까요?

이게 병렬 고속 수행 경로 디자인 패턴을 뒷받침하는 아이디어로, 전체 알고리즘을
공격적으로 병렬화 하려 할 때 요구되는 복잡성 없이 일반적인 경우를 위한 코드
수행 경로를 공격적으로 병렬화 시키는 것입니다.
이를 위해선 병렬화 하려는 특정 알고리즘만 이해해선 안되고, 그 알고리즘이 목표로
하는 워크로드에 대해서도 이해해야만 합니다.
병렬 고속 수행 경로를 만들기 위해선 커다란 창조성과 설계에 들이는 노력이 많은
경우에 필요합니다.

병렬 고속 수행 경로는 서로 다른 패턴들 (고속 수행 경로를 위해 하나, 다른 경우를
위해 또 하나) 을 조합하며, 따라서 임시적 패턴입니다.
아래에 나열된 병렬 고속 수행 경로의 예들은
Figure~\ref{fig:SMPdesign:Parallel-Fastpath Design Patterns} 에 그려진 것처럼
그 자신의 패턴을 정당화 하기 충분할 만큼 자주 사용됩니다:
\iffalse

Fine-grained (and therefore \emph{usually} higher-performance)
designs are typically more complex than are coarser-grained designs.
In many cases, most of the overhead is incurred by a small fraction
of the code~\cite{Knuth73}.
So why not focus effort on that small fraction?

This is the idea behind the parallel-fastpath design pattern, to aggressively
parallelize the common-case code path without incurring the complexity
that would be required to aggressively parallelize the entire algorithm.
You must understand not only the specific algorithm you wish
to parallelize, but also the workload that the algorithm will
be subjected to.  Great creativity and design
effort is often required to construct a parallel fastpath.

Parallel fastpath combines different patterns (one for the
fastpath, one elsewhere) and is therefore a template pattern.
The following instances of parallel
fastpath occur often enough to warrant their own patterns,
as depicted in Figure~\ref{fig:SMPdesign:Parallel-Fastpath Design Patterns}:
\fi

\begin{figure}[tbp]
\centering
\resizebox{2.3in}{!}{\includegraphics{SMPdesign/ParallelFastpath}}
% \includegraphics{SMPdesign/ParallelFastpath}
\caption{Parallel-Fastpath Design Patterns}
\label{fig:SMPdesign:Parallel-Fastpath Design Patterns}
\end{figure}

\begin{enumerate}
\item	Reader/Writer 락킹
	(Section~\ref{sec:SMPdesign:Reader/Writer Locking} 에서 설명합니다).
\item	고성능을 위해 Reader/Writer 락킹을 대체할 수 있으며 이 챕터에서는
	더이상 설명되지 않을 Read-copy update (RCU).
\item	Section~\ref{sec:SMPdesign:Hierarchical Locking} 에서 다뤄질 계층적
	락킹~(\cite{McKenney95b}).
\item	리소스 할당자 캐시~(\cite{McKenney95b,McKenney93}).
	더 자세한 내용을 위해선
	Section~\ref{sec:SMPdesign:Resource Allocator Caches} 을 참고하십시오.
\iffalse

\item	Reader/Writer Locking
	(described below in Section~\ref{sec:SMPdesign:Reader/Writer Locking}).
\item	Read-copy update (RCU), which may be used as a high-performance
	replacement for reader/writer locking, is introduced in
	Section~\ref{sec:defer:Read-Copy Update (RCU)}, and will not
	be discussed further in this chapter.
\item   Hierarchical Locking~(\cite{McKenney95b}), which is touched upon
	in Section~\ref{sec:SMPdesign:Hierarchical Locking}.
\item	Resource Allocator Caches~(\cite{McKenney95b,McKenney93}).
	See Section~\ref{sec:SMPdesign:Resource Allocator Caches}
	for more detail.
\fi
\end{enumerate}

\subsection{Reader/Writer Locking}
\label{sec:SMPdesign:Reader/Writer Locking}

동기화 오버헤드가 무시할만 하다면 (예를 들어, 프로그램이 커다란 크리티컬
섹션들에 굵게 쪼개진 병렬성을 사용한다면), 그리고 그 크리티컬 섹션들의 작은
부분들만이 데이터를 수정한다면, 여러 읽는 작업들을 병렬로 수행될 수 있도록 하면
확장성을 크게 개선시킬 것입니다.
쓰기 작업은 읽기 작업과도, 다른 쓰기 작업들과도 배타적으로 수행되어야 합니다.
reader-writer 락킹의 많은 구현이 존재하는데,
Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking} 에 설명된 POSIX
구현도 그 중 하나입니다.
Listing~\ref{lst:SMPdesign:Reader-Writer-Locking Hash Table Search} 는 해시
테이블이 reader-writer 락킹을 사용해 어떻게 구현될 수 있는지 보입니다.
\iffalse

If synchronization overhead is negligible (for example, if the program
uses coarse-grained parallelism with large critical sections), and if
only a small fraction of the critical sections modify data, then allowing
multiple readers to proceed in parallel can greatly increase scalability.
Writers exclude both readers and each other.
There are many implementations of reader-writer locking, including
the POSIX implementation described in
Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking}.
Listing~\ref{lst:SMPdesign:Reader-Writer-Locking Hash Table Search}
shows how the hash search might be implemented using reader-writer locking.
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}
  1 rwlock_t hash_lock;
  2
  3 struct hash_table
  4 {
  5   long nbuckets;
  6   struct node **buckets;
  7 };
  8
  9 typedef struct node {
 10   unsigned long key;
 11   struct node *next;
 12 } node_t;
 13
 14 int hash_search(struct hash_table *h, long key)
 15 {
 16   struct node *cur;
 17   int retval;
 18
 19   read_lock(&hash_lock);
 20   cur = h->buckets[key % h->nbuckets];
 21   while (cur != NULL) {
 22     if (cur->key >= key) {
 23       retval = (cur->key == key);
 24       read_unlock(&hash_lock);
 25       return retval;
 26     }
 27     cur = cur->next;
 28   }
 29   read_unlock(&hash_lock);
 30   return 0;
 31 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Reader-Writer-Locking Hash Table Search}
\label{lst:SMPdesign:Reader-Writer-Locking Hash Table Search}
\end{listing}

Reader/writer 락킹은 비대칭적 락킹에 대한 하나의 간단한 예입니다.
Snaman~\cite{Snaman87} 은 여러 클러스터 시스템에서 사용되는, 더 화려한 여섯개
모드의 비대칭적 락킹 디자인을 설명합니다.
일반적인 락킹과 reader-writer 락킹은 Chapter~\ref{chp:Locking} 에서 특별히
자세히 설명됩니다.
\iffalse

Reader/writer locking is a simple instance of asymmetric locking.
Snaman~\cite{Snaman87} describes a more ornate six-mode
asymmetric locking design used in several clustered systems.
Locking in general and reader-writer locking in particular is described
extensively in
Chapter~\ref{chp:Locking}.
\fi

\subsection{Hierarchical Locking}
\label{sec:SMPdesign:Hierarchical Locking}

계층적 락킹의 아이디어는 잘게 쪼개진 락을 잡는 동작을 하는 동안만 잡는 굵게
쪼개진 락을 두자는 것입니다.
Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
는 해시 테이블 탐색에 계층적 락킹이 어떻게 사용될 수 있을지 보여주는데, 또한 이
방법의 커다란 단점 역시 보여줍니다:
두번째 락을 잡기 위한 오버헤드를 감내했지만, 그 락은 짧은 시간동안만 잡습니다.
이 경우, 간단한 데이터 락킹 방법은 더 간단하고 더 좋은 성능을 보일 것입니다.
\iffalse

The idea behind hierarchical locking is to have a coarse-grained lock
that is held only long enough to work out which fine-grained lock
to acquire.
Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
shows how our hash-table search might be adapted to do hierarchical
locking, but also shows the great weakness of this approach:
we have paid the overhead of acquiring a second lock, but we only
hold it for a short time.
In this case, the simpler data-locking approach would be simpler
and likely perform better.
\fi

\begin{listing}[tb]
{ \scriptsize
\begin{verbbox}
  1 struct hash_table
  2 {
  3   long nbuckets;
  4   struct bucket **buckets;
  5 };
  6
  7 struct bucket {
  8   spinlock_t bucket_lock;
  9   node_t *list_head;
 10 };
 11
 12 typedef struct node {
 13   spinlock_t node_lock;
 14   unsigned long key;
 15   struct node *next;
 16 } node_t;
 17
 18 int hash_search(struct hash_table *h, long key)
 19 {
 20   struct bucket *bp;
 21   struct node *cur;
 22   int retval;
 23
 24   bp = h->buckets[key % h->nbuckets];
 25   spin_lock(&bp->bucket_lock);
 26   cur = bp->list_head;
 27   while (cur != NULL) {
 28     if (cur->key >= key) {
 29       spin_lock(&cur->node_lock);
 30       spin_unlock(&bp->bucket_lock);
 31       retval = (cur->key == key);
 32       spin_unlock(&cur->node_lock);
 33       return retval;
 34     }
 35     cur = cur->next;
 36   }
 37   spin_unlock(&bp->bucket_lock);
 38   return 0;
 39 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Hierarchical-Locking Hash Table Search}
\label{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
\end{listing}

\QuickQuiz{}
	계층적 락킹이 잘 동작할 만한 상황은 뭐가 있을까요?
	\iffalse

	In what situation would hierarchical locking work well?
	\fi
\QuickQuizAnswer{
	Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search} 의
	라인~31 에서의 비교 연산이 훨씬 무거운 연산으로 교체된다면,
	\co{bp->bucket_lock} 을 놓는 작업은 \emph{아마도} 락 경쟁을 충분히
	줄여줘서 추가적인 \co{cur->node_lock} 의 획득과 해제에 드는
	오버헤드보다 우세해질 수도 있을 것입니다.
	\iffalse

	If the comparison on line~31 of
	Listing~\ref{lst:SMPdesign:Hierarchical-Locking Hash Table Search}
	were replaced by a much heavier-weight operation,
	then releasing \co{bp->bucket_lock} \emph{might} reduce lock
	contention enough to outweigh the overhead of the extra
	acquisition and release of \co{cur->node_lock}.
	\fi
} \QuickQuizEnd

\subsection{Resource Allocator Caches}
\label{sec:SMPdesign:Resource Allocator Caches}

이 섹션에서는 병렬 고정 크기 블럭 메모리 얼로케이터의 단순화된 개요를
제공합니다.
더 자세한 설명은
literature~\cite{McKenney92a,McKenney93,Bonwick01slab,McKenney01e} 나
리눅스 커널~\cite{Torvalds2.6kernel} 에서 찾을 수 있습니다.
\iffalse

This section presents a simplified schematic of a parallel fixed-block-size
memory allocator.
More detailed descriptions may be found in the
literature~\cite{McKenney92a,McKenney93,Bonwick01slab,McKenney01e}
or in the Linux kernel~\cite{Torvalds2.6kernel}.
\fi

\subsubsection{Parallel Resource Allocation Problem}

병렬 메모리 할당자가 마주하는 기본적인 문제는 일반적인 경우의 매우 빠른 메모리
할당과 해제 기능을 제공해야 하는 필요와 불리한 할당 / 해제 패턴들을 마주했을 때
효과적으로 메모리를 분산시켜야 하는 필요성 사이의 갈등입니다.

이 갈등을 보기 위해, 이 문제에 데이터 소유권을 직접적으로 활용한 경우를 생각해
봅시다 --- 단순히 각 CPU 가 자기 몫을 소유하도록 메모리를 분할하는 방법입니다.
예를 들어, 두개의 CPU 를 갖고 2 기가바이트의 메모리를 갖는 시스템 (제가 바로
지금 글을 쓰고 있는 기계와 같습니다) 이라고 생각해 봅시다.
간단히 각 CPU 에 1 기가바이트씩 메모리를 할당해 주고 각 CPU 가 그 자신이
소유하고 있는 메모리 덩어리에 접근하도록 할 수 있는데, 이렇게 되면 락킹의
필요성과 복잡성, 그리고 오버헤드들이 없습니다.
불행히도, 이 간단한 계획은 간단한 생산자-소비자 워크로드 같은 경우에서 일어날
수 있는, CPU~0 이 모든 메모리를 할당받고 CPU~1 이 그걸 해제하는 알고리즘이
있다면 망가질 수 있습니다.

다른 극단적 방법인 코드 락킹의 경우에는 과한 락 경쟁과 오버헤드로 고통받게 될
수 있습니다~\cite{McKenney93}.
\iffalse

The basic problem facing a parallel memory allocator is the tension
between the need to provide extremely fast memory allocation and
freeing in the common case and the need to efficiently distribute
memory in face of unfavorable allocation and freeing patterns.

To see this tension, consider a straightforward application of
data ownership to this problem---simply carve up memory so that
each CPU owns its share.
For example, suppose that a system with two CPUs has two gigabytes
of memory (such as the one that I am typing on right now).
We could simply assign each CPU one gigabyte of memory, and allow
each CPU to access its own private chunk of memory, without the
need for locking and its complexities and overheads.
Unfortunately, this simple scheme breaks down if an algorithm happens
to have CPU~0 allocate all of the memory and CPU~1 the free it, as
would happen in a simple producer-consumer workload.

The other extreme, code locking, suffers from excessive lock contention
and overhead~\cite{McKenney93}.
\fi

\subsubsection{Parallel Fastpath for Resource Allocation}

일반적으로 사용되는 방법은 각각의 CPU 가 적당한 캐시나 블럭들을 소유하는 병렬
빠른 수행 경로를 사용하고 추가적인 블럭들을 위한 공유 풀에는 커다란 코드 락킹을
사용해서 관리하는 방법입니다.
한 CPU 가 메모리 블럭들을 독점하는 것을 막기 위해, 각 CPU 의 캐시에 존재할 수
있는 블럭들의 갯수에 제한을 걸어 둡니다.
두개의 CPU 가 있는 시스템에서, 메모리 블럭들의 흐름은
Figure~\ref{fig:SMPdesign:Allocator Cache Schematic} 에 보여진 대로일 것입니다:
한 CPU 가 자신의 풀이 꽉 차서 블럭을 하나 해제하려고 할 때에는, 블럭들을 글로벌
풀에 보내고, 비슷하게, 그 CPU 가 자신의 풀이 비어서 블럭을 할당받으려 할 때에는
글로벌 풀로부터 블럭들을 얻어옵니다.
\iffalse

The commonly used solution uses parallel fastpath with each CPU
owning a modest cache of blocks, and with a large code-locked
shared pool for additional blocks.
To prevent any given CPU from monopolizing the memory blocks,
we place a limit on the number of blocks that can be in each CPU's
cache.
In a two-CPU system, the flow of memory blocks will be as shown
in Figure~\ref{fig:SMPdesign:Allocator Cache Schematic}:
when a given CPU is trying to free a block when its pool is full,
it sends blocks to the global pool, and, similarly, when that CPU
is trying to allocate a block when its pool is empty, it retrieves
blocks from the global pool.
\fi

\begin{figure}[tbp]
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/allocatorcache}}
\caption{Allocator Cache Schematic}
\label{fig:SMPdesign:Allocator Cache Schematic}
\end{figure}

\subsubsection{Data Structures}

Listing~\ref{lst:SMPdesign:Allocator-Cache Data Structures} 에 할당자 캐시들의
``장난감'' 구현을 위한 실제 데이터 구조체가 있습니다.
Figure~\ref{fig:SMPdesign:Allocator Cache Schematic} 의 ``Global Pool'' 은
\co{struct globalmempool} 타입의 \co{globalmem} 으로 구현되었고 \co{struct
percpumempool} 타입의 per-CPU 변수 \co{percpumem} 으로 두개의 CPU 별 풀들이
구현되었습니다.
이 데이터 구조체들 둘 다 각자의 \co{pool} 필드 안의 블럭들로의 포인터들의
배열을 가지고 있는데, 이것들은 인덱스 0부터 위쪽으로 채워져 나갑니다.
따라서, 만약 \co{globalmem.pool[3]} 이 \co{NULL} 이라면, 인덱스 4부터 위쪽인 이
배열의 나머지들 역시 모두 \co{NULL} 이어야 합니다.
\co{cur} 필드는 \co{pool} 배열의 꽉찬 원소들 중 가장 높은 숫자의 인덱스를
갖는데, 만약 모든 원소들이 텅 비어있다면 -1을 갖습니다.
\co{globalmem.pool[0]} 부터 \co{globalmem.pool[globalmem.cur]} 사이의 모든
원소들은 반드시 꽉 차 있어야 하고, 나머지 것들은 모두 비어있어야만
합니다.\footnote{
	두 풀 사이즈 (\co{TARGET_POOL_SIZE} 와 \co{GLOBAL_POOL_SIZE}) 모두
	비현실적으로 작습니다만, 이 작은 크기가 이 프로그램의 행동이 어떻게
	이루어지는지 이해하기 위해 프로그램을 한단계씩 실행시켜 나가기 편하게
	도와줄 겁니다.}
\iffalse

The actual data structures for a ``toy'' implementation of allocator
caches are shown in
Listing~\ref{lst:SMPdesign:Allocator-Cache Data Structures}.
The ``Global Pool'' of Figure~\ref{fig:SMPdesign:Allocator Cache Schematic}
is implemented by \co{globalmem} of type \co{struct globalmempool},
and the two CPU pools by the per-CPU variable \co{percpumem} of
type \co{struct percpumempool}.
Both of these data structures have arrays of pointers to blocks
in their \co{pool} fields, which are filled from index zero upwards.
Thus, if \co{globalmem.pool[3]} is \co{NULL}, then the remainder of
the array from index 4 up must also be \co{NULL}.
The \co{cur} fields contain the index of the highest-numbered full
element of the \co{pool} array, or $-1$ if all elements are empty.
All elements from \co{globalmem.pool[0]} through
\co{globalmem.pool[globalmem.cur]} must be full, and all the rest
must be empty.\footnote{
	Both pool sizes (\co{TARGET_POOL_SIZE} and
	\co{GLOBAL_POOL_SIZE}) are unrealistically small, but this small
	size makes it easier to single-step the program in order to get
	a feel for its operation.}
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}
  1 #define TARGET_POOL_SIZE 3
  2 #define GLOBAL_POOL_SIZE 40
  3
  4 struct globalmempool {
  5   spinlock_t mutex;
  6   int cur;
  7   struct memblock *pool[GLOBAL_POOL_SIZE];
  8 } globalmem;
  9
 10 struct percpumempool {
 11   int cur;
 12   struct memblock *pool[2 * TARGET_POOL_SIZE];
 13 };
 14
 15 DEFINE_PER_THREAD(struct percpumempool, percpumem);
\end{verbbox}
}
\centering
\theverbbox
\caption{Allocator-Cache Data Structures}
\label{lst:SMPdesign:Allocator-Cache Data Structures}
\end{listing}

풀 데이터 구조체의 동작이 어떻게 될지에 대한 그림이
Figure~\ref{fig:SMPdesign:Allocator Pool Schematic} 에 있는데, 여섯개의
박스들은 \co{pool} 필드를 구성하는 포인터들의 배열을 의미하며, 그 앞의 숫자는
\co{cur} 필드를 의미합니다.
색이 칠해진 박스들은 \co{NULL} 이 아닌 포인터들을 의미하며, 비어있는 박스들은
\co{NULL} 포인터들을 의미합니다.
중요하지만 좀 혼란스러울 수도 있는 이야기입니다만 이 데이터 구조체가 항상
지키게 되는 사실 (불변식, invariant) 은, \co{cur} 필드는 항상 \co{NULL} 이 아닌
포인터들의 수보다 하나 작을 것이란 점입니다.
\iffalse

The operation of the pool data structures is illustrated by
Figure~\ref{fig:SMPdesign:Allocator Pool Schematic},
with the six boxes representing the array of pointers making up
the \co{pool} field, and the number preceding them representing
the \co{cur} field.
The shaded boxes represent non-\co{NULL} pointers, while the empty
boxes represent \co{NULL} pointers.
An important, though potentially confusing, invariant of this
data structure is that the \co{cur} field is always one
smaller than the number of non-\co{NULL} pointers.
\fi

\begin{figure}[tbp]
\centering
\resizebox{2.6in}{!}{\includegraphics{SMPdesign/AllocatorPool}}
\caption{Allocator Pool Schematic}
\label{fig:SMPdesign:Allocator Pool Schematic}
\end{figure}

\subsubsection{Allocation Function}

할당을 하는 함수인 \co{memblock_alloc()} 을
Listing~\ref{lst:SMPdesign:Allocator-Cache Allocator Function} 에서 확인할 수
있습니다.
Line~7 에서는 현재 쓰레드의 per-thread 풀을 가져오고, line~8 에서 그게
비어있는지 확인합니다.

만약 그렇다면, line~9-16 에서 line~9 에서 획득하고 line~16 에서 해제하는 스핀락
아래 글로벌 풀로부터 해당 per-thread 풀을 채우려 시도합니다.
Line~10-14 는 블럭들을 글로벌에서 per-thread 풀로 로컬 풀이 목표로 하는 크기
(절반) 에 도달하거나 글로벌 풀이 텅 빌 때까지 옮기고, line~15 에서는 per-thread
풀의 수를 올바른 값으로 설정합니다.

어떤 경우든, line~18 에서 per-thread 풀이 여전히 비어있는지 확인하고, 만약
그렇지 않다면, line~19-21 에서 블럭 하나를 제거하고 그걸 리턴합니다.
그렇지 않다면, line~23 에서 메모리가 부족하다는 슬픈 이야기를 전합니다.
\iffalse

The allocation function \co{memblock_alloc()} may be seen in
Listing~\ref{lst:SMPdesign:Allocator-Cache Allocator Function}.
Line~7 picks up the current thread's per-thread pool,
and line~8 check to see if it is empty.

If so, lines~9-16 attempt to refill it from the global pool
under the spinlock acquired on line~9 and released on line~16.
Lines~10-14 move blocks from the global to the per-thread pool until
either the local pool reaches its target size (half full) or
the global pool is exhausted, and line~15 sets the per-thread pool's
count to the proper value.

In either case, line~18 checks for the per-thread pool still being
empty, and if not, lines~19-21 remove a block and return it.
Otherwise, line~23 tells the sad tale of memory exhaustion.
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}
  1 struct memblock *memblock_alloc(void)
  2 {
  3   int i;
  4   struct memblock *p;
  5   struct percpumempool *pcpp;
  6
  7   pcpp = &__get_thread_var(percpumem);
  8   if (pcpp->cur < 0) {
  9     spin_lock(&globalmem.mutex);
 10     for (i = 0; i < TARGET_POOL_SIZE &&
 11                 globalmem.cur >= 0; i++) {
 12       pcpp->pool[i] = globalmem.pool[globalmem.cur];
 13       globalmem.pool[globalmem.cur--] = NULL;
 14     }
 15     pcpp->cur = i - 1;
 16     spin_unlock(&globalmem.mutex);
 17   }
 18   if (pcpp->cur >= 0) {
 19     p = pcpp->pool[pcpp->cur];
 20     pcpp->pool[pcpp->cur--] = NULL;
 21     return p;
 22   }
 23   return NULL;
 24 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Allocator-Cache Allocator Function}
\label{lst:SMPdesign:Allocator-Cache Allocator Function}
\end{listing}

\subsubsection{Free Function}
Listing~\ref{lst:SMPdesign:Allocator-Cache Free Function} 는 메모리 블럭 해제
함수를 보입니다.
Line~6 에서는 이 쓰레드의 풀로의 포인터를 얻어오고 line~7 에서 이 per-thread
풀이 꽉 차 있는지 확인합니다.

만약 그렇다면, line~8-15 에서 이 per-thread 풀의 절반을 글로벌 풀로 비워내는데,
이 때 line~8 과 14 에서 글로벌 풀을 위한 스핀락을 각각 잡고 풉니다.
Line~9-12 에서는 로컬에서 글로벌 풀로 블럭들을 옮기는 루프를 구현하고 있으며,
line~13 에서는 per-thread 풀의 카운트를 올바른 값으로 재조정 합니다.

어떤 경우든, line~16 에서는 새로 해제된 블럭을 per-thread 풀에 넣습니다.
\iffalse

Listing~\ref{lst:SMPdesign:Allocator-Cache Free Function} shows
the memory-block free function.
Line~6 gets a pointer to this thread's pool, and
line~7 checks to see if this per-thread pool is full.

If so, lines~8-15 empty half of the per-thread pool into the global pool,
with lines~8 and 14 acquiring and releasing the spinlock.
Lines~9-12 implement the loop moving blocks from the local to the
global pool, and line~13 sets the per-thread pool's count to the proper
value.

In either case, line~16 then places the newly freed block into the
per-thread pool.
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}
  1 void memblock_free(struct memblock *p)
  2 {
  3   int i;
  4   struct percpumempool *pcpp;
  5
  6   pcpp = &__get_thread_var(percpumem);
  7   if (pcpp->cur >= 2 * TARGET_POOL_SIZE - 1) {
  8     spin_lock(&globalmem.mutex);
  9     for (i = pcpp->cur; i >= TARGET_POOL_SIZE; i--) {
 10       globalmem.pool[++globalmem.cur] = pcpp->pool[i];
 11       pcpp->pool[i] = NULL;
 12     }
 13     pcpp->cur = i;
 14     spin_unlock(&globalmem.mutex);
 15   }
 16   pcpp->pool[++pcpp->cur] = p;
 17 }
\end{verbbox}
}
\centering
\theverbbox
\caption{Allocator-Cache Free Function}
\label{lst:SMPdesign:Allocator-Cache Free Function}
\end{listing}

\subsubsection{Performance}

대략적인 성능 결과\footnote{
	이 데이터는 통계적으로 유의미한 방식으로 수집되지는 않았습니다, 따라서
	많은 비판적 시각과 의심을 가지고 봐야만 합니다.
	좋은 데이터 수집과 요약을 위한 방법은 Chapter~\ref{chp:Validation} 에서
	이야기 됩니다.
	그렇다곤 하나, 반복해서 진행한 수행은 비슷한 결과를 내놓았고, 이 값들은
	비슷한 알고리즘들의 더 세심하게 설계된 성능 평가 실험과 비슷한 결과를
	보입니다.}
	가 Figure~\ref{fig:SMPdesign:Allocator Cache Performance} 에 있는데, 이
데이터를 위한 성능 평가 실험은 1GHz 로 동작하는 듀얼코어 Intel x86 (CPU 당 4300
bogomips) 에서 각 CPU 의 캐시에 최대 여섯개의 블럭들을 주고 수행되었습니다.
이 마이크로 벤치마크에서, 각 쓰레드는 반복적으로 한 그룹의 블럭들을 할당받고 그
그룹의 모든 블럭들을 해제하는데, 이 그룹에 속하는 블럭들의 갯수는 x 축에
표시되는 ``allocation run length (할당 수행 시간 길이)'' 와 같습니다.
Y 축은 마이크로세컨드당 성공한 할당/해제 짝들의 갯수를 보입니다 --- 실패한
할당들은 카운트 되지 않습니다.
``X'' 는 두 쓰레드 수행의 결과이고, ``+'' 는 단일 쓰레드 수행의 결과입니다.
\iffalse

Rough performance results\footnote{
	This data was not collected in a statistically meaningful way,
	and therefore should be viewed with great skepticism and suspicion.
	Good data-collection and -reduction practice is discussed
	in Chapter~\ref{chp:Validation}.
	That said, repeated runs gave similar results, and these results
	match more careful evaluations of similar algorithms.}
are shown in
Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
running on a dual-core Intel x86 running at 1\,GHz (4300 bogomips per CPU)
with at most six blocks allowed in each CPU's cache.
In this micro-benchmark,
each thread repeatedly allocates a group of blocks and then frees all
the blocks in that group, with
the number of blocks in the group being the ``allocation run length''
displayed on the x-axis.
The y-axis shows the number of successful allocation/free pairs per
microsecond---failed allocations are not counted.
The ``X''s are from a two-thread run, while the ``+''s are from a
single-threaded run.
\fi

\begin{figure}[tbp]
\centering
\resizebox{2.5in}{!}{\includegraphics{SMPdesign/smpalloc}}
\caption{Allocator Cache Performance}
\label{fig:SMPdesign:Allocator Cache Performance}
\end{figure}

할당 수행 시간 길이가 6일 때 까지는 선형적으로 확장되고 훌륭한 성능을 보이지만,
6보다 큰 할당 수행 시간 길이에서는 부족한 성능을 보이고 거의 항상 \emph{음의}
확장도를 보임에 주의하시기 바랍니다.
따라서 \co{TARGET_POOL_SIZE} 의 크기를 충분히 크게 잡는게 상당히 중요한데,
다행히도 실제 사례~\cite{McKenney01e} 에서는 그렇게 하기가 상당히 쉬운 경우가
일반적이고, 특히 오늘날의 큰 메모리에선 더욱 그렇습니다.
예를 들어, 대부분의 시스템에서 \co{TARGET_POOL_SIZE} 를 100 으로 잡는 것은
상당히 합리적으로, 이 경우 전체 시간의 99\,\% 는 할당과 해제 작업들은 per-thread
풀에서만 이루어질 것이 보장됩니다.

이 그림에서 볼 수 있듯이, 데이터 소유권이 적용될 수 있는 일반적인 경우의 상황
(할당 수행 시간이 6 미만일 때) 에서는 락을 반드시 사용해야만 하는 상황들에 비해
훨씬 향상된 성능을 보입니다.
일반적인 경우에 동기화를 피하는 것은 이 책 전체에 걸쳐 반복되는 주제가 될
것입니다.
\iffalse

Note that run lengths up to six scale linearly and give excellent performance,
while run lengths greater than six show poor performance and almost always
also show \emph{negative} scaling.
It is therefore quite important to size \co{TARGET_POOL_SIZE}
sufficiently large,
which fortunately is usually quite easy to do in actual
practice~\cite{McKenney01e}, especially given today's large memories.
For example, in most systems, it is quite reasonable to set
\co{TARGET_POOL_SIZE} to 100, in which case allocations and frees
are guaranteed to be confined to per-thread pools at least 99\,\% of
the time.

As can be seen from the figure, the situations where the common-case
data-ownership applies (run lengths up to six) provide greatly improved
performance compared to the cases where locks must be acquired.
Avoiding synchronization in the common case will be a recurring theme through
this book.
\fi

\QuickQuiz{}
	Figure~\ref{fig:SMPdesign:Allocator Cache Performance} 에서, 세개의
	샘플들마다 할당 수행 시간 길이가 증가함에 따라 성능이 따라 증가하는
	패턴이 존재하는데, 예를 들어, 할당 수행 시간 길이 10, 11, 12 의
	경우입니다.
	왜 그런거죠?
	\iffalse

	In Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
	there is a pattern of performance rising with increasing run
	length in groups of three samples, for example, for run lengths
	10, 11, and 12.
	Why?
	\fi
\QuickQuizAnswer{
	이는 per-CPU 타겟 값이 3이기 때문입니다.
	할당 수행 시간 길이 12 의 경우는 글로벌 풀 락을 두번 잡아야 하는데,
	반면 할당 수행 시간 길이 13은 글로벌 풀 락을 세번 잡아야 합니다.
	\iffalse

	This is due to the per-CPU target value being three.
	A run length of 12 must acquire the global-pool lock twice,
	while a run length of 13 must acquire the global-pool lock
	three times.
	\fi
} \QuickQuizEnd

\QuickQuiz{}
	할당 실패 횟수들은 두개 쓰레드를 사용한 테스트에서 할당 수행 길이가 19
	이상일 때 발견되었습니다.
	글로벌 풀 사이즈가 40 이고 per-thread 타겟 풀 사이즈 $s$ 가 3이고,
	쓰레드의 갯수 $n$ 가 2이며, per-thread 풀들은 초기에 비어있고 메모리를
	아무도 사용하고 있지 않다고 가정하는 조건 하에서, 할당 실패가 발생할 수
	있는 최소의 할당 수행 시간 길이 $m$ 은 무엇일까요?
	(각 쓰레드는 메모리의 $m$ 블럭을 할당받고 $m$ 블럭의 메모리를 해제하는
	것을 반복함을 다시 이야기 합니다.)
	그 대신에, $n$ 쓰레드들이 각각 풀 사이즈 $s$ 를 갖는다면, 그리고 각
	쓰레드가 먼저 메모리의 $m$ 개 블럭을 할당받고 그 $m$ 블럭들을 해제하는
	것을 반복한다고 하는 조건이라면, 글로벌 풀 사이즈는 얼마나 커야 할까요?
	\emph{Note:} 올바른 답을 얻기 위해서는 \path{smpalloc.c} 소스 코드를
	자세히 들여다보고, 한단계 한단계씩 들여다봐야 할 겁니다.
	분명 경고했어요!
	\iffalse

	Allocation failures were observed in the two-thread
	tests at run lengths of 19 and greater.
	Given the global-pool size of 40 and the per-thread target
	pool size $s$ of three, number of threads $n$ equal to two,
	and assuming that the per-thread pools are initially
	empty with none of the memory in use, what is the smallest allocation
	run length $m$ at which failures can occur?
	(Recall that each thread repeatedly allocates $m$ block of memory,
	and then frees the $m$ blocks of memory.)
	Alternatively, given $n$ threads each with pool size $s$, and
	where each thread repeatedly first allocates $m$ blocks of memory
	and then frees those $m$ blocks, how large must the global pool
	size be?
	\emph{Note:} Obtaining the correct answer will require you to
	examine the \path{smpalloc.c} source code, and very likely
	single-step it as well.
	You have been warned!
	\fi
\QuickQuizAnswer{
	이 답안은 Alexey Roytman 이 제출한 것으로부터 받아들여졌습니다.
	이는 다음의 정의에 기초합니다:
	\begin{description}
	\item[$g$]	전역적으로 사용 가능한 블럭들의 갯수
	\item[$i$]	초기화 쓰레드의 per-thread 풀에 남아 있는 블럭들의 수.
			(이게 코드를 들여다봐야 하는 이유 중 하나입니다!)
	\item[$m$]	할당/해제 수행 시간 길이.
	\item[$n$]	초기화 쓰레드를 제외한 쓰레드의 수.
	\item[$p$]	실제로 할당된 블럭들과 per-thread 풀에 남아있는
			블럭들을 포함해서 쓰레드 당 최대 블럭 소비량.
	\end{description}
	\iffalse

	This solution is adapted from one put forward by Alexey Roytman.
	It is based on the following definitions:

	\begin{description}
	\item[$g$]	Number of blocks globally available.
	\item[$i$]	Number of blocks left in the initializing thread's
			per-thread pool.  (This is one reason you needed
			to look at the code!)
	\item[$m$]	Allocation/free run length.
	\item[$n$]	Number of threads, excluding the initialization thread.
	\item[$p$]	Per-thread maximum block consumption, including
			both the blocks actually allocated and the blocks
			remaining in the per-thread pool.
	\end{description}
	\fi

	$g$, $m$, 그리고 $n$ 의 값은 문제에서 이미 주어졌습니다.
	$p$ 의 값은 $m$ 을 $s$ 의 배수로 반올림된 값으로, 다음과 같습니다:
	\iffalse

	The values $g$, $m$, and $n$ are given.  The value for $p$ is
	$m$ rounded up to the next multiple of $s$, as follows:
	\fi

	\begin{equation}
		p = s \left \lfloor \frac{m + s - 1}{s} \right \rfloor
	\label{sec:SMPdesign:p}
	\end{equation}

	$i$ 의 값은 다음과 같습니다:
	\iffalse

	The value for $i$ is as follows:
	\fi

	\begin{equation}
		i = \left \{
			\begin{array}{l}
				g \pmod{2 s} = 0: 2 s \\
				g \pmod{2 s} \ne 0: g \pmod{2 s}
			\end{array}
		    \right .
	\label{sec:SMPdesign:i}
	\end{equation}

	\begin{figure}[tb]
	\centering
	\resizebox{3in}{!}{\includegraphics{SMPdesign/smpalloclim}}
	\caption{Allocator Cache Run-Length Analysis}
	\label{fig:SMPdesign:Allocator Cache Run-Length Analysis}
	\end{figure}

	이 값들 사이의 관계는
	Figure~\ref{fig:SMPdesign:Allocator Cache Run-Length Analysis} 에
	보여진 것과 같습니다.
	글로벌 풀은 이 그림의 꼭대기에 그려져 있고, ``나머지'' 초기화 쓰레드의
	쓰레드별 풀과 쓰레드별 할당은 가장 왼쪽의 두개의 박스들로 그려져
	있습니다.
	이 초기화 쓰레드는 할당된 블럭을 가지고 있지 않습니다만, $i$ 개의
	블럭들이 초기화 쓰레드의 쓰레드별 풀에 고립되어 있습니다.
	가장 오른쪽의 두개의 박스들은 최대 가능한 수의 블럭을 쥐고 있는
	쓰레드들의 쓰레드별 풀들과 쓰레드별 할당들이고, 왼쪽에서 두번째의
	박스들은 현재 할당을 시도하고 있는 쓰레드를 가리킵니다.

	블럭 전체의 갯수는 $g$ 이고, 쓰레드별 할당과 쓰레드별 풀들을 함께
	고려해보면, 우리는 글로벌 풀이 $g-i-p(n-1)$ 블럭들을 가지고 있음을 알
	수 있습니다.
	할당을 하는 쓰레드가 할당에 성공하려면, 글로벌 풀에는 최소한 $m$ 개의
	블럭들은 남아 있어야 하는데, 달리 말하자면 다음과 같습니다:
	\iffalse

	The relationships between these quantities is shown in
	Figure~\ref{fig:SMPdesign:Allocator Cache Run-Length Analysis}.
	The global pool is shown on the top of this figure, and
	the ``extra'' initializer thread's per-thread pool and
	per-thread allocations are the left-most pair of boxes.
	The initializer thread has no blocks allocated, but has
	$i$ blocks stranded in its per-thread pool.
	The rightmost two pairs of boxes are the per-thread pools and
	per-thread allocations of threads holding the maximum possible
	number of blocks, while the second-from-left pair of boxes
	represents the thread currently trying to allocate.

	The total number of blocks is $g$, and adding up the per-thread
	allocations and per-thread pools, we see that the global pool
	contains $g-i-p(n-1)$ blocks.
	If the allocating thread is to be successful, it needs at least
	$m$ blocks in the global pool, in other words:
	\fi

	\begin{equation}
		g - i - p(n - 1) \ge m
	\label{sec:SMPdesign:g-vs-m}
	\end{equation}

	문제에서 $g=40$, $s=3$, $n=2$ 였습니다.
	Equation~\ref{sec:SMPdesign:i} 에 따라 $i=4$ 이고
	Equation~\ref{sec:SMPdesign:p} 에 따라 $m=18$ 이면 $p=18$ 이고 $m=19$
	이면 $p=21$ 입니다.
	이것들을 Equation~\ref{sec:SMPdesign:g-vs-m} 에 대입해 보면 $m=18$ 은
	넘치지 않는 반면, $m=19$ 는 넘칠 것을 알 수 있습니다.

	$i$ 의 존재는 버그로 여겨질 수도 있을 것입니다.
	무엇보다도, 왜 메모리를 초기화 쓰레드의 캐시에 고립된 채로 남겨둡니까?
	이를 고치는 한가지 방법은 현재 쓰레드의 풀을 글로벌 풀로 비워버리는
	\co{memblock_flush()} 함수를 제공하는 것이 될 것입니다.
	그렇게 되면 이 초기화 쓰레드는 블럭들을 모두 해제한 이후에 이 함수를
	호출할 수 있을 것입니다.
	\iffalse

	The question has $g=40$, $s=3$, and $n=2$.
	Equation~\ref{sec:SMPdesign:i} gives $i=4$, and
	Equation~\ref{sec:SMPdesign:p} gives $p=18$ for $m=18$
	and $p=21$ for $m=19$.
	Plugging these into Equation~\ref{sec:SMPdesign:g-vs-m}
	shows that $m=18$ will not overflow, but that $m=19$ might
	well do so.

	The presence of $i$ could be considered to be a bug.
	After all, why allocate memory only to have it stranded in
	the initialization thread's cache?
	One way of fixing this would be to provide a \co{memblock_flush()}
	function that flushed the current thread's pool into the
	global pool.
	The initialization thread could then invoke this function
	after freeing all of the blocks.
	\fi
} \QuickQuizEnd

\subsubsection{Real-World Design}

이 장난감 병렬 리소스 할당자는 상당히 간단했습니다만 실제 세계에서의 설계는 이
방법으로부터 여러가지 방법으로 확장됩니다.

먼저, 실제 세계에서의 할당자는 넓은 범위의 할당 크기들을 다룰 수 있어야 하는데,
이 장난감 예제에서 보았던 단일한 크기와 반대됩니다.
이를 해결하는 대중적인 방법 하나는 1980년대 후반 BSD 메모리
할당자~\cite{McKusick88} 같은 방식으로, 고정된 크기들의 집합을 제공하는 것인데,
외부의, 그리고 내부의 파편화 문제를 균형 맞추기 위해 그렇게 공간을 사용합니다.
이렇게 하는 것은 곧 ``globalmem'' 변수가 크기별로 복사본이 존재해야 함을
의미하고, 연관된 락 역시 비슷하게 복사되어야 해서, 이 장난감 프로그램의 코드
락킹보다는 데이터 락킹에 가까운 결과를 이끌어낼 것입니다.
\iffalse

The toy parallel resource allocator was quite simple, but real-world
designs expand on this approach in a number of ways.

First, real-world allocators are required to handle a wide range
of allocation sizes, as opposed to the single size shown in this
toy example.
One popular way to do this is to offer a fixed set of sizes, spaced
so as to balance external and internal fragmentation, such as in
the late-1980s BSD memory allocator~\cite{McKusick88}.
Doing this would mean that the ``globalmem'' variable would need
to be replicated on a per-size basis, and that the associated
lock would similarly be replicated, resulting in data locking
rather than the toy program's code locking.
\fi

두번째로, 상품 단계 품질의 시스템들은 메모리의 용도를 바꿀수도 있어야만 하는데,
다시 말해 그것들은 블럭들을 합체해서 페이지~\cite{McKenney93} 와 같은 더 큰
구조체로 만들어질 수도 있어야만 합니다.
이 합체 역시 락으로 보호되어야 하는데, 이것 역시 앞의 것들과 마찬가지로
크기별로 복사본이 있어야 할 수 있습니다.

세번째로, 합쳐진 메모리는 아랫단의 메모리 시스템에 반환되어야만 하고, 메모리의
페이지들은 또한 그 아랫단의 메모리 시스템들로부터 할당되어야만 합니다.
이 단계에서 필요한 락킹은 이 아랫단의 메모리 시스템에 의존적일 것입니다만, 코드
락킹이 될 수도 있습니다.
코드 락킹은 종종 이 단계에서 받아들여질 수 있는데, 이 단계에 도달하는 것은 잘
설계된 시스템에서는 매우 희박하게 이루어지기 때문입니다~\cite{McKenney01e}.

이 실제 세계 설계의 커다란 복잡도에도 불구하고, 그 아래 자리잡고 있는
아이디어는 똑같습니다 --- Table~\ref{fig:app:questions:Schematic of Real-World
Parallel Allocator} 에 보인것과 같은 병렬 빠른 수행 경로의 반복적인 적용.
\iffalse

Second, production-quality systems must be able to repurpose memory,
meaning that they must be able to coalesce blocks into larger structures,
such as pages~\cite{McKenney93}.
This coalescing will also need to be protected by a lock, which again
could be replicated on a per-size basis.

Third, coalesced memory must be returned to the underlying memory
system, and pages of memory must also be allocated from the underlying
memory system.
The locking required at this level will depend on that of the underlying
memory system, but could well be code locking.
Code locking can often be tolerated at this level, because this
level is so infrequently reached in well-designed systems~\cite{McKenney01e}.

Despite this real-world design's greater complexity, the underlying
idea is the same---repeated application of parallel fastpath,
as shown in
Table~\ref{fig:app:questions:Schematic of Real-World Parallel Allocator}.
\fi

\begin{table}[tbp]
\rowcolors{1}{}{lightgray}
\small
\centering
\renewcommand*{\arraystretch}{1.25}
\begin{tabularx}{\twocolumnwidth}{ll>{\raggedright\arraybackslash}X}
\toprule
Level	& Locking & Purpose \\
\midrule
Per-thread pool	  & Data ownership & High-speed allocation \\
Global block pool & Data locking   & Distributing blocks among threads \\
Coalescing	  & Data locking   & Combining blocks into pages \\
System memory	  & Code locking   & Memory from/to system \\
\bottomrule
\end{tabularx}
\caption{Schematic of Real-World Parallel Allocator}
\label{fig:app:questions:Schematic of Real-World Parallel Allocator}
\end{table}

\input{SMPdesign/beyond}
