% future/formalregress.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\section{Formal Regression Testing?}
\label{sec:future:Formal Regression Testing?}

Formal verification 은 일부 경우에 유용한 것으로 증명되었습니다만, hard-core
formal verification 이 리눅스 커널과 같이 복잡한 동시성을 가진 코드를 위한
자동화된 regression-test 도구에 포함될 수 있을 것인지에 대한 질문이 많습니다.
리눅스 커널 SRCU를 위한 이론적 증명은 이미 있지만~\cite{LanceRoy2017CBMC-SRCU},
이 테스트는 가장 간단한 RCU 구현들 가운데 하나의 작은 부분을 위한 테스트이고,
이걸 계속 변화하는 리눅스 커널에 맞춰 유지하기는 어려움이 증명되었습니다.
따라서 formal verification 을 리눅스 커널의 regression test 에 첫번째 멤버로
추가하기 위해 뭐가 필요할지 물어볼 가치가 있습니다.

다음은 좋은 시작이 될 수 있을
겁니다~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:
\iffalse

Formal verification has proven useful in some cases, but a pressing
open question is whether hard-core formal verification will ever be
included in automated regression-test suites for complex concurrent
code bases, such as the Linux kernel.
Although there is already a proof of concept for Linux-kernel
SRCU~\cite{LanceRoy2017CBMC-SRCU}, this test is for a small portion
of one of the simplest RCU implementations, and has proven difficult
to keep it caught up with the ever-changing Linux kernel.
It is therefore worth asking what would be required to incorporate
formal verification as first-class members of the Linux kernel's
regression tests.

The following list is a good
start~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:
\fi

\begin{enumerate}
\item	모든 필요한 변환은 자동화 되어야만 합니다.
\item	환경 (메모리 순서 규칙 포함) 은 올바르게 처리되어야만 합니다.
\item	메모리와 CPU 오버헤드는 받아들일 수 있을 만큼 적어야만 합니다.
\item	버그의 위치를 알려주는 정보가 제공되어야만 합니다.
\item	소스코드와 입력 이외의 정보는 너무 범위가 크지 않아야만 합니다.
\item	발견되는 버그는 해당 코드의 사용자들에 관련되어 있어야만 합니다.
\iffalse

\item	Any required translation must be automated.
\item	The environment (including memory ordering) must be correctly
	handled.
\item	The memory and CPU overhead must be acceptably modest.
\item	Specific information leading to the location of the bug
	must be provided.
\item	Information beyond the source code and inputs must be
	modest in scope.
\item	The bugs located must be relevant to the code's users.
\fi
\end{enumerate}

이 목록은 만들어져가는 중입니다만 Richard Bornat 의 격언에 비하면 간단합니다:
``Formal-verification 연구자들은 개발자들이 작성하는 코드를 그들이 작성한
언어로, 그들이 수행하는 환경에서 검증해야 한다.''
다음 섹션들은 앞의 요구사항들 각각에 대해 논해보고, 이어서 몇가지 도구들이 이
요구사항들을 얼만큼 만족시키고 있는지 점수를 매겨봅니다.
\iffalse

This list builds on, but is somewhat more modest than, Richard Bornat's
dictum: ``Formal-verification researchers should verify the code that
developers write, in the language they write it in, running in the
environment that it runs in, as they write it.''
The following sections discuss each of the above requirements, followed
by a section presenting a scorecard of how well a few tools stack up
against these requirements.
\fi

\subsection{Automatic Translation}
\label{sec:future:Automatic Translation}

Promela 와 \co{spin} 이 설계에는 도움이 되지만, 여러분이 정형적으로 C-언어
프로그램을 regression-test 하려면, 여러분은 여러분의 코드를 다시 검증하고 싶을
때마다 Promela 로 일일이 손으로 변환시켜야만 합니다.
여러분의 코드가 매 60-90일마다 릴리즈되는 리눅스 커널 안에 있다면, 여러분은
매년 네번에서 여섯번씩 수작업으로 번환을 해야 할겁니다.
시간이 흐름에 따라, 사람의 실수가 생겨날 텐데, 이는 해당 검증이 소스 코드에
들어맞지 않음을 의미해서, 해당 검증이 쓸모없게 만듭니다.
반복된 검증은 해당 formal-verification 도구가 여러분의 코드를 직접 입력받을 수
있거나, 여러분의 코드를 검증에 필요한 형태로 자동으로 변환해주는 도구가 있을
것을 필요로 합니다.
\iffalse

Although Promela and \co{spin}
are invaluable design aids, if you need to formally regression-test
your C-language program, you must hand-translate to Promela each time
you would like to re-verify your code.
If your code happens to be in the Linux kernel, which releases every
60-90 days, you will need to hand-translate from four to six times
each year.
Over time, human error will creep in, which means that the verification
won't match the source code, rendering the verification useless.
Repeated verification clearly requires either that the formal-verification
tooling input your code directly, or that there be automatic translation
of your code to the form required for verification.
\fi

PPCMEM 과 \co{herd} 는 이론적으로 어셈블리 언어와 C++ 코드를 직접 입력받을 수
있습니다만, 이 도구들은 매우 작은 리트머스 테스트에서만 동작하여서,
일반적으로는 여러분의 메커니즘의 핵심을 수작업으로 추출해 내야만 함을
의미합니다.
Promela 와 \co{spin} 만큼, PPCMEM 과 \co{herd} 는 매우 유용합니다만, regression
suite 에 적합하지는 않습니다.

반면, \co{cbmc} 와 Nidhugg 는 합리적인 (여전히 매우 제한되어 있긴 하지만)
크기의 C 프로그램을 입력받을 수 있고, 그 기능이 계속 발전한다면, regression
test 에 훌륭한 추가물이 될 수도 있을 겁니다.

C 코드를 입력으로 받는 것의 한가지 단점은, 해당 컴파일러가 올바르다고
가정한다는 겁니다.
한가지 대안적인 접근법은 C 컴파일러가 만들어낸 바이너리를 입력으로 받음으로써,
관련된 컴파일러 버그들을 모두 파악해내는 겁니다.
이 방법은 여러 검증 시도에서 사용되었는데, SEL4
프로젝트~\cite{ThomasSewell2013L4binaryVerification} 가 특히 그랬습니다.
\iffalse

PPCMEM and \co{herd} can in theory directly input assembly language
and C++ code, but these tools work only on very small litmus tests,
which normally means that you must extract the core of your
mechanism---by hand.
As with Promela and \co{spin}, both PPCMEM and \co{herd} are
extremely useful, but they are not well-suited for regression suites.

In contrast, \co{cbmc} and Nidhugg can input C programs of reasonable
(though still quite limited) size, and if their capabilities continue
to grow, could well become excellent additions to regression suites.

One shortcoming of taking C code as input is that it assumes that the
compiler is correct.
An alternative approach is to take the binary produced by the C compiler
as input, thereby accounting for any relevant compiler bugs.
This approach has been used in a number of verification efforts,
perhaps most notably by the SEL4
project~\cite{ThomasSewell2013L4binaryVerification}.
\fi

\QuickQuiz{}
	SEL4 프로젝트에서 사용된 다양한 검증기들의 획기적인 성격이 있는데, 이
	챕터는 왜 이걸 더 깊게 다루지 않나요?
	\iffalse

	Given the groundbreaking nature of the various verifiers used
	in the SEL4 project, why doesn't this chapter cover them in
	more depth?
	\fi
\QuickQuizAnswer{
	SEL4 프로젝트에서 사용된 검증기들이 정말로 사용 가능한지에 대해서는
	의혹의 여지가 없습니다.
	하지만, SEL4 가 단일 CPU 프로젝트 외의 것들에 사용된 지는 (2017년에
	이르러) 2년밖에 되지 않았습니다.
	그리고 SEL4 가 멀티 프로세서 기능을 늘려가기 시작했지만, 리눅스 커널의
	기존의 Big Kernel Lock (BKL) 과 유사한, 매우 크게 락을 잡는 락킹 방식을
	사용하고 있습니다.
	SEL4 의 검증기를 병렬 프로그래밍 책에 더하는게 말이 되는 날도 올거라
	믿습니다만, 불행히도, 지금은 그때가 아닙니다.
	\iffalse

	There can be no doubt that the verifiers used by the SEL4
	project are quite capable.
	However, it has been only in the past couple of years
	(as of 2017) that SEL4 has been anything other than
	a single-CPU project.
	And although SEL4 is starting to gain multi-processor
	capabilities, it is currently using very coarse-grained
	locking that is similar to the Linux kernel's old
	Big Kernel Lock (BKL).
	There will hopefully come a day when it makes sense to add
	SEL4's verifiers to a book on parallel programming, but
	unfortunately, this is not yet that day.
	\fi
} \QuickQuizEnd

하지만, 소스나 바이너리로부터 직접 검증을 하는게 모두 사람의 변환 과정에서의
에러를 제거하는, 안정적인 regression test 에 매우 중요한 장점을 갖습니다.
\iffalse

However, verifying directly from either the source or binary both have the
advantage of eliminating human translation errors, which is critically
important for reliable regression testing.
\fi

\subsection{Environment}
\label{sec:future:Environment}

Formal-verification 도구들이 각자의 환경을 올바르게 모델링 하는 것은 매우
중요합니다.
모두에게 너무나 흔한 생략은 메모리 모델로, Promela/spin 을 포함한 매우 많은
formal-verification 도구들이 순차적 일관성에 국한되어 있습니다.
Section~\ref{sec:formal:Is QRCU Really Correct?}
에 연관된 QRCU 의 경험은 중요한 교훈적 이야기입니다.

Promela 와 \co{spin} 은 순차적 일관성을 가정하는데, 이는
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering} 에서 볼 수 있듯이
최근의 컴퓨터 시스템에서는 좋은 맞춤이 아닙니다.
대조적으로, PPCMEM 과 \co{herd} 의 큰 강점 중 하나는 x86, ARM, Power, 그리고,
\co{herd} 의 경우에는 리눅스 커널 v4.17 에서 받아들여진 리눅스 커널 메모리
모델~\cite{Alglave:2018:FSC:3173162.3177156} 을 포함해 다양한 CPU 제품군들의
메모리 모델에 대한 모델링입니다.
\iffalse

It is critically important that formal-verification tools correctly
model their environment.
One all-too-common omission is the memory model, where a great
many formal-verification tools, including Promela/spin, are
restricted to sequential consistency.
The QRCU experience related in
Section~\ref{sec:formal:Is QRCU Really Correct?}
is an important cautionary tale.

Promela and \co{spin} assume sequential consistency, which is not a
good match for modern computer systems, as was seen in
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering}.
In contrast, one of the great strengths of PPCMEM and \co{herd}
is their detailed modeling of various CPU families memory models,
including x86, ARM, Power, and, in the case of \co{herd},
even a Linux-kernel memory model~\cite{Alglave:2018:FSC:3173162.3177156},
which has been accepted into version 4.17 of
the Linux kernel.
\fi

\co{cbmc} 와 Nidhugg 도구들은 메모리 모델을 선택할 수 있는 기능을 일부
제공합니다만, PPCMEM 과 \co{herd} 만큼 다양한 기능은 아닙니다.
하지만, 시간이 흐름에 따라 더 커다란 규모의 도구들이 더 많은 메모리 모델을
받아들일 가능성이 있습니다.

장기적으로 보면, formal-verification 도구들이 I/O 를 포함하는게 도움이
될겁니다~\cite{PaulEMcKenney2016LinuxKernelMMIO}만, 그렇게 되기전에 많은 시간이
필요할 겁니다.
\iffalse

The \co{cbmc} and Nidhugg tools provide some ability to select
memory models, but do not provide the variety that PPCMEM and
\co{herd} do.
However, it is likely that the larger-scale tools will adopt
a greater variety of memory models as time goes on.

In the longer term, it would be helpful for formal-verification
tools to include I/O~\cite{PaulEMcKenney2016LinuxKernelMMIO},
but it may be some time before this comes to pass.
\fi

\subsection{Overhead}
\label{sec:future:Overhead}

거의 모든 하드코어 formal-verification 도구들이 기본적으로 기하급수적으로
증가하는 오버헤드를 가져서, 많은 흥미로운 소프트웨어 질문들은 실제로 논증
불능이란 걸 여러분이 고려하기 전까지는 사용하고 싶지 않게 보일 겁니다.
하지만, 그 기하급수적 단계 사이에도 그 정도에는 차이가 있습니다.

PPCMEM 은 설계적으로 최적화 되어 있지 않은데, 관심의 메모리 모델이 실제로
정확하게 표현되었음에 대한 확증을 주기 위해서입니다.
\co{herd} 는 더 적극적으로 최적화를 하며, 따라서
Section~\ref{sec:formal:Axiomatic Approaches} 에서 설명한대로, PPCMEM 보다 열배
이상 빠릅니다.
그러나, PPCMEM 과 \co{herd} 모두 커다란 코드보다는 매우 작은 리트머스 테스트를
목표로 삼습니다.
\iffalse

Almost all hard-core formal-verification tools are exponential
in nature, which might seem discouraging until you consider that
many of the most interesting software questions are in fact undecidable.
However, there are differences in degree, even among exponentials.

PPCMEM by design is unoptimized, in order to provide greater assurance
that the memory models of interest are in fact accurately represented.
The \co{herd} tool optimizes more aggressively, and so as described in
Section~\ref{sec:formal:Axiomatic Approaches}, is orders of magnitude
faster than PPCMEM.
Nevertheless, both PPCMEM and \co{herd} target very small litmus tests
rather than larger bodies of code.
\fi

반면에, Promela/\co{spin}, \co{cbmc}, 그리고 Nidhugg 는 더 커다란 코드 (무언가)
를 위해 설계되었습니다.
Promela/\co{spin} 은 Curiosity rover 의
파일시스템~\cite{DBLP:journals/amai/GroceHHJX14} 을 검증하기 위해 사용되었고,
앞에서도 이야기되었듯 \co{cbmc} 와 Nidhugg 는 모두 리눅스 커널 RCU 에
적용되었습니다.

휴리스틱의 발전이 지난 25년간의 속도로 지속된다면, 우리는 formal verification
의 오버헤드의 많은 감소를 예상할 수 있습니다.
그렇다고는 해도, 오버헤드의 조합적인 증가는 여전히 조합적 증가인데, 이는
휴리스틱의 개선이 지속되든 안되든 검증될 수 있는 프로그램의 크기는 여전히 대폭
제한할 겁니다.
\iffalse

In contrast, Promela/\co{spin}, \co{cbmc}, and Nidhugg are designed for
(somewhat) larger bodies of code.
Promela/\co{spin} was used to verify the Curiosity rover's
filesystem~\cite{DBLP:journals/amai/GroceHHJX14} and, as noted earlier,
both \co{cbmc} and Nidhugg were appled to Linux-kernel RCU.

If advances in heuristics continue at the rate of the past quarter
century, we can look forward to large reductions in overhead for
formal verification.
That said, combinatorial explosion is still combinatorial explosion,
which would be expected to sharply limit the size of programs that
could be verified, with or without continued improvements in
heuristics.
\fi

하지만, 조합적 폭증의 반대면은 Macedon 의 Philip II 의 영원한 충고입니다:
``분할하고 지배하라.''
커다란 프로그램이 분할될 수 있고 그 조각들이 검증될 수 있다면, 그 결과는 조합적
\emph{폭발}~\cite{PaulEMcKenney2011Verico} 일 겁니다.
분할을 할 자연스러운 장소는 API 경계인데, 예를 들어, 락킹 기능들의 그것입니다.
그러면 하나의 검증 패스는 락킹 구현이 올바른지 검증하고, 추가적인 검증 패스들은
락킹 API 들의 올바른 사용을 검증할 수 있을 겁니다.
\iffalse

However, the flip side of combinatorial explosion is Philip II of
Macedon's timeless advice: ``Divide and rule.''
If a large program can be divided and the pieces verified, the result
can be combinatorial \emph{implosion}~\cite{PaulEMcKenney2011Verico}.
One natural place to divide is on API boundaries, for example, those
of locking primitives.
One verification pass can then verify that the locking implementation
is correct, and additional verification passes can verify correct
use of the locking APIs.
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}[\LstLineNo]
C C-SB+l-o-o-u+l-o-o-u-C

{
}

P0(int *sl, int *x0, int *x1)
{
  int r2;
  int r1;

  r2 = cmpxchg_acquire(sl, 0, 1);
  WRITE_ONCE(*x0, 1);
  r1 = READ_ONCE(*x1);
  smp_store_release(sl, 0);
}

P1(int *sl, int *x0, int *x1)
{
  int r2;
  int r1;

  r2 = cmpxchg_acquire(sl, 0, 1);
  WRITE_ONCE(*x1, 1);
  r1 = READ_ONCE(*x0);
  smp_store_release(sl, 0);
}

filter (0:r2=0 /\ 1:r2=0)
exists (0:r1=0 /\ 1:r1=0)
\end{verbbox}
}
\centering
\theverbbox
\caption{Emulating Locking with \tco{cmpxchg_acquire()}}
\label{lst:future:Emulating Locking with cmpxchg}
\end{listing}

\begin{table}[tbh]
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]}
	\toprule
	\multicolumn{1}{c}{\# Threads} & \multicolumn{1}{c}{Locking} &
			\multicolumn{1}{c}{\tco{cmpxchg_acquire}} \\
	\midrule
	2 & 0.004 &  0.022 \\
	3 & 0.041 &  0.743 \\
	4 & 0.374 & 59.565 \\
	5 & 4.905 &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance (s)}
\label{tab:future:Emulating Locking: Performance (s)}
\end{table}

이 접근법의 성능 이득은 리눅스 커널 메모리
모델~\cite{JadeAlglave2017LWN-LKMM-1,JadeAlglave2017LWN-LKMM-2} 을 통해 보여질
수 있습니다.
이 모델은 \co{spin_lock()} 과 \co{spin_unlock()} 기능을 제공하지만, 이 기능들은
또한
Listing~\ref{lst:future:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} 와 \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus})
에 보인대로 \co{cmpxchg_acquire()} 와 \co{smp_store_release()} 를 사용해서도
에뮬레이션 될 수 있습니다.
Table~\ref{tab:future:Emulating Locking: Performance (s)}
은 이 모델의 \co{spin_lock()} 과 \co{spin_unlock()} 을 사용할 때의 성능과
확장성을 이 기능들을 앞의 리스트에 보인대로 에뮬레이션 했을 때와 비교합니다.
이 차이는 무의미하지 않습니다: 네개 프로세스에서, 이 모델은 에뮬레이션보다 백배
이상 빠릅니다!
\iffalse

The performance benefits of this approach can be demonstrated using
the Linux-kernel memory
model~\cite{Alglave:2018:FSC:3173162.3177156}.
This model provides \co{spin_lock()} and \co{spin_unlock()}
primitives, but these primitives can also be emulated using
\co{cmpxchg_acquire()} and \co{smp_store_release()}, as shown in
Listing~\ref{lst:future:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} and \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus}).
Table~\ref{tab:future:Emulating Locking: Performance (s)}
compares the performance and scalability of using the model's
\co{spin_lock()} and \co{spin_unlock()} against emulating these
primitives as shown in the listing.
The difference is not insignificant: At four processes, the model
is more than two orders of magnitude faster than emulation!
\fi

\QuickQuiz{}
	Listing~\ref{lst:future:Emulating Locking with cmpxchg}
	의 line~26 에서 해당 컨디션을 그냥 \co{exists} 절에 넣는 대신 왜 별개의
	\co{filter} 커맨드를  사용하나요?
	그리고 \co{cmpxchg_acquire()} 대신 \co{xchg_acquire()} 를 사용하는게 더
	간단하지 않겠어요?
	\iffalse

	Why bother with a separate \co{filter} command on line~28 of
	Listing~\ref{lst:future:Emulating Locking with cmpxchg}
	instead of just adding the condition to the \co{exists} clause?
	And wouldn't it be simpler to use \co{xchg_acquire()} instead
	of \co{cmpxchg_acquire()}?
	\fi
\QuickQuizAnswer{
	이 \co{filter} 절은 \co{herd} 툴이 \co{exists} 절보다 더 이른 처리
	단계에서 수행을 멈추게 해주는데, 이는 상당한 속도 향상을 가져다 줍니다.
	\iffalse

	The \co{filter} clause causes the \co{herd} tool to discard
	executions at an earlier stage of processing than does
	the \co{exists} clause, which provides significant speedups.
	\fi

\begin{table}[tbh]
\rowcolors{7}{lightgray}{}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]
		S[table-format=3.3]S[table-format=2.3]S[table-format=3.3]}
	\toprule
	& & \multicolumn{2}{c}{\tco{cmpxchg_acquire()}}
		& \multicolumn{2}{c}{\tco{xchg_acquire()}} \\
	\cmidrule(l){3-4} \cmidrule(l){5-6}
	\multicolumn{1}{c}{\#} & \multicolumn{1}{c}{Lock}
		& \multicolumn{1}{c}{\tco{filter}}
			& \multicolumn{1}{c}{\tco{exists}}
				& \multicolumn{1}{c}{\tco{filter}}
					& \multicolumn{1}{c}{\tco{exists}} \\
	\cmidrule{1-1} \cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-6}
	2 & 0.004 &  0.022 &   0.039 &  0.027 &  0.058 \\
	3 & 0.041 &  0.743 &   1.653 &  0.968 &  3.203 \\
	4 & 0.374 & 59.565 & 151.962 & 74.818 & 500.96 \\
	5 & 4.905 &        &         &        &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance Comparison (s)}
\label{tab:future:Emulating Locking: Performance Comparison (s)}
\end{table}

	\co{xchg_acquire()} 어토믹 오퍼레이션은 락 획득이 성공하든 실패하든
	쓰기를 하게 되는데, 이는 \co{xchg_acquire()} 를 사용하는 모델은 락 획득
	실패의 경우에는 쓰기를 하지 않을 \co{cmpxchg_acquire()} 를 사용하는
	모델보다 많은 오퍼레이션을 갖게 될 것을 의미합니다.
	더 많은 쓰기는
	Table~\ref{tab:future:Emulating Locking: Performance Comparison (s)}
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, 그리고
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}) 에서 보이는 것과 같이 더 많은
	조합의 폭증을 의미합니다.
	이 테이블은 \co{cmpxchg_acquire()} 가 \co{xchg_acquire()} 보다 성능이
	높음을 , 그리고 \co{filter} 절의 사용이 \co{exists} 절의 사용보다
	성능이 높음을 보입니다.
	\iffalse

	As for \co{xchg_acquire()}, this atomic operation will do a
	write whether or not lock acquisition succeeds, which means
	that a model using \co{xchg_acquire()} will have more operations
	than one using \co{cmpxchg_acquire()}, which won't do a write
	in the failed-acquisition case.
	More writes means more combinatorial to explode, as shown in
	Table~\ref{tab:future:Emulating Locking: Performance Comparison (s)}
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, and
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}).
	This table clearly shows that \co{cmpxchg_acquire()}
	outperforms \co{xchg_acquire()} and that use of the
	\co{filter} clause outperforms use of the \co{exists} clause.
	\fi
} \QuickQuizEnd

도구들이 자동으로 커다란 프로그램들을 쪼개고, 그 조각들을 검증하고, 그 조각들의
조합을 검증한다면 물론 매우 유용할 겁니다.
그렇게 되기 전까지는, 커다란 프로그램의 검증은 상당한 수작업을 필요로 할
겁니다.
이 작업은 매 릴리즈마다 반복된 검증을 안정적으로 해내기에 낫고 결국 continuous
integration 에 잘 맞는 방식인 스크립팅으로 중개를 하는게 바람직할겁니다.

어떤 경우든, 우린 formal-verification 기능이 시간에 따라 증가하길 계속할 것을
기대할 수 있습니다.
\iffalse

It would of course be quite useful for tools to automatically divide
up large programs, verify the pieces, and then verify the combinations
of pieces.
In the meantime, verification of large programs will require significant
manual intervention.
This intervention will preferably mediated by scripting, the better to
reliably carry out repeated verifications on each release, and
preferably eventually in a manner well-suited for continuous integration.

In any case, we can expect formal-verification capabilities to continue
to increase over time.
\fi

\subsection{Locate Bugs}
\label{sec:future:Locate Bugs}

어떤 크기의 소프트웨어 제품이든 모두 버그를 갖고 있습니다.
따라서, 버그의 존재 여부만 알려주는 formal-verification 도구는 딱히 유용하지
않습니다.
필요한건 최소한 버그가 어디에 있고 그 버그의 특성이 무엇인지에 대한 \emph{어떤}
정보를 제공해 주는 도구입니다.

\co{cbmc} 의 출력은 소스 코드로 매핑되는 기록을 포함하며, Promela/spin 의 것과
유사하고, Nidhugg 도 그러합니다.
물론, 이런 기록은 매우 길수도 있고, 그걸 분석하는건 상당히 지루할 수 있습니다.
하지만, 그러는게 과거의 방식으로 버그를 찾는것보다는 일반적으로 훨씬 빠르고
즐겁습니다.
\iffalse

Any software artifact of any size contains bugs.
Therefore, a formal-verification tool that reports only the
presence or absence of bugs is not particularly useful.
What is needed is a tool that gives at least \emph{some} information
as to where the bug is located and the nature of that bug.

The \co{cbmc} output includes a traceback mapping back to the source
code, similar to Promela/spin's, as does Nidhugg.
Of course, these tracebacks can be quite long, and analyzing them
can be quite tedious.
However, doing so is usually quite a bit faster
and more pleasant than locating bugs the old-fashioned way.
\fi

또한, 가장 간단한 formal-verification 도구들의 테스트 중 하나는 버그
추가입니다.
무엇보다도, 우리 중 누구도 \co{printf("VERIFIED\\n")} 을 쓸 수는 없습니다만,
평범한 사실 하나는 formal-verification 도구의 개발자들은 우리들만큼이나 버그를
만들기 쉽다는 것입니다.
\iffalse

In addition, one of the simplest tests of formal-verification tools is
bug injection.
After all, not only could any of us write
\co{printf("VERIFIED\\n")}, but the plain fact is that
developers of formal-verification tools are just as bug-prone as
are the rest of us.
\fi

\subsection{Minimal Scaffolding}
\label{sec:future:Minimal Scaffolding}

과거, formal-verification 연구자들은 어떤 소프트웨어가 검증될 것인지에 대한
완전한 명세를 원했습니다.
불행히도, 수학적으로 엄격한 명세는 실제 코드보다도 클 수 있고, 명세의 각 줄은
코드의 각 줄이 그런것처럼 버그를 포함하고 있을 수 있습니다.
코드가 해당 명세를 착실히 구현하고 있는지를 증명하는 formal verification 노력은
둘 사이의 버그를 위한 버그의 호환성에 대한 증명이 될 수 있는데, 이는 의도된
결과가 아닙니다.

이게 다가 아니라, 리눅스 커널 RCU 를 포함한 여러 소프트웨어 제품들을 위한
요구사항은 근본적으로
경험적입니다~\cite{PaulEMcKenney2015RCUreqts1,PaulEMcKenney2015RCUreqts2,PaulEMcKenney2015RCUreqts3}.\footnote{
	또는, formal-verification 용어로, 리눅스 커널 RCU 는 \emph{불완전한
	명세}를 가지고 있습니다.}
이런 흔한 종류의 소프트웨어에게, 완전한 명세는 환상입니다.
하드웨어에게 완전한 명세는 더이상 환상만이 아닌데, 2017년 말의 Meltdown 과
Spectre side-channel 공격~\cite{JannHorn2018MeltdownSpectre} 을 통해
분명해졌습니다.
\iffalse

In the old days, formal-verification researchers demanded a full
specification against which the software would be verified.
Unfortunately, a mathematically rigorous specification might well
be larger than the actual code, and each line of specification
is just as likely to contain bugs as is each line of code.
A formal verification effort proving that the code faithfully
implemented the specification would be a proof of bug-for-bug
compatibility between the two, which might not be the intended
result.

Worse yet, the requirements for a number of software artifacts,
including Linux-kernel RCU, are empirical in
nature~\cite{PaulEMcKenney2015RCUreqts1,PaulEMcKenney2015RCUreqts2,PaulEMcKenney2015RCUreqts3}.\footnote{
	Or, in formal-verification parlance, Linux-kernel RCU has an
	\emph{incomplete specification}.}
For this common type of software, a complete specification is a
polite fiction.
Nor are complete specifications any less fictional for hardware,
as was made clear by the late 2017 advent of the Meltdown and Spectre
side-channel attacks~\cite{JannHorn2018MeltdownSpectre}.
\fi

이런 상황은 사람들이 실제 세상의 소프트웨어와 하드웨어 제품에 대한 formal
verification 의 희망을 포기하게 할 수도 있지만, 명세될 수 있는게 꽤 있다는 게
드러났습니다.
예를 들어, 설계와 코딩 규칙, 코드내에 포함된 단정문은 부분적 명세로 동작될 수
있습니다.
그리고 실제로 \co{cbmc} 와 Nidhugg 와 같은 formal-verification 도구들은 발동될
수 있는 단정문들을 검사하는데, 암묵적으로 이 단정문들을 명세의 부분으로
취급하는 것입니다.
하지만, 이 단정문들은 또한 코드의 부분인데, 이는 이 단정문들이 폐용되지 않을
것이며, 특히 이 코드가 스트레스 테스트를 위한 것이라면 그러할
것입니다.\footnote{
	그리고 여러분은 여러분의 코드에 대해 스트레스 테스트를 \emph{하죠},
	그렇죠?}
\co{cbmc} 도구는 또한 배열 범위를 벗어난 레퍼런스를 검사하는데, 이 역시
암묵적으로 이 명세에 추가하는 것입니다.
\iffalse

This situation might cause one to give up all hope of formal verification
of real-world software and hardware artifacts, but it turns out that there is
quite a bit that can be done.
For example, design and coding rules can act as a partial specification,
as can assertions contained in the code.
And in fact formal-verification tools such as \co{cbmc} and Nidhugg
both check for assertions that can be triggered, implicitly treating
these assertions as part of the specification.
However, the assertions are also part of the code, which makes it less
likely that they will become obsolete, especially if the code is
also subjected to stress tests.\footnote{
	And you \emph{do} stress-test your code, don't you?}
The \co{cbmc} tool also checks for array-out-of-bound references,
thus implicitly adding them to the specification.
\fi

이 암묵적 명세 접근법은 상당히 말이 되는데, 여러분이 formal verification 을
올바름에 대한 완전한 증명으로 보기보다는 일반적으로 사용되는, 테스트와 다른
강점과 약점을 가진 하나의 대안적 형태의 검증이라고 생각하고 있다면 특히
그렇습니다.
이 관점에서, 소프트웨어는 항상 버그를 가지고 있고, 따라서 이런 버그를 찾아주는
모든 종류의 도구는 실제로 매우 좋은 물건입니다.
\iffalse

This implicit-specification approach makes quite a bit of sense, particularly
if you look at formal verification not as a full proof of correctness,
but rather an alternative form of validation with a different set of
strengths and weaknesses than the common case, that is, testing.
From this viewpoint, software will always have bugs, and therefore any
tool of any kind that helps to find those bugs is a very good thing
indeed.
\fi

\subsection{Relevant Bugs}
\label{sec:future:Relevant Bugs}

버그를 찾는것---그리고 그걸 고치는것---은 물론 모든 검증의 완전한 목적입니다.
분명히, 거짓 양성반응은 없어야 합니다.
하지만 거짓 양성반응이 없다 해도, 버그는 존재하고 존재합니다.

예를 들어, 어떤 소프트웨어 제품이 정확히 100개의 남아있는 버그가 있고, 그 버그
각각은 평균적으로 백만년의 수행시간에 한번 나타난다고 가정해 봅시다.
더 나아가서 어떤 전지전능한 formal-verification 도구가 모든 100개의 버그를
찾아내서 개발자가 이를 수정했다고 해봅시다.
이 소프틑웨어 제품의 안정성에는 어떤 일이 벌어질까요?

아마도 그 놀랄만한 답은 안정성이 \emph{하락한다}는 것일 겁니다.
\iffalse

Finding bugs---and fixing them---is of course the whole point of any
type of validation effort.
Clearly, false positives are to be avoided.
But even in the absense of false positives, there are bugs and there are bugs.

For example, suppose that a software artifact had exactly 100 remaining
bugs, each of which manifested on average once every million years
of runtime.
Suppose further that an omniscient formal-verification tool located
all 100 bugs, which the developers duly fixed.
What happens to the reliability of this software artifact?

The perhaps surprising answer is that the reliability \emph{decreases}.
\fi

이를 이해하기 위해, 약 7\,\% 의 수정은 새로운 버그를 만들어냈다는 역사적
경험~\cite{RexBlack2012SQA} 을 명심하기 바랍니다.
따라서, 발생하기까지 조합된 평균 시간 (MTBF: Mean Time Between Failure) 10,000
년을 갖는 100 개의 버그를 고치는 것은 7개의 추가적 버그를 만들어 냅니다.
역사적 통계는 각각의 이 새로운 버그가 70,000 년보다 훨씬 적은 MTBF 를 가질 것을
이야기 합니다.
이는 결국 이 7개의 새로운 버그의 조합된 MTBF 가 10,000 년보다 훨씬 적을 것을
이야기하는데, 이는 결국 원래의 100개 버그의 잘 의도된 수정은 전체 소프트웨어의
안정성을 실제로 하락시켰음을 의미합니다.
\iffalse

To see this, keep in mind that historical experience indicates that
about 7\,\% of fixes introduce a new bug~\cite{RexBlack2012SQA}.
Therefore, fixing the 100 bugs, which had a combined mean time to failure
(MTBF) of about 10,000 years, will introduce seven more bugs.
Historical statistics indicate that each new bug will have an MTBF
much less than 70,000 years.
This in turn suggests that the combined MTBF of these seven new bugs
will most likely be much less than 10,000 years, which in turn means
that the well-intentioned fixing of the original 100 bugs actually
decreased the reliability of the overall software.
\fi

\QuickQuiz{}
	알려진 버그들의 MTBF 들이 아직 발견되지 않은 버그들의 MTBF 에 대한 좋은
	추측이 될 수 있음을 어떻게 아나요?
	\iffalse

	How do we know that the MTBFs of known bugs is a good estimate
	of the MTBFs of bugs that have not yet been located?
	\fi
\QuickQuizAnswer{
	모릅니다만, 그건 문제가 되지 않습니다.

	이를 이해하기 위해, 7\,\% 예측은 차후에 발견되는, 추가된 버그들에만
	적용됨을 알아두시기 바랍니다: 발견되지 않은, 추가되 버그들은
	무시됩니다.
	따라서, 알려진 버그들의 MTBF 통계는 차후에 발견되는, 추가된 버그들에
	대한 좋은 예측이 될 수 있습니다.

	이 섹션 전체의 핵심은, 우리가 실제로 발견되지 않은 버그들보다
	사용자들을 괴롭히는 버그들에 더 주의를 기울여야 한다는 것입니다.
	이는 물론 우리가 아직 사용자들을 괴롭히지 않은 버그들은 무시해도 된다고
	말하려는건 \emph{아니고}, 다만 우리가 가장 중요하고 급한 버그들을 먼저
	고치는데 우리의 노력의 우선순위를 둬야 한다는 이야기입니다.
	\iffalse

	We don't, but it does not matter.

	To see this, note that the 7\,\% figure only applies to injected
	bugs that were subsequently located: It necessarily ignores
	any injected bugs that were never found.
	Therefore, the MTBF statistics of known bugs is likely to be
	a good approximation of that of the injected bugs that are
	subsequently located.

	A key point in this whole section is that we should be more
	concerned about bugs that inconvenience users than about
	other bugs that never actually manifest.
	This of course is \emph{not} to say that we should completely
	ignore bugs that have not yet inconvenienced users, just that
	we should properly prioritize our efforts so as to fix the
	most important and urgent bugs first.
	\fi
} \QuickQuizEnd

\QuickQuiz{}
	하지만 formal-verification 도구들은 이 수정으로 만들어지는 모든
	버그들을 곧바로 발견해 낼텐데, 이게 왜 문제죠?
	\iffalse

	But the formal-verification tools should immediately find all the
	bugs introduced by the fixes, so why is this a problem?
	\fi
\QuickQuizAnswer{
	실제 세계의 formal-verification 도구들은 (formal verification 의 더
	떠들썩한 지지자들의 상상 속에서만 존재하는 것들과 반대로) 전지전능하지
	않고, 따라서 특정한 종류의 버그들만 찾아낼 수 있기 때문에 문제가
	됩니다.
	하나만 예를 들어 보면, formal-verification 도구들은 누락된 단정문이나,
	동등하게, 명세에서 누락된 부분에 연관된 버그들은 발견하지 못할 겁니다.
	\iffalse

	It is a problem because real-world formal-verification tools
	(as opposed to those that exist only in the imaginations of
	the more vociferous proponents of formal verification) are
	not omniscient, and thus are only able to locate certain types
	of bugs.
	For but one example, formal-verification tools are unlikely to
	spot a bug corresponding to an omitted assertion or, equivalently,
	a bug corresponding to an omitted portion of the specification.
	\fi
} \QuickQuizEnd

아직 끝이 아닌게, 평균적으로 하루에 한번 발견되는 버그와 매 100만년마다
발견되는 99개의 버그를 갖는 소프트웨어 제품을 생각해 봅시다.
어떤 formal-verification 도구가 이 99개의 백만년짜리 버그를 발견하지만,
하루짜리 버그는 발견하지 못한다고 생각해 봅시다.
이 99개의 버그를 고치는 것은 시간과 노력을 필요로 할텐데, 약간 안정성을
떨어뜨릴 테고, 훨씬 부끄럽고 훨씬 나쁠, 매일 발견되는 버그에 대해서는 아무일도
하지 않게 될겁니다.

따라서, 대부분의 문제가 되는 버그들을 우선적으로 찾아내는 검증 도구를 갖는게
좋을 겁니다.

이에 대해서는 물어보고 싶은게 많겠지만, 이는 우리가 정말로 소프트웨어 안정성을
개선하고자 한다면 필요한 것입니다.
\iffalse

Worse yet, imagine another software artifact with one bug that fails
once every day on average and 99 more that fail every million years
each.
Suppose that a formal-verification tool located the 99 million-year
bugs, but failed to find the one-day bug.
Fixing the 99 bugs located will take time and effort, likely slightly
decrease reliability, and do nothing at all about the pressing
each-day failure that is likely causing much embarrassment and perhaps
much worse besides.

Therefore, it would be best to have a validation tool that
preferentially located the most troublesome bugs.

This might sound like too much to ask, but it is what is really
required if we are to actually increase software reliability.
\fi

\subsection{Formal Regression Scorecard}
\label{sec:future:Formal Regression Scorecard}

\begin{table*}[tbh]
% \rowcolors{6}{}{lightgray}
%\renewcommand*{\arraystretch}{1.1}
\small
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccccccccc}
	\toprule
	& & Promela & & PPCMEM & & \tco{herd} & & \tco{cbmc} & & Nidhugg \\
	\midrule
	(1) Automated &
		& \cellcolor{red!50} &
			& \cellcolor{orange!50} &
				& \cellcolor{orange!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(2) Environment &
		& \cellcolor{red!50} (MM) &
			& \cellcolor{green!50} &
				& \cellcolor{blue!50} &
					& \cellcolor{yellow!50} (MM) &
						& \cellcolor{orange!50} (MM) \\
	\addlinespace[3pt]
	(3) Overhead &
		& \cellcolor{yellow!50} &
			& \cellcolor{red!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{yellow!50} (SAT) &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(4) Locate Bugs &
		& \cellcolor{yellow!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{green!50} &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(5) Minimal Scaffolding &
		& \cellcolor{green!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(6) Relevant Bugs &
		& \cellcolor{yellow!50} ??? &
			& \cellcolor{yellow!50} ??? &
				& \cellcolor{yellow!50} ??? &
					& \cellcolor{yellow!50} ??? &
						& \cellcolor{yellow!50} ??? \\
	\bottomrule
\end{tabular}
\caption{Formal Regression Scorecard}
\label{tab:future:Formal Regression Scorecard}
\end{table*}

Table~\ref{tab:future:Formal Regression Scorecard}
는 이 챕터에서 다룬 formal-verification 도구들의 대략적인 점수를 보입니다.
더 짧은 파장이 긴 파장보다 나은 것입니다.

Promela 는 수작업 변환을 필요로 하고 sequential consistency 만을 지원하므로,
이것의 처음 두개 셀은 빨갛습니다.
이 도구는 합리적인 오버헤드 (formal verification 에 대해서는요, 어쨌든) 를 갖고
traceback 기능을 제공하므로, 다음 두개 셀은 노란색입니다.
수작업 변환을 필요로 하긴 하나, Promela 는 단정문을 자연적인 방법으로 처리하며,
따라서 다섯번째 셀은 초록색입니다.
\iffalse

Table~\ref{tab:future:Formal Regression Scorecard}
shows a rough-and-ready scorecard for the formal-verification tools
covered in this chapter.
Shorter wavelengths are better than longer wavelengths.

Promela requires hand translation and supports only sequential
consistency, so its first two cells are red.
It has reasonable overhead (for formal verification, anyway)
and provides a traceback, so its next two cells are yellow.
Despite requiring hand translation, Promela handles assertions
in a natural way, so its fifth cell is green.
\fi

PPCMEM 은 지원하는 리트머스 테스트의 작은 크기 때문에 일반적으로 수작업을
필요로 하며, 따라서 첫번째 셀은 오렌지색입니다.
이 도구의 오버헤드는 상당히 높아서, 세번째 셀은 빨간색입니다.
이 도구는 오퍼레이션들간의 관계를 그림으로 보여주는데, 이는 traceback 만큼이나
유용하면서 상당히 유용하므로, 네번째 셀은 노란색입니다.
이 도구는 \co{exists} 절을 만들 것을 필요로 하고 프로세스간 단정문은 받지
못하기 때문에, 그 다섯번째 셀 역시 노란색입니다.

\co{herd} 도구는 PPCMEM 과 유사하게 검증되는 코드의 크기에 제한이 있으며,
따라서 \co{herd} 의 첫번째 셀은 오렌지색입니다.
이 도구는 다양한 종류의 메모리 모델을 지원하므로, 두번째 셀은 파란색입니다.
합리적인 오버헤드를 가지고 있으므로, 세번째 셀은 노란색입니다.
버그 파악과 단정문 기능은 PPCMEM 의 그것과 상당히 유사해서, \co{herd} 또한 그
다음 두개 셀은 노란색입니다.
\iffalse

PPCMEM usually requires hand translation due to the small size of litmus
tests that it supports, so its first cell is orange.
It handles several memory models, so its second cell is green.
Its overhead is quite high, so its third cell is red.
It provides a graphical display of relations among operations, which
is not as helpful as a traceback, but is still quite useful, so its
fourth cell is yellow.
It requires constructing an \co{exists} clause and cannot take
intra-process assertions, so its fifth cell is also yellow.

The \co{herd} tool has size restrictions similar to those of PPCMEM,
so \co{herd}'s first cell is also orange.
It supports a wide variety of memory models, so its second cell is blue.
It has reasonable overhead, so its third cell is yellow.
Its bug-location and assertion capabilities are quite similar to those
of PPCMEM, so \co{herd} also gets yellow for the next two cells.
\fi

\co{cbmc} 도구는 C 코드를 직접 입력받고, 따라서 첫번째 셀은 파란색입니다.
이 도구는 몇가지 메모리 모델을 지원하며, 따라서 두번째 셀은 노란색입니다.
합리적인 오버헤드를 가지므로, 세번째 셀은 노란색입니다만, SAT-solver 성능은
계속해서 증가할 수 있습니다.
이 도구는 traceback 을 제공하므로, 네번째 셀은 초록색입니다.
이 도구는 C 코드로부터 단정문을 곧바로 가져오므로, 다섯번째 셀은 파란색입니다.

Nidhugg 또한 C 코드를 직접 입력받으므로 첫번째 셀은 파란색입니다.
이 도구는 두가지 메모리 모델만을 지원하므로, 두번째 셀은 오렌지색입니다.
오버헤드는 상당히 낮으므로 (formal-verification 에 대해서), 세번째 셀은 초록색입니다.
Traceback 을 지원하므로, 네번째 셀은 초록색입니다.
단정문을 C 코드로부터 곧바로 받으므로, 다섯번째 셀은 파란색입니다.

그런데 여섯번째 마지막 줄은 뭐죠?
이 중 어떤 도구가 올바른 버그를 찾는지 이야기하기엔 너무 이르므로, 이것들은
모두 물음표와 함께 노란색으로 칠해져 있습니다.
\iffalse

The \co{cbmc} tool inputs C code directly, so its first cell is blue.
It supports a few memory models, so its second cell is yellow.
It has reasonable overhead, so its third cell is also yellow, however,
perhaps SAT-solver performance will continue improving.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

Nidhugg also inputs C code directly, so its first cell is also blue.
It supports only a couple of memory models, so its second cell is orange.
Its overhead is quite low (for formal-verification), so its
third cell is green.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

So what about the sixth and final row?
It is too early to tell how any of the tools do at finding the right bugs,
so they are all yellow with question marks.
\fi

\QuickQuiz{}
	Table~\ref{tab:future:Formal Regression Scorecard}
	에 보인 점수표에 테스트는 어떻게 올라갈까요?
	\iffalse

	How would testing stack up in the scorecard shown in
	Table~\ref{tab:future:Formal Regression Scorecard}?
	\fi
\QuickQuizAnswer{
	테스트는 모두 파란 셀을 가질 겁니다만, 세번째 줄 (오버헤드) 는 불가능한
	버그를 찾을 때의 테스트의 어려움에 따라서 다른 색이 될수도 있는 예외가
	있습니다.

	달리 말해서, 불가능한 버그들은 또한 부적절한 버그이기도 해서, 여러분의
	마일리지는 매우 다를 수 있을 겁니다.
	\iffalse

	It would be blue all the way down, with the possible
	exception of the third row (overhead) which might well
	be marked down for testing's difficulty finding
	improbable bugs.

	On the other hand, improbable bugs are often also
	irrelevant bugs, so your mileage may vary.
	\fi

	많은 부분이 여러분의 설치 환경 크기에 의존적입니다.
	여러분의 코드가 (대략) 10,000 개 시스템에서만 돌아간다면, Murphy 는
	매우 친절한 사람일 수도 있습니다.
	모든것이 잘못될 수 있고, 그럴 겁니다.
	결국은요.
	어쩌면 지질 시대적 시간 안에 말입니다.

	하지만 여러분의 코드가 2017년 말의 리눅스 커널이 그렇다 이야기되었듯
	200억개의 시스템에서 돌아가고 있다면, Murphy 는 진짜 짜증나는 놈일 수
	있어요!
	모든것은 잘못될 수 있고, 그럴 것이며, 매우 빨리 잘못될 수 있습니다!!!
	\iffalse

	Much depends on the size of your installed base.
	If your code is only ever going to run on (say) 10,000
	systems, Murphy can actually be a really nice guy.
	Everything that can go wrong, will.
	Eventually.
	Perhaps in geologic time.

	But if your code is running on 20~billion systems,
	like the Linux kernel was said to in late 2017,
	Murphy can be a real jerk!
	Everything that can go wrong, will, and it can go wrong
	really quickly!!!
	\fi
} \QuickQuizEnd

\QuickQuiz{}
	하지만 Table~\ref{tab:future:Formal Regression Scorecard} 에 보인 것들
	외에도 상당히 많은 formal-verification 시스템들이 존재하지 않나요?
	\iffalse

	But aren't there a great many more formal-verification systems
	than are shown in
	Table~\ref{tab:future:Formal Regression Scorecard}?
	\fi
\QuickQuizAnswer{
	실제로 그렇습니다!
	이 표는 Paul 이 사용해본 것들에 주목하고 있지만, 다른 것들도 유용하다고
	알려져 있습니다.
	Formal verification 은 seL4
	프로젝트~\cite{ThomasSewell2013L4binaryVerification} 에서 많이
	사용되었고, 그 도구들은 이제 동시성의 너무 크지 않은 단계들을 다룰 수
	있습니다.
	더 최근에, Catalin Marinas 는 리눅스 커널의 queued spinlock 구현상의
	일부 forward-progress 버그들을 찾아내는데에 Lamport 의 TLA
	도구~\cite{Lamport:2002:SST:579617} 를 사용했습니다.
	Will Deacon 은 이 버그들을 고쳤고~\cite{WillDeacon2018qspinlock},
	Catalin 은 Will 의 수정을
	검증했습니다~\cite{CatalinMarinas2018qspinlockTLA}.
	\iffalse

	Indeed there are!
	This table focuses on those that Paul has used, but others are
	proving to be useful.
	Formal verification has been heavily used in the seL4
	project~\cite{ThomasSewell2013L4binaryVerification},
	and its tools can now handle modest levels of concurrency.
	More recently, Catalin Marinas used Lamport's
	TLA tool~\cite{Lamport:2002:SST:579617} to locate some
	forward-progress bugs in the Linux kernel's queued spinlock
	implementation.
	Will Deacon fixes these bugs~\cite{WillDeacon2018qspinlock},
	and Catalin verified Will's
	fixes~\cite{CatalinMarinas2018qspinlockTLA}.
	\fi
} \QuickQuizEnd

다시 말하건대, 이 표는 이 도구들을 regression testing 에 사용할 경우에 대한
채점임을 알아두시기 바랍니다.
이것들중 많은 것들이 regression testing 에 전혀 적합하지 않다는 것은 이게
쓸모없음을 의미하지는 않으며, 사실, 이것들 중 많은 것들이 지난 시간동안
가치있음을 증명했습니다.\footnote{
	다만 한가지 예를 들면, Promela 는 다른 것도 아니고 Curiosity Rover 의
	파일 시스템을 검증하는데 사용되었습니다.
	\emph{여러분의} formal verification 도구는 Mars 탐사선에 사용되었나요?}
단지 regression testing 을 위한 건 아니란 거죠.

하지만, 이는 바뀔 수 있습니다.
무엇보다, formal verification 도구들은 2010년대에 인상적인 활보들을 보였습니다.
이 발전이 지속된다면, formal verification 은 병렬 프로그래머의 검증 도구상자에
없을 수 없는 도구가 될 수도 있을 겁니다.
\iffalse

Once again, please note that this table rates these tools for use in
regression testing.
Just because many of them are poor fit for regression testing does
not at all mean that they are useless, in fact,
many of them have proven their worth many times over.\footnote{
	For but one example, Promela was used to verify the file system
	of none other than the Curiosity Rover.
	Was \emph{your} formal verification tool used on a Mars rover?}
Just not for regression testing.

However, this might well change.
After all, formal verification tools made impressive strides in the 2010s.
If that progress continues, formal verification might well become an
indespensible tool in the parallel programmer's validation toolbox.
\fi
