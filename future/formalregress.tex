% future/formalregress.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\section{Formal Regression Testing?}
\label{sec:future:Formal Regression Testing?}
%
\epigraph{Theory without experiments: Have we gone too far?}
	 {\emph{Michael Mitzenmacher}}

정형적 검증은 여러 제품 환경에서 유용한 것으로
증명되었습니다~\cite{JamesRLarus2004RightingSoftware,AlBessey2010BillionLoCLater,ByronCook2018FormalAmazon,CaitlinSadowski2018staticAnalysisGoogle,DinoDistefano2019FBstaticAnalysis}.
그러나, 리눅스 커널과 같은 복잡한 동시성 코드베이스에 결합된 지속적 통합에
사용되는 자동화된 회귀 테스트 집합에 하드코어 정형 검증이 포함될 수 있기는
할지는 의문입니다.
리눅스 커널 SRCU 를 위한 컨셉 증명은 존재하지만~\cite{LanceRoy2017CBMC-SRCU},
이 테스트는 가장 간단한 RCU 구현 중 하나의 작은 부분을 위한 것이고, 계속
변화하는 리눅스 커널과 함께 유지하기는 어려운 것으로 증명되었습니다.
따라서 리눅스 커널의 회귀 테스트에 정형 검증을 첫번째 멤버로 포함시키기 위해선
무엇이 필요할지 물어볼 가치가 있습니다.

다음 리스트는 좋은 시작이 될 수 있을
겁니다~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:

\iffalse

Formal verification has long proven useful in a number of production
environments~\cite{JamesRLarus2004RightingSoftware,AlBessey2010BillionLoCLater,ByronCook2018FormalAmazon,CaitlinSadowski2018staticAnalysisGoogle,DinoDistefano2019FBstaticAnalysis}.
However, it is an question as to whether hard-core formal verification
will ever be included in the automated regression-test suites used for
continuous integration within complex concurrent codebases, such as the
Linux kernel.
Although there is already a proof of concept for Linux-kernel
SRCU~\cite{LanceRoy2017CBMC-SRCU}, this test is for a small portion
of one of the simplest RCU implementations, and has proven difficult
to keep it current with the ever-changing Linux kernel.
It is therefore worth asking what would be required to incorporate
formal verification as first-class members of the Linux kernel's
regression tests.

The following list is a good
start~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:

\fi

\begin{enumerate}
\item	모든 필요한 변환은 자동화 되어야 합니다.
\item	환경은 (메모리 순서 규칙 포함) 올바르게 처리되어야만 합니다.
\item	메모리와 CPU 오버헤드는 받아들여질 수 있을만큼 적어야 합니다.
\item	버그의 위치를 향하는 구체적 정보가 주어져야 합니다.
\item	소스코드와 입력 이상의 정보의 규모는 작아야만 합니다.
\item	발견된 버그는 코드의 사용자에게 적합해야만 합니다.

\iffalse

\item	Any required translation must be automated.
\item	The environment (including memory ordering) must be correctly
	handled.
\item	The memory and CPU overhead must be acceptably modest.
\item	Specific information leading to the location of the bug
	must be provided.
\item	Information beyond the source code and inputs must be
	modest in scope.
\item	The bugs located must be relevant to the code's users.

\fi

\end{enumerate}

이 리스트는 Richard Bornat 의 명언에 기반하지만 그보다 더 수수합니다: ``정형적
검증 연구자들은 개발자들이 작성하는 코드를 그들이 작성하는 언어로 그것이
수행되는 환경에서 그들이 작성하는 방법으로 검증해야 한다.;;
다음 섹션들은 앞의 요구사항 각자를 논하며, 이 필요성에 몇가지 도구들은 얼마나
잘 화답하고 있는지 보이는 섹션이 그를 뒤따릅니다.

\iffalse

This list builds on, but is somewhat more modest than, Richard Bornat's
dictum: ``Formal-verification researchers should verify the code that
developers write, in the language they write it in, running in the
environment that it runs in, as they write it.''
The following sections discuss each of the above requirements, followed
by a section presenting a scorecard of how well a few tools stack up
against these requirements.

\fi

\subsection{Automatic Translation}
\label{sec:future:Automatic Translation}

Promela 와 \co{spin} 은 귀중한 설계상의 도움이 되지만, 여러분이 C 언어
프로그램을 정형적으로 회귀 테스트 하려 한다면 여러분은 여러분이 코드를 다시
검증하고 싶을 때마다 그 코드를 Promela 로 직접 변환해야 합니다.
여러분의 코드가 매 60-90 일마다 릴리즈 되는 리눅스 커널이라면, 여러분은 매년
네번에서 여섯번 가량 직접 변환을 해야할 겁니다.
시간이 흐름에 따라, 사람의 오류가 발생할 것인데, 이는 이 검증이 소스코드와 맞지
않음을 의미해서 이 검증을 쓸모없게 만들 겁니다.
반복된 검증은 이 정형적 검증 도구이 여러분의 코드를 직접 입력받게 하거나
버그로부터 자유로운 여러분의 코드를 검증에 필요한 형태로 자동으로 변환하는
도구를 가지게 해야 할 겁니다.

\iffalse

Although Promela and \co{spin}
are invaluable design aids, if you need to formally regression-test
your C-language program, you must hand-translate to Promela each time
you would like to re-verify your code.
If your code happens to be in the Linux kernel, which releases every
60--90 days, you will need to hand-translate from four to six times
each year.
Over time, human error will creep in, which means that the verification
won't match the source code, rendering the verification useless.
Repeated verification clearly requires either that the formal-verification
tooling input your code directly, or that there be bug-free automatic
translation of your code to the form required for verification.

\fi

PPCMEM 과 \co{herd} 는 이론상 입력 어셈블리어와 C++ 코드를 입력받을 수 있으나,
이 도구들은 매우 작은 리트머스 테스트에만 동작해서, 일반적으로는 여러분이
여러분의 메커니즘의 핵심 부분을 직접 노출시켜야만 합니다.
Promela 와 \co{spin} 에서처럼, PPCMEM 과 \co{herd} 는 매우 유용하나, 회귀
테스트에 잘 맞지는 않습니다.

\iffalse

PPCMEM and \co{herd} can in theory directly input assembly language
and C++ code, but these tools work only on very small litmus tests,
which normally means that you must extract the core of your
mechanism---by hand.
As with Promela and \co{spin}, both PPCMEM and \co{herd} are
extremely useful, but they are not well-suited for regression suites.

\fi

대조적으로, \co{cbmc} 와 Nidhugg 는 합리적 (여전히 상당히 제한되어 있지만)
크기의 C 프로그램을 입력받으며, 그 능력이 계속 성장한다면, 회귀 테스트 도구에
훌륭한 추가품이 될 수 있을 겁니다.
Coverity 정적 분석 도구 또한 리눅스 커널을 포함한 상당히 큰 크기의 C 프로그램을
입력으로 받습니다.
물론, Coverity 의 정적 분석은 \co{cbmc} 와 Nidhugg 의 것에 비하면 상당히
단순합니다.
다른 한편, Coverity 는 특수한 도전을~\cite{AlBessey2010BillionLoCLater} 갖는
총괄적인 ``C 프로그램'' 에 대한 정의를 가졌습니다.
Amazon Web Services 는 \co{cbmc} 를 포함한 다양한 정형적 검증 도구를 사용하며,
이 도구들 중 일부를 회귀 테스팅에 사용합니다~\cite{ByronCook2018FormalAmazon}.
Google 은 상대적으로 간단한 정적 분석 도구를 논란의 여기가 있지만 C
코드베이스보다 덜 다양한 거대한 Java 코드베이스에
사용합니다~\cite{CaitlinSadowski2018staticAnalysisGoogle}.
Facebook 은 동시성 분석을 포함해 그들의 코드베이스에 보다 공격적인 형태의 정형
검증을
사용합니다만~\cite{DinoDistefano2019FBstaticAnalysis,PeterWOHearn2019incorrectnessLogic}
아직 리눅스 커널에까지 적용하진 않았습니다.
마지막으로, 마이크로소프트는 그들의 코드베이스에 정적 분석을 오랜 기간 사용해
왔습니다~\cite{JamesRLarus2004RightingSoftware}.

\iffalse

In contrast, \co{cbmc} and Nidhugg can input C programs of reasonable
(though still quite limited) size, and if their capabilities continue
to grow, could well become excellent additions to regression suites.
The Coverity static-analysis tool also inputs C programs, and of very
large size, including the Linux kernel.
Of course, Coverity's static analysis is quite simple compared to that
of \co{cbmc} and Nidhugg.
On the other hand, Coverity had an all-encompassing definition of
``C program'' that posed special challenges~\cite{AlBessey2010BillionLoCLater}.
Amazon Web Services uses a variety of formal-verification tools,
including \co{cbmc}, and applies some of these tools to regression
testing~\cite{ByronCook2018FormalAmazon}.
Google uses a number of relatively simple static analysis tools directly
on large Java code bases, which are arguably less diverse than C code
bases~\cite{CaitlinSadowski2018staticAnalysisGoogle}.
Facebook uses more aggressive forms of formal verification against its
code bases, including analysis of concurrency~\cite{DinoDistefano2019FBstaticAnalysis,PeterWOHearn2019incorrectnessLogic},
though not yet on the Linux kernel.
Finally, Microsoft has long used static analysis on its code
bases~\cite{JamesRLarus2004RightingSoftware}.

\fi

이 리스트를 놓고 보면, 제품 수준 소스 코드를 직접 사용할 수 있는 잘 만들어진
정형적 검증 도구를 만들 수 있을 것이 분명합니다.

그러나, C 코드를 입력으로 받는 것의 한가지 단점은 컴파일러가 올바를 거라
가정한다는 겁니다.
한가지 대안은 C 컴파일러가 생성한 바이너리를 입력으로 받아서 모든 연관된
컴파일러 버그를 처리하게 하는 겁니다.
이 방법은 여러 검증 노력에서 사용되었는데, 가장 두곽을 드러내는 것은 SEL4
프로젝트~\cite{ThomasSewell2013L4binaryVerification} 일 겁니다.

\iffalse

Given this list, it is clearly possible to create sophisticated
formal-verification tools that directly consume production-quality
source code.

However, one shortcoming of taking C code as input is that it assumes
that the compiler is correct.
An alternative approach is to take the binary produced by the C compiler
as input, thereby accounting for any relevant compiler bugs.
This approach has been used in a number of verification efforts,
perhaps most notably by the SEL4
project~\cite{ThomasSewell2013L4binaryVerification}.

\fi

\QuickQuiz{
	SEL4 프로젝트에서 사용된 여러 검증 도구의 놀라운 본성을 놓고 보면, 왜
	이 챕터는 그걸 더 다루지 않는지 궁금하군요?

	\iffalse

	Given the groundbreaking nature of the various verifiers used
	in the SEL4 project, why doesn't this chapter cover them in
	more depth?

	\fi

}\QuickQuizAnswer{
	SEL4 프로젝트에서 사용된 검증 도구들이 상당히 쓸모있을 거라는 데에는
	의심의 여지가 없습니다.
	그러나, SEL4 는 단일 CPU 프로젝트로 시작되었습니다.
	그리고 SEL4 가 멀티 프로세서 기능을 얻었지만, 현재로써는 리눅스 커널의
	과거의 Big Kernel Lock (BKL) 과 유사한 큰 규모의 락킹을 사용하고
	있습니다.
	SEL4 의 검증 도구를 병렬 프로그래밍에 대한 책에 추가하는 게 말이 되는
	날이 오길 기대합니다만, 아직은 그 날이 아닙니다.

	\iffalse

	There can be no doubt that the verifiers used by the SEL4
	project are quite capable.
	However, SEL4 started as a single-CPU project.
	And although SEL4 has gained multi-processor
	capabilities, it is currently using very coarse-grained
	locking that is similar to the Linux kernel's old
	Big Kernel Lock (BKL).
	There will hopefully come a day when it makes sense to add
	SEL4's verifiers to a book on parallel programming, but
	this is not yet that day.

	\fi

}\QuickQuizEnd

그러나, 소스나 바이너리에서 직접 검증을 하는 것은 사람에 의한 변환 과정에서의
오류를 제거하는 장점을 갖는데, 안정적인 회귀 테스팅에 치명적으로 중요합니다.

이는 특수 목적 언어를 사용하는 도구가 쓸모없다는 말이 아닙니다.
그와 반대로, 그것들은
\cref{chp:Formal Verification} 에서 논의 했듯 설계 시점 검증에 매우 도움이 될
수 있습니다.
그러나, 그런 도구는 이 섹션의 주제인 자동화된 회귀 테스트에서 특별히 도움되진
않습니다.

\iffalse

However, verifying directly from either the source or binary both have the
advantage of eliminating human translation errors, which is critically
important for reliable regression testing.

This is not to say that tools with special-purpose languages are useless.
On the contrary, they can be quite helpful for design-time verification,
as was discussed in
\cref{chp:Formal Verification}.
However, such tools are not particularly helpful for automated regression
testing, which is in fact the topic of this section.

\fi

\subsection{Environment}
\label{sec:future:Environment}

정형적 검증 도구들이 올바르게 그들의 환경을 모델링하는 것은 치명적으로
중요합니다.
한가지 너무 흔한 누락은 메모리 모델로, Promela/\co{spin} 을 포함한 수많은
정형적 검증 도구들이 sequential consistency 로의 제약을 갖습니다.
\Cref{sec:formal:Is QRCU Really Correct?} 와 연관된 QRCU 경험은 중요한 주의를
기울여야 할 이야기 입니다.

Promela 와 \co{spin} 은
\cref{chp:Advanced Synchronization: Memory Ordering} 에서 알아봤듯 현대의
컴퓨터 시스템과는 잘 들어맞지 않는 sequential consistency 를 가정합니다.
대조적으로, PPCMEM 과 \co{herd} 의 강력한 장점 중 하나는 x86, \ARM, Power,
그리고 \co{herd} 의 경우 리눅스 커널 버전 v4.17 에서 받아들여진 리눅스 커널
메모리 모델~\cite{Alglave:2018:FSC:3173162.3177156} 을 포함한 다양한 CPU
제품군들의 메모리 모델에 대한 자세한 모델링입니다.

\iffalse

It is critically important that formal-verification tools correctly
model their environment.
One all-too-common omission is the memory model, where a great
many formal-verification tools, including Promela/\co{spin}, are
restricted to sequential consistency.
The QRCU experience related in
\cref{sec:formal:Is QRCU Really Correct?}
is an important cautionary tale.

Promela and \co{spin} assume sequential consistency, which is not a
good match for modern computer systems, as was seen in
\cref{chp:Advanced Synchronization: Memory Ordering}.
In contrast, one of the great strengths of PPCMEM and \co{herd}
is their detailed modeling of various CPU families memory models,
including x86, \ARM, Power, and, in the case of \co{herd},
a Linux-kernel memory model~\cite{Alglave:2018:FSC:3173162.3177156},
which was accepted into Linux-kernel version v4.17.

\fi

\co{cmbc} 와 Nihugg 도구들은 메모리 모델을 선택할 수 있는 어떤 능력을 제공하나,
PPCMEM 과 \co{herd} 만큼의 다양성은 아닙니다.
그러나, 시간이 갈수록 거대 규모 도구들이 상당히 다양한 메모리 모델을 수용할
겁니다.

장기적으로는, 정형적 검증 도구가 I/O 를 포함하는 것이 도움이
될테지만~\cite{PaulEMcKenney2016LinuxKernelMMIO} 그러기까진 시간이 좀 걸릴
겁니다.

그렇다고 하나, 환경을 맞추는데 실패하는 도구들도 여전히 유용할 수 있습니다.
예를 들어, 상당히 많은 동시성 버그가 가상의 sequential consistency 제공
시스템에서도 버그일 것이며, 이 버그들은 이 시스템의 메모리 모델을 sequential
consistency 로 지나치게 간략화 하는 도구에 의해서도 발견될 수 있습니다.
그러나, 이런 도구들은 누락된 메모리 순서 지시어에 연관된 버그들은 찾지 못할
텐데, 이 점은
\cref{sec:formal:Is QRCU Really Correct?} 의 주의를 요하는 이야기에서
언급되었습니다.

\iffalse

The \co{cbmc} and Nidhugg tools provide some ability to select
memory models, but do not provide the variety that PPCMEM and
\co{herd} do.
However, it is likely that the larger-scale tools will adopt
a greater variety of memory models as time goes on.

In the longer term, it would be helpful for formal-verification
tools to include I/O~\cite{PaulEMcKenney2016LinuxKernelMMIO},
but it may be some time before this comes to pass.

Nevertheless, tools that fail to match the environment can still
be useful.
For example, a great many concurrency bugs would still be bugs on
a mythical sequentially consistent system, and these bugs could
be located by a tool that over-approximates the system's memory model
with sequential consistency.
Nevertheless, these tools will fail to find bugs involving missing
memory-ordering directives, as noted in the aforementioned
cautionary tale of
\cref{sec:formal:Is QRCU Really Correct?}.

\fi

\subsection{Overhead}
\label{sec:future:Overhead}

모든 하드코어 정형적 검증 도구들은 본성적으로 폭발적인데, 흥미로운 소프트웨어
질문 중 대부분이 실제로 비결정적이라 생각하기 전까지는 실망스러워 보일 겁니다.
그러나, 폭발적인 성질에도 정도의 차이가 있습니다.

PPCMEM 은 설계상 최적화 되어 있지 않은데, 문제의 메모리 모델이 올바르게
표현되었음을 확실히 보장하기 위함입니다.
\co{herd} 는
\cref{sec:formal:Axiomatic Approaches} 에서 언급되었듯 더 적극적인 최적화를
하는데, 따라서 PPCMEM 보다 수십 수백배 빠릅니다.
그렇다고 하나, PPCMEM 과 \co{herd} 둘 다 커다란 코드의 몸통보다는 작은 리트머스
테스트를 목표로 합니다.

대조적으로, Promela/\co{spin}, \co{cbmc} 그리고 Nidhugg 는 (어떤) 더 큰 분량의
코드를 위해 설계되었습니다.
Promela/\co{spin} 은 Curiosity rover 의 파일시스템을 검증하는데
사용되었고~\cite{DBLP:journals/amai/GroceHHJX14}, 앞서 언급되었듯 \co{cbmc} 와
Nidhugg 는 리눅스 커널 RCU 에 적용되었습니다.

\iffalse

Almost all hard-core formal-verification tools are exponential
in nature, which might seem discouraging until you consider that
many of the most interesting software questions are in fact undecidable.
However, there are differences in degree, even among exponentials.

PPCMEM by design is unoptimized, in order to provide greater assurance
that the memory models of interest are accurately represented.
The \co{herd} tool optimizes more aggressively, as described in
\cref{sec:formal:Axiomatic Approaches}, and is thus orders of magnitude
faster than PPCMEM\@.
Nevertheless, both PPCMEM and \co{herd} target very small litmus tests
rather than larger bodies of code.

In contrast, Promela/\co{spin}, \co{cbmc}, and Nidhugg are designed for
(somewhat) larger bodies of code.
Promela/\co{spin} was used to verify the Curiosity rover's
filesystem~\cite{DBLP:journals/amai/GroceHHJX14} and, as noted earlier,
both \co{cbmc} and Nidhugg were appled to Linux-kernel RCU\@.

\fi

만약 휴리스틱 상의 발전이 지난 30년간의 속도와 비슷하게 계속된다면 우린 정형적
검증의 오버헤드가 크게 감소할 것을 기대할 수 있습니다.
그러나, 조합상의 폭발 (combinatorial explosion) 은 여전히 조합상의 폭발이므로,
휴리스틱의 계속된 개선과 관계없이 검증될 수 있는 프로그램의 크기를 분명하게
제한할 것입니다.

그러나, 조합상의 폭발의 이면은 마케도니아의 Philip II 의 조언입니다: ``분할하고
정복하라.''
만약 큰 프로그램이 분할될 수 있고 그 조각들이 검증된다면, 그 결과는 조합상의
\emph{내파 (implosion)}~\cite{PaulEMcKenney2011Verico} 입니다.
분할을 할 자연스러운 장소는 API 경계인데, 예를 들ㅇ면 락킹 기능의 그것들입니다.
그러면 하나의 검증 경로는 이 락킹 구현이 올바른지를 검증하고, 추가적인 검증
경로는 락킹 API 의 올바른 사용을 검증할 수 있습니다.

\iffalse

If advances in heuristics continue at the rate of the past three
decades, we can look forward to large reductions in overhead for
formal verification.
That said, combinatorial explosion is still combinatorial explosion,
which would be expected to sharply limit the size of programs that
could be verified, with or without continued improvements in
heuristics.

However, the flip side of combinatorial explosion is Philip II of
Macedon's timeless advice: ``Divide and rule.''
If a large program can be divided and the pieces verified, the result
can be combinatorial \emph{implosion}~\cite{PaulEMcKenney2011Verico}.
One natural place to divide is on API boundaries, for example, those
of locking primitives.
One verification pass can then verify that the locking implementation
is correct, and additional verification passes can verify correct
use of the locking APIs.

\fi

\begin{listing}[tbp]
\input{CodeSamples/formal/herd/C-SB+l-o-o-u+l-o-o-u-C@whole.fcv}
\caption{Emulating Locking with \tco{cmpxchg_acquire()}}
\label{lst:future:Emulating Locking with cmpxchg}
\end{listing}

\begin{table}[tbh]
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]}
	\toprule
	\multicolumn{1}{c}{\# Threads} & \multicolumn{1}{c}{Locking} &
			\multicolumn{1}{c}{\tco{cmpxchg_acquire}} \\
	\midrule
	2 & 0.004 &  0.022 \\
	3 & 0.041 &  0.743 \\
	4 & 0.374 & 59.565 \\
	5 & 4.905 &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance (s)}
\label{tab:future:Emulating Locking: Performance (s)}
\end{table}

이 방법의 성능상 이득은 리눅스 커널 메모리
모델~\cite{Alglave:2018:FSC:3173162.3177156} 을 사용해 선보일 수 있습니다.
이 모델은 \co{spin_lock()} 과 \co{spin_unlock()} 기능을 제공합니다만 이
기능들은 또한
\cref{lst:future:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} 와 \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus})
에서 보이듯 \co{cmpxchg_acquire()} 와 \co{smp_store_release()} 를 사용해
에뮬레이션 될 수 있습니다.
\Cref{tab:future:Emulating Locking: Performance (s)}
는 이 모델의 \co{spin_lock()} 과 \co{spin_unlock()} 을 사용하는 것의 성능과
확장성을 이 기능들을 에뮬레이션 하는 것과 비교합니다.
차이는 사소하지 않습니다: 네개의 프로세스에서, 이 모델은 에뮬레이션보다 수백배
이상 빠릅니다!

\iffalse

The performance benefits of this approach can be demonstrated using
the Linux-kernel memory
model~\cite{Alglave:2018:FSC:3173162.3177156}.
This model provides \co{spin_lock()} and \co{spin_unlock()}
primitives, but these primitives can also be emulated using
\co{cmpxchg_acquire()} and \co{smp_store_release()}, as shown in
\cref{lst:future:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} and \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus}).
\Cref{tab:future:Emulating Locking: Performance (s)}
compares the performance and scalability of using the model's
\co{spin_lock()} and \co{spin_unlock()} against emulating these
primitives as shown in the listing.
The difference is not insignificant: At four processes, the model
is more than two orders of magnitude faster than emulation!

\fi

\QuickQuiz{
\begin{fcvref}[ln:future:formalregress:C-SB+l-o-o-u+l-o-o-u-C:whole]
	\Cref{lst:future:Emulating Locking with cmpxchg}
	의 라인~\lnref{filter_} 에서는 왜 단순하게 그 조건을 \co{exists} 절에
	추가하는 대신 별도의 \co{filter} 커맨드를 사용하나요?
	그리고 \co{cmpxchg_acquire()} 대신 \co{xchg_acquire()} 를 사용하는게 더
	간단하지 않을까요?

	\iffalse

	Why bother with a separate \co{filter} command on line~\lnref{filter_} of
	\cref{lst:future:Emulating Locking with cmpxchg}
	instead of just adding the condition to the \co{exists} clause?
	And wouldn't it be simpler to use \co{xchg_acquire()} instead
	of \co{cmpxchg_acquire()}?

	\fi

\end{fcvref}
}\QuickQuizAnswer{
	이 \co{filter} 절은 \co{herd} 도구가 \co{exists} 절이 그러는 것보다
	이른 처리 단계에서 수행을 폐기하게 해줘서 상당한 속도향상을 가져옵니다.

	\iffalse

	The \co{filter} clause causes the \co{herd} tool to discard
	executions at an earlier stage of processing than does
	the \co{exists} clause, which provides significant speedups.

	\fi

\begin{table}[tbh]
\rowcolors{7}{lightgray}{}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]
		S[table-format=3.3]S[table-format=2.3]S[table-format=3.3]}
	\toprule
	& & \multicolumn{2}{c}{\tco{cmpxchg_acquire()}}
		& \multicolumn{2}{c}{\tco{xchg_acquire()}} \\
	\cmidrule(l){3-4} \cmidrule(l){5-6}
	\multicolumn{1}{c}{\#} & \multicolumn{1}{c}{Lock}
		& \multicolumn{1}{c}{\tco{filter}}
			& \multicolumn{1}{c}{\tco{exists}}
				& \multicolumn{1}{c}{\tco{filter}}
					& \multicolumn{1}{c}{\tco{exists}} \\
	\cmidrule{1-1} \cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-6}
	2 & 0.004 &  0.022 &   0.039 &  0.027 &  0.058 \\
	3 & 0.041 &  0.743 &   1.653 &  0.968 &  3.203 \\
	4 & 0.374 & 59.565 & 151.962 & 74.818 & 500.96 \\
	5 & 4.905 &        &         &        &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance Comparison (s)}
\label{tab:future:Emulating Locking: Performance Comparison (s)}
\end{table}

	\co{xchg_acquire()} 의 경우, 이 어토믹 오퍼레이션은 락 획득이
	성공했는지와 관계없이 쓰기를 할건데, 이는 \co{xchg_acquire()} 를
	사용하는 모델은 락 획득 실패의 경우 쓰기를 하지 않을
	\co{cmpxchg_acquire()} 를 사용하는 것보다 더 맣은 오퍼레이션을 갖게
	됨을 의미합니다.
	더 많은 쓰기는 폭발할 수 있는 더 많은 조합을 의미하는데,
	\cref{tab:future:Emulating Locking: Performance Comparison (s)} 에 보인
	것과 같습니다
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, 그리고
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}).
	이 표는 \co{cmpxchg_acquire()} 가 \co{xchg_acquire()} 보다, 그리고
	\co{filter} 의 사용이 \co{exists} 절의 사용보다 성능이 나음을 분명하게
	보입니다.

	\iffalse

	As for \co{xchg_acquire()}, this atomic operation will do a
	write whether or not lock acquisition succeeds, which means
	that a model using \co{xchg_acquire()} will have more operations
	than one using \co{cmpxchg_acquire()}, which won't do a write
	in the failed-acquisition case.
	More writes means more combinatorial to explode, as shown in
	\cref{tab:future:Emulating Locking: Performance Comparison (s)}
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, and
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}).
	This table clearly shows that \co{cmpxchg_acquire()}
	outperforms \co{xchg_acquire()} and that use of the
	\co{filter} clause outperforms use of the \co{exists} clause.

	\fi

}\QuickQuizEnd

도구들이 자동으로 커다란 프로그램을 분할하고 그 조각들을 검증하고 그 조각들의
조합들을 검증할 수 있다면 매우 유용할 겁니다.
그 전까지는, 커다란 프로그램의 검증은 상당한 직접적 개입을 필요로 할 겁니다.
이 개입은 스크립트를 사용하는 것으로 중개되는게 선호될 것이고, 각 릴리즈마다
반복된 검증을 안정적으로 이끌어 갈 수 있을수록 좋을 것이고, 종국적으로는 잘
짜여진 지속적 통합의 형태면 좋을 겁니다.
그리고 Facebook 의 Infer 도구는 compositionality 와 abstraction 을
통해~\cite{SamBlackshear2018RacerD,DinoDistefano2019FBstaticAnalysis} 그걸 위한
중요한 단계를 밟고 있습니다.

어떤 경우든, 우린 정형적 검증 기능이 시간에 따라 증가하길, 그리고 그런 증가가
결국 정형적 검증의 회귀 테스트에서의 적합도를 증가시킬 것을 예상할 수 있습니다.

\iffalse

It would of course be quite useful for tools to automatically divide
up large programs, verify the pieces, and then verify the combinations
of pieces.
In the meantime, verification of large programs will require significant
manual intervention.
This intervention will preferably mediated by scripting, the better to
reliably carry out repeated verifications on each release, and
preferably eventually in a manner well-suited for continuous integration.
And Facebook's Infer tool has taken important steps towards doing just
that, via compositionality and
abstraction~\cite{SamBlackshear2018RacerD,DinoDistefano2019FBstaticAnalysis}.

In any case, we can expect formal-verification capabilities to continue
to increase over time, and any such increases will in turn increase
the applicability of formal verification to regression testing.

\fi

\subsection{Locate Bugs}
\label{sec:future:Locate Bugs}

모든 크기의 모든 소프트웨어 작품은 버그를 갖습니다.
따라서, 버그의 존재나 부재만을 알리는 정형적 검증 도구는 특별히 유용하지
않습니다.
필요한 건 그 버그가 어디에 있는지와 그 버그의 본성에 대한 최소한의 \emph{어떤}
정보를 제공하는 도구입니다.

\co{cbmc} 의 출력은 소스코드로 매핑되는 추적 기록을 포함하는데
Promela/\co{spin} 과 Nidhugg 의 것과 유사합니다.
물론, 이 기록은 무척 길 수 있으며, 그걸 분석하는 건 매우 귀찮을 수 있습니다.
그러나, 그러는게 과거 방법으로 버그를 찾는 것보다는 보통 훨씬 빠르고
즐겁습니다.

또한, 정형적 검증 도구의 가장 간단한 테스트 중 하나는 버그 주입입니다.
어쨌건, 아무나 \co{printf("VERIFIED\\n")} 을 작성할 수 있는 건 아니지만, 정형적
검증 도구의 개발자들 역시 나머지 사람들 만큼이나 버그에 취약하다는 게
사실입니다.
따라서, 버그가 존재한다고 주장하기만 하는 정형적 검증 도구는 그걸 실제 세계의
코드에서 검증하기가 더 어려우므로 기본적으로 덜 믿음직합니다.

\iffalse

Any software artifact of any size contains bugs.
Therefore, a formal-verification tool that reports only the
presence or absence of bugs is not particularly useful.
What is needed is a tool that gives at least \emph{some} information
as to where the bug is located and the nature of that bug.

The \co{cbmc} output includes a traceback mapping back to the source
code, similar to Promela/\co{spin}'s, as does Nidhugg.
Of course, these tracebacks can be quite long, and analyzing them
can be quite tedious.
However, doing so is usually quite a bit faster
and more pleasant than locating bugs the old-fashioned way.

In addition, one of the simplest tests of formal-verification tools is
bug injection.
After all, not only could any of us write
\co{printf("VERIFIED\\n")}, but the plain fact is that
developers of formal-verification tools are just as bug-prone as
are the rest of us.
Therefore, formal-verification tools that just proclaim that a
bug exists are fundamentally less trustworthy because it is
more difficult to verify them on real-world code.

\fi

이 모든 것을 차치하고, 정형적 검증 도구를 작성하는 사람들은 존재하는 도구들을
사용할 수 있습니다.
예를 들어, 심각하고 드문 버그의 존재와 부재를 탐지하도록 설계된 도구는
bisection 을 도울 수도 있습니다.
만약 이 프로그램의 어떤 과거 버전이 버그를 포함하지 않았지만 새 버전은
포함한다면, bisection 은 이 버그를 주입한 커밋을 빠르게 찾는데 사용될 수
있으며, 그 정보는 버그를 찾고 고치는데 충분할 수도 있습니다.
물론, 이런 종류의 전략은 흔한 버그에서는 잘 동작하지 않을텐데, 이 경우
bisection 은 모든 커밋이 최소 하나의 흔한 버그는 가질 것이기 때문입니다.

따라서, 많은 정형적 검증 도구들에 의해 제공되는 수행 기록은 계속해서 가치있을
것이며, 특히 복잡하고 이해하기 어려운 버그에서 그럴 겁니다.
또한, 최근의 작업은 full-up 정확성 증명을 위해 사용되던 전통적 Hoare 로직을
생각나게 하는 \emph{incorrectness-logic} 정형화를, 그러나 버그를 찾는
목적만으로~\cite{PeterWOHearn2019incorrectnessLogic} 적용합니다. 

\iffalse

All that aside, people writing formal-verification tools are
permitted to leverage existing tools.
For example, a tool designed to determine only the presence
or absence of a serious but rare bug might leverage bisection.
If an old version of the program under test did not contain the bug,
but a new version did, then bisection could be used to quickly
locate the commit that inserted the bug, which might be
sufficient information to find and fix the bug.
Of course, this sort of strategy would not work well for common
bugs because in this case bisection would fail due to all commits
having at least one instance of the common bug.

Therefore, the execution traces provided
by many formal-verification tools will continue to be valuable,
particularly for complex and difficult-to-understand bugs.
In addition, recent work applies \emph{incorrectness-logic}
formalism reminiscent of the traditional Hoare logic used for
full-up correctness proofs, but with the sole purpose of finding
bugs~\cite{PeterWOHearn2019incorrectnessLogic}.

\fi

\subsection{Minimal Scaffolding}
\label{sec:future:Minimal Scaffolding}

과거, 정형적 검증 연구자들은 소프트웨어의 무엇이 검증될 것인지에 대한 전체
명세서를 요구했습니다.
불행히도, 수학적으로 엄격한 명세서는 실제 코드보다 클 수도 있고, 명세의 각 행은
코드의 각 행이 그렇듯 버그를 포함하고 있을 수 있습니다.
코드가 명세를 올바르게 구현했다는 걸 증명하기 위한 정형적 검증 노력은 둘 사이의
버그 대비 버그 비교의 증명이 될텐데, 이는 그다지 도움되지 않을 수 있습니다.

더 나쁜게, 리눅스 커널 RCU 를 포함한 여러 소프트웨어 작품에 대한 그 요구사항은
본성적으로 경험에 편중되어
있습니다~\cite{PaulEMcKenney2015RCUreqts1,PaulEMcKenney2015RCUreqts2,PaulEMcKenney2015RCUreqts3}.\footnote{
	또는, 정형적 검증의 말투로 말하자면, 리눅스 커널 RCU 는 \emph{불완전한
	명세} 를 가졌습니다.}
이런 흔한 종류의 소프트웨어에서, 완전환 명세는 세련된 허구입니다.
완전한 명세란 2017년 말의 Meltdown 과 Spectre 사이드 채널 공격으로
분명해진 하드웨어에서의 허구보다 덜하지도
않습니다~\cite{JannHorn2018MeltdownSpectre}.

\iffalse

In the old days, formal-verification researchers demanded a full
specification against which the software would be verified.
Unfortunately, a mathematically rigorous specification might well
be larger than the actual code, and each line of specification
is just as likely to contain bugs as is each line of code.
A formal verification effort proving that the code faithfully implemented
the specification would be a proof of bug-for-bug compatibility between
the two, which might not be all that helpful.

Worse yet, the requirements for a number of software artifacts,
including Linux-kernel RCU, are empirical in
nature~\cite{PaulEMcKenney2015RCUreqts1,PaulEMcKenney2015RCUreqts2,PaulEMcKenney2015RCUreqts3}.\footnote{
	Or, in formal-verification parlance, Linux-kernel RCU has an
	\emph{incomplete specification}.}
For this common type of software, a complete specification is a
polite fiction.
Nor are complete specifications any less fictional for hardware,
as was made clear by the late-2017 Meltdown and Spectre side-channel
attacks~\cite{JannHorn2018MeltdownSpectre}.

\fi

이 상황은 실제 세계의 소프트웨어와 하드웨어 작품에 대한 정형적 검증의 희망을
포기하게 할 수도 있겠으나, 할 수 있는게 상당히 있다는 것이 드러났습니다.
예를 들어, 설계와 코딩 규칙은 코드에 포함된 단정문들처럼 부분적 명세로 동작할
수 있습니다.
그리고 실제로 \co{cbmc} 와 Nidhugg 같은 정형적 검증 도구는 발동될 수 있는
단정문들을 검사함으로써 암묵적으로 이 단정문들을 명세의 부분으로 취급합니다.
그러나, 단정문들도 코드의 한 부분인데, 이는 특히나 그 코드가 스트레스 테스트에
적합하다면 그게 쓸모없어질 확률을 낮춥니다.\footnote{
	그리고 여러분은 여러분의 코드를 스트레스 테스트 \emph{합니다}, 그렇지
	않습니까?}
\co{cbmc} 도구 역시 array-out-of-bound 참조를 검사하며, 따라서 암묵적으로 그걸
명세에 포함시킵니다.
앞서 언급된 비정확성 로직 또한 명세를 사용하는 걸로 생각될 수
있습니다~\cite{PeterWOHearn2019incorrectnessLogic}.

\iffalse

This situation might cause one to give up all hope of formal verification
of real-world software and hardware artifacts, but it turns out that there is
quite a bit that can be done.
For example, design and coding rules can act as a partial specification,
as can assertions contained in the code.
And in fact formal-verification tools such as \co{cbmc} and Nidhugg
both check for assertions that can be triggered, implicitly treating
these assertions as part of the specification.
However, the assertions are also part of the code, which makes it less
likely that they will become obsolete, especially if the code is
also subjected to stress tests.\footnote{
	And you \emph{do} stress-test your code, don't you?}
The \co{cbmc} tool also checks for array-out-of-bound references,
thus implicitly adding them to the specification.
The aforementioned incorrectness logic can also be thought of as using
an implicit bugs-not-present
specification~\cite{PeterWOHearn2019incorrectnessLogic}.

\fi

이 암묵적 명세 접근법은 상당히 말이 되는데, 특히 여러분이 정형적 검증을 완전한
올바름의 증명이 아니라 일반적인 경우보다 다른 강점과 약점을 갖는 검증의 대안적
형태, 즉 테스트로 생각한다면 그렇습니다.
이 관점에서, 소프트웨어는 항상 버그를 가질 것이며, 따라서 버그를 찾는 걸 돕는
모든 종류의 모든 도구는 실제로 아주 좋은 것입니다.

\iffalse

This implicit-specification approach makes quite a bit of sense, particularly
if you look at formal verification not as a full proof of correctness,
but rather an alternative form of validation with a different set of
strengths and weaknesses than the common case, that is, testing.
From this viewpoint, software will always have bugs, and therefore any
tool of any kind that helps to find those bugs is a very good thing
indeed.

\fi

\subsection{Relevant Bugs}
\label{sec:future:Relevant Bugs}

버그를 발견하는 것---그리고 고치는 것---은 물론 모든 종류의 검증 노력의 모든
요점입니다.
분명, 위양성 (false positive) 는 막아져야 합니다.
그러나 위양성의 부재 하에서 조차도, 보그는 존재합니다.

예를 들어, 어떤 소프트웨어 작품이 정확히 100개의 버그를 가지고 있으며, 이것들
각자는 평균적으로 백만년의 수행 시간 중 한번 발생한다고 해봅시다.
더 나아가서 박식한 정형 검증 도구가 개발자들은 마땅히 고쳐야 할 이 모든 100개의
버그를 찾아냈다고 해 봅시다.
이 소프트웨어 제품의 안정성에는 무슨 일이 벌어지겠습니까?

답은 안정성의 \emph{하락} 입니다.

\iffalse

Finding bugs---and fixing them---is of course the whole point of any
type of validation effort.
Clearly, false positives are to be avoided.
But even in the absence of false positives, there are bugs and there are bugs.

For example, suppose that a software artifact had exactly 100 remaining
bugs, each of which manifested on average once every million years
of runtime.
Suppose further that an omniscient formal-verification tool located
all 100 bugs, which the developers duly fixed.
What happens to the reliability of this software artifact?

The answer is that the reliability \emph{decreases}.

\fi

이를 보기 위해, 역사적 경험은 약 7\,\% 의 수정이 새로운 버그를
불러들인다고~\cite{RexBlack2012SQA} 함을 기억하십시오.
따라서, 조합된 실패까지의 중간 시간 (meat time to failure: MTBF) 약 10,000 년을
갖는 이 100개의 버그를 고치는 것은 일곱개의 버그를 더 불러들입니다.
역사적 통계는 새 버그 각각은 70,000 년보다 훨씬 적은 MTBF 를 가질 것이라
말합니다.
이는 결국 이 일곱개의 새 버그의 조합된 MTBF 는 10,000 년보다 훨씬 적을 것임을
의미하며, 이는 결국 좋은 의도로 만들어진 원래의 100개의 버그의 수정이 실제로는
전체 소프트웨어의 안정성을 실제로는 하락시켰음을 의미하게 됩니다.

\iffalse

To see this, keep in mind that historical experience indicates that
about 7\,\% of fixes introduce a new bug~\cite{RexBlack2012SQA}.
Therefore, fixing the 100 bugs, which had a combined mean time to failure
(MTBF) of about 10,000 years, will introduce seven more bugs.
Historical statistics indicate that each new bug will have an MTBF
much less than 70,000 years.
This in turn suggests that the combined MTBF of these seven new bugs
will most likely be much less than 10,000 years, which in turn means
that the well-intentioned fixing of the original 100 bugs actually
decreased the reliability of the overall software.

\fi

\QuickQuizSeries{%
\QuickQuizB{
	알려진 버그의 MTBF 들이 아직 발견되지 않은 버그의 MTBF 를 예측하기 좋은
	정보임을 어떻게 아나요?

	\iffalse

	How do we know that the MTBFs of known bugs is a good estimate
	of the MTBFs of bugs that have not yet been located?

	\fi

}\QuickQuizAnswerB{
	우린 모릅니다, 그렇지만 그건 중요치 않습니다.

	이를 보기 위해, 7\,\% 라는 숫자는 뒤따라서 발견된 버그의 주입에만
	적용됨을 기억하십시오: 이는 발견되지 못한 버그의 주입은 완전히 무시해야
	합니다.
	따라서, 이 알려진 버그에 대한 MTBF 통계는 뒤따라서 발견된 주입된 버그에
	대한 좋은 추정이 됩니다.

	이 전체 섹션의 핵심 요점은, 결코 발견되지 못한 버그보다는 사용자를
	불편하게 하는 버그를 더 주의해야 한다는 겁니다.
	이는 물론 우리가 아직 사용자를 불편하게 하지 않은 버그를 완전히
	무시해야 한다는 말이 \emph{아니라}, 우리는 가장 중요하고 시급한 버그를
	고치는데 있어 노력의 우선순위를 올바르게 잡아야 한다는 것입니다.

	\iffalse

	We don't, but it does not matter.

	To see this, note that the 7\,\% figure only applies to injected
	bugs that were subsequently located: It necessarily ignores
	any injected bugs that were never found.
	Therefore, the MTBF statistics of known bugs is likely to be
	a good approximation of that of the injected bugs that are
	subsequently located.

	A key point in this whole section is that we should be more
	concerned about bugs that inconvenience users than about
	other bugs that never actually manifest.
	This of course is \emph{not} to say that we should completely
	ignore bugs that have not yet inconvenienced users, just that
	we should properly prioritize our efforts so as to fix the
	most important and urgent bugs first.

	\fi

}\QuickQuizEndB
%
\QuickQuizE{
	하지만 정형적 검증 도구는 그 수정에 의해 만들어진 버그를 곧바로
	찾아낼텐데 왜 이게 문제인가요?

	\iffalse

	But the formal-verification tools should immediately find all the
	bugs introduced by the fixes, so why is this a problem?

	\fi

}\QuickQuizAnswerE{
	실제 세계의 정형적 검증 도구는 (더 목소리 높은 제안자의 정형 검증의
	상상에만 존재하는 것과는 달리) 현명치 못하기 때문에, 그리고 따라서 특정
	종류의 버그를 찾아내지 못하기 때문에 문제가 됩니다.
	한가지만 예를 들어보자면, 정형적 검증 도구는 누락된 단정문에 연관된,
	또는 명세의 발견되지 못한 부분에 연관된 버그를 찾지 못할 겁니다.

	\iffalse

	It is a problem because real-world formal-verification tools
	(as opposed to those that exist only in the imaginations of
	the more vociferous proponents of formal verification) are
	not omniscient, and thus are only able to locate certain types
	of bugs.
	For but one example, formal-verification tools are unlikely to
	spot a bug corresponding to an omitted assertion or, equivalently,
	a bug corresponding to an undiscovered portion of the specification.

	\fi

}\QuickQuizEndE
}

더 나쁜게, 평균적으로 매일 한번씩 나타나는 하나의 버그와 백만년마다 나타나는
99개의 버그를 갖는 소프트웨어 작품을 생각해 봅시다.
어떤 정형적 검증 도구가 이 99 백만년짜리 버그를 발견했지만, 이 하루짜리 버그를
찾지 못했다고 해봅시다.
이 99개의 발견된 버그를 고치는데에는 시간과 노력을 필요로 할 것이고 안정성을
떨어뜨리며, 매일 나타나는 버그를 고치기 위해서는 아무것도 하지 않게 될 것인데,
이는 부끄럽고 훨씬 나쁜 일입니다.

따라서, 가장 문제가 되는 버그를 찾는걸 우선시하는 검증 도구를 갖는게 최선일
겁니다.
그러나,
\cref{sec:future:Locate Bugs}
에서 이야기된 바와 같이, 추가적인 도구를 사용하는게 가능합니다.
그런 한가지 강력한 도구는 평범한 과거의 테스트 기법입니다.
버그에 대한 정보를 가졌다면, 이를 위한 구체적 테스트를 만드는게 가능할 것이고,
버그가 드러날 확률을 높이기 위해
\cref{sec:debugging:Hunting Heisenbugs}
에서 설명된 기법들을 사용할 수 있을 겁니다.
이 기법들은 버그의 평소 실패 확률을 대략적으로 추정하는 계산을 가능하게 할
것이고, 이는 결국 버그 수정 노력의 우선순위를 정하는데 사용될 수 있을 겁니다.

\iffalse

Worse yet, imagine another software artifact with one bug that fails
once every day on average and 99 more that fail every million years
each.
Suppose that a formal-verification tool located the 99 million-year
bugs, but failed to find the one-day bug.
Fixing the 99 bugs located will take time and effort, decrease
reliability, and do nothing at all about the pressing each-day failure
that is likely causing embarrassment and perhaps much worse besides.

Therefore, it would be best to have a validation tool that
preferentially located the most troublesome bugs.
However, as noted in
\cref{sec:future:Locate Bugs},
it is permissible to leverage additional tools.
One powerful tool is none other than plain old testing.
Given knowledge of the bug, it should be possible to construct
specific tests for it, possibly also using some of the techniques
described in
\cref{sec:debugging:Hunting Heisenbugs}
to increase the probability of the bug manifesting.
These techniques should allow calculation of a rough estimate of the
bug's raw failure rate, which could in turn be used to prioritize
bug-fix efforts.

\fi

\QuickQuiz{
	하지만 많은 정형적 검증 도구는 한번에 하나의 버그만 찾을 수 있으므로,
	이 도구가 다음 버그를 찾기 전에 각 버그가 고쳐져야만 합니다.
	그런 도구가 주어졌을 때 어떻게 버그 수정 노력의 우선순위를 정할 수
	있겠습니까?

	\iffalse

	But many formal-verification tools can only find one bug at
	a time, so that each bug must be fixed before the tool can
	locate the next.
	How can bug-fix efforts be prioritized given such a tool?

	\fi

}\QuickQuizAnswer{
	한가지 방법은 제품 환경에서는 적합하지 않을 수도 있으나 그 도구가 다음
	버그를 찾는 것은 허용할 수 있는 간단한 수정을 제공하는 것입니다.
	또다른 방법은 설정이나 입력을 제한해서 지금까지 발견된 버그가 발생하지
	못하게 하는 겁니다.
	여러 비슷한 방법들이 있습니다만, 공통적인 주제는 이 도구의 관점에서
	버그를 고치는 것은 제품 품질 수정을 만들고 검증하는 것보다 훨씬 쉽다는
	것이며, 핵심은 제품 품질 수정을 만들고 검증하는데 필요한 더 거대한
	노력을 우선순위화 조정 하자는 겁니다.

	\iffalse

	One approach is to provide a simple fix that might not be
	suitable for a production environment, but which allows
	the tool to locate the next bug.
	Another approach is to restrict configuration or inputs
	so that the bugs located thus far cannot occur.
	There are a number of similar approaches, but the common theme
	is that fixing the bug from the tool's viewpoint is usually much
	easier than constructing and validating a production-quality fix,
	and the key point is to prioritize the larger efforts required
	to construct and validate the production-quality fixes.

	\fi

}\QuickQuizEnd

더 적은 수의 preemption 이 더 발생 가능성 높다는 합리적 가정 하에 더 적은
preemption 을 갖는 수행의 우선순위를 높이는 정형 검증 작업이 최근에 있었습니다.

연관된 버그를 찾는 것은 너무 큰 요구로 들릴 수도 있겠으나, 우리가 소프트웨어의
안정성을 정말로 높이고자 한다면 정말로 필요한 것입니다.

\iffalse

There has been some recent formal-verification work that prioritizes
executions having fewer preemptions, under that reasonable assumption
that smaller numbers of preemptions are more likely.

Identifying relevant bugs might sound like too much to ask, but it is what
is really required if we are to actually increase software reliability.

\fi

\subsection{Formal Regression Scorecard}
\label{sec:future:Formal Regression Scorecard}

\begin{table*}[tbh]
% \rowcolors{6}{}{lightgray}
%\renewcommand*{\arraystretch}{1.1}
\small
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccccccccc}
	\toprule
	& & Promela & & PPCMEM & & \tco{herd} & & \tco{cbmc} & & Nidhugg \\
	\midrule
	(1) Automated &
		& \cellcolor{red!50} &
			& \cellcolor{orange!50} &
				& \cellcolor{orange!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(2) Environment &
		& \cellcolor{red!50} (MM) &
			& \cellcolor{green!50} &
				& \cellcolor{blue!50} &
					& \cellcolor{yellow!50} (MM) &
						& \cellcolor{orange!50} (MM) \\
	\addlinespace[3pt]
	(3) Overhead &
		& \cellcolor{yellow!50} &
			& \cellcolor{red!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{yellow!50} (SAT) &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(4) Locate Bugs &
		& \cellcolor{yellow!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{green!50} &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(5) Minimal Scaffolding &
		& \cellcolor{green!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(6) Relevant Bugs &
		& \cellcolor{yellow!50} ??? &
			& \cellcolor{yellow!50} ??? &
				& \cellcolor{yellow!50} ??? &
					& \cellcolor{yellow!50} ??? &
						& \cellcolor{yellow!50} ??? \\
	\bottomrule
\end{tabular}
\caption{Formal Regression Scorecard}
\label{tab:future:Formal Regression Scorecard}
\end{table*}

\Cref{tab:future:Formal Regression Scorecard}
shows a rough-and-ready scorecard for the formal-verification tools
covered in this chapter.
Shorter wavelengths are better than longer wavelengths.

Promela requires hand translation and supports only sequential
consistency, so its first two cells are red.
It has reasonable overhead (for formal verification, anyway)
and provides a traceback, so its next two cells are yellow.
Despite requiring hand translation, Promela handles assertions
in a natural way, so its fifth cell is green.

PPCMEM usually requires hand translation due to the small size of litmus
tests that it supports, so its first cell is orange.
It handles several memory models, so its second cell is green.
Its overhead is quite high, so its third cell is red.
It provides a graphical display of relations among operations, which
is not as helpful as a traceback, but is still quite useful, so its
fourth cell is yellow.
It requires constructing an \co{exists} clause and cannot take
intra-process assertions, so its fifth cell is also yellow.

The \co{herd} tool has size restrictions similar to those of PPCMEM,
so \co{herd}'s first cell is also orange.
It supports a wide variety of memory models, so its second cell is blue.
It has reasonable overhead, so its third cell is yellow.
Its bug-location and assertion capabilities are quite similar to those
of PPCMEM, so \co{herd} also gets yellow for the next two cells.

The \co{cbmc} tool inputs C code directly, so its first cell is blue.
It supports a few memory models, so its second cell is yellow.
It has reasonable overhead, so its third cell is also yellow, however,
perhaps SAT-solver performance will continue improving.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

Nidhugg also inputs C code directly, so its first cell is also blue.
It supports only a couple of memory models, so its second cell is orange.
Its overhead is quite low (for formal-verification), so its
third cell is green.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

So what about the sixth and final row?
It is too early to tell how any of the tools do at finding the right bugs,
so they are all yellow with question marks.

\QuickQuizSeries{%
\QuickQuizB{
	How would testing stack up in the scorecard shown in
	\cref{tab:future:Formal Regression Scorecard}?
}\QuickQuizAnswerB{
	It would be blue all the way down, with the possible
	exception of the third row (overhead) which might well
	be marked down for testing's difficulty finding
	improbable bugs.

	On the other hand, improbable bugs are often also
	irrelevant bugs, so your mileage may vary.

	Much depends on the size of your installed base.
	If your code is only ever going to run on (say) 10,000
	systems, Murphy can actually be a really nice guy.
	Everything that can go wrong, will.
	Eventually.
	Perhaps in geologic time.

	But if your code is running on 20~billion systems,
	like the Linux kernel was said to be by late 2017,
	Murphy can be a real jerk!
	Everything that can go wrong, will, and it can go wrong
	really quickly!!!
}\QuickQuizEndB
%
\QuickQuizE{
	But aren't there a great many more formal-verification systems
	than are shown in
	\cref{tab:future:Formal Regression Scorecard}?
}\QuickQuizAnswerE{
	Indeed there are!
	This table focuses on those that Paul has used, but others are
	proving to be useful.
	Formal verification has been heavily used in the SEL4
	project~\cite{ThomasSewell2013L4binaryVerification},
	and its tools can now handle modest levels of concurrency.
	More recently, Catalin Marinas used Lamport's
	TLA tool~\cite{Lamport:2002:SST:579617} to locate some
	forward-progress bugs in the Linux kernel's queued spinlock
	implementation.
	Will Deacon fixed these bugs~\cite{WillDeacon2018qspinlock},
	and Catalin verified Will's
	fixes~\cite{CatalinMarinas2018qspinlockTLA}.

	Lighter-weight formal verification tools have been used heavily
	in production~\cite{JamesRLarus2004RightingSoftware,AlBessey2010BillionLoCLater,ByronCook2018FormalAmazon,CaitlinSadowski2018staticAnalysisGoogle,DinoDistefano2019FBstaticAnalysis}.
}\QuickQuizEndE
}

Once again, please note that this table rates these tools for use in
regression testing.
Just because many of them are a poor fit for regression testing does
not at all mean that they are useless, in fact,
many of them have proven their worth many times over.\footnote{
	For but one example, Promela was used to verify the file system
	of none other than the Curiosity Rover.
	Was \emph{your} formal verification tool used on software that
	currently runs on Mars???}
Just not for regression testing.

However, this might well change.
After all, formal verification tools made impressive strides in the 2010s.
If that progress continues, formal verification might well become an
indispensible tool in the parallel programmer's validation toolbox.
