% formal/regression.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\section{Formal Regression Testing?}
\label{sec:formal:Formal Regression Testing?}

Formal verification 은 일부 경우에 유용한 것으로 증명되었습니다만, hard-core
formal verification 이 리눅스 커널과 같이 복잡한 동시성을 가진 코드를 위한
자동화된 regression-test 도구에 포함될 수 있을 것인지에 대한 질문이 많습니다.
리눅스 커널 SRCU를 위한 이론적 증명은 이미 있지만~\cite{LanceRoy2017CBMC-SRCU},
이 테스트는 가장 간단한 RCU 구현들 가운데 하나의 작은 부분을 위한 테스트이고,
이걸 계속 변화하는 리눅스 커널에 맞춰 유지하기는 어려움이 증명되었습니다.
따라서 formal verification 을 리눅스 커널의 regression test 에 첫번째 멤버로
추가하기 위해 뭐가 필요할지 물어볼 가치가 있습니다.

다음은 좋은 시작이 될 수 있을
겁니다~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:
\iffalse

Formal verification has proven useful in some cases, but a pressing
open question is whether hard-core formal verification will ever be
included in automated regression-test suites for complex concurrent
code bases, such as the Linux kernel.
Although there is already a proof of concept for Linux-kernel
SRCU~\cite{LanceRoy2017CBMC-SRCU}, this test is for a small portion
of one of the simplest RCU implementations, and has proven difficult
to keep it caught up with the ever-changing Linux kernel.
It is therefore worth asking what would be required to incorporate
formal verification as first-class members of the Linux kernel's
regression tests.

The following list is a good
start~\cite[slide 34]{PaulEMcKenney2015DagstuhlVerification}:
\fi

\begin{enumerate}
\item	모든 필요한 변환은 자동화 되어야만 합니다.
\item	환경 (메모리 순서 규칙 포함) 은 올바르게 처리되어야만 합니다.
\item	메모리와 CPU 오버헤드는 받아들일 수 있을 만큼 적어야만 합니다.
\item	버그의 위치를 알려주는 정보가 제공되어야만 합니다.
\item	소스코드와 입력 이외의 정보는 너무 범위가 크지 않아야만 합니다.
\item	발견되는 버그는 해당 코드의 사용자들에 관련되어 있어야만 합니다.
\iffalse

\item	Any required translation must be automated.
\item	The environment (including memory ordering) must be correctly
	handled.
\item	The memory and CPU overhead must be acceptably modest.
\item	Specific information leading to the location of the bug
	must be provided.
\item	Information beyond the source code and inputs must be
	modest in scope.
\item	The bugs located must be relevant to the code's users.
\fi
\end{enumerate}

이 목록은 만들어져가는 중입니다만 Richard Bornat 의 격언에 비하면 간단합니다:
``Formal-verification 연구자들은 개발자들이 작성하는 코드를 그들이 작성한
언어로, 그들이 수행하는 환경에서 검증해야 한다.''
다음 섹션들은 앞의 요구사항들 각각에 대해 논해보고, 이어서 몇가지 도구들이 이
요구사항들을 얼만큼 만족시키고 있는지 점수를 매겨봅니다.
\iffalse

This list builds on, but is somewhat more modest than, Richard Bornat's
dictum: ``Formal-verification researchers should verify the code that
developers write, in the language they write it in, running in the
environment that it runs in, as they write it.''
The following sections discuss each of the above requirements, followed
by a section presenting a scorecard of how well a few tools stack up
against these requirements.
\fi

\subsection{Automatic Translation}
\label{sec:formal:Automatic Translation}

Promela 와 \co{spin} 이 설계에는 도움이 되지만, 여러분이 정형적으로 C-언어
프로그램을 regression-test 하려면, 여러분은 여러분의 코드를 다시 검증하고 싶을
때마다 Promela 로 일일이 손으로 변환시켜야만 합니다.
여러분의 코드가 매 60-90일마다 릴리즈되는 리눅스 커널 안에 있다면, 여러분은
매년 네번에서 여섯번씩 수작업으로 번환을 해야 할겁니다.
시간이 흐름에 따라, 사람의 실수가 생겨날 텐데, 이는 해당 검증이 소스 코드에
들어맞지 않음을 의미해서, 해당 검증이 쓸모없게 만듭니다.
반복된 검증은 해당 formal-verification 도구가 여러분의 코드를 직접 입력받을 수
있거나, 여러분의 코드를 검증에 필요한 형태로 자동으로 변환해주는 도구가 있을
것을 필요로 합니다.
\iffalse

Although Promela and \co{spin}
are invaluable design aids, if you need to formally regression-test
your C-language program, you must hand-translate to Promela each time
you would like to re-verify your code.
If your code happens to be in the Linux kernel, which releases every
60-90 days, you will need to hand-translate from four to six times
each year.
Over time, human error will creep in, which means that the verification
won't match the source code, rendering the verification useless.
Repeated verification clearly requires either that the formal-verification
tooling input your code directly, or that there be automatic translation
of your code to the form required for verification.
\fi

PPCMEM 과 \co{herd} 는 이론적으로 어셈블리 언어와 C++ 코드를 직접 입력받을 수
있습니다만, 이 도구들은 매우 작은 리트머스 테스트에서만 동작하여서,
일반적으로는 여러분의 메커니즘의 핵심을 수작업으로 추출해 내야만 함을
의미합니다.
Promela 와 \co{spin} 만큼, PPCMEM 과 \co{herd} 는 매우 유용합니다만, regression
suite 에 적합하지는 않습니다.

반면, \co{cbmc} 와 Nidhugg 는 합리적인 (여전히 매우 제한되어 있긴 하지만)
크기의 C 프로그램을 입력받을 수 있고, 그 기능이 계속 발전한다면, regression
test 에 훌륭한 추가물이 될 수도 있을 겁니다.

C 코드를 입력으로 받는 것의 한가지 단점은, 해당 컴파일러가 올바르다고
가정한다는 겁니다.
한가지 대안적인 접근법은 C 컴파일러가 만들어낸 바이너리를 입력으로 받음으로써,
관련된 컴파일러 버그들을 모두 파악해내는 겁니다.
이 방법은 여러 검증 시도에서 사용되었는데, SEL4
프로젝트~\cite{ThomasSewell2013L4binaryVerification} 가 특히 그랬습니다.
\iffalse

PPCMEM and \co{herd} can in theory directly input assembly language
and C++ code, but these tools work only on very small litmus tests,
which normally means that you must extract the core of your
mechanism---by hand.
As with Promela and \co{spin}, both PPCMEM and \co{herd} are
extremely useful, but they are not well-suited for regression suites.

In contrast, \co{cbmc} and Nidhugg can input C programs of reasonable
(though still quite limited) size, and if their capabilities continue
to grow, could well become excellent additions to regression suites.

One shortcoming of taking C code as input is that it assumes that the
compiler is correct.
An alternative approach is to take the binary produced by the C compiler
as input, thereby accounting for any relevant compiler bugs.
This approach has been used in a number of verification efforts,
perhaps most notably by the SEL4
project~\cite{ThomasSewell2013L4binaryVerification}.
\fi

\QuickQuiz{}
	SEL4 프로젝트에서 사용된 다양한 검증기들의 획기적인 성격이 있는데, 이
	챕터는 왜 이걸 더 깊게 다루지 않나요?
	\iffalse

	Given the groundbreaking nature of the various verifiers used
	in the SEL4 project, why doesn't this chapter cover them in
	more depth?
	\fi
\QuickQuizAnswer{
	SEL4 프로젝트에서 사용된 검증기들이 정말로 사용 가능한지에 대해서는
	의혹의 여지가 없습니다.
	하지만, SEL4 가 단일 CPU 프로젝트 외의 것들에 사용된 지는 (2017년에
	이르러) 2년밖에 되지 않았습니다.
	그리고 SEL4 가 멀티 프로세서 기능을 늘려가기 시작했지만, 리눅스 커널의
	기존의 Big Kernel Lock (BKL) 과 유사한, 매우 크게 락을 잡는 락킹 방식을
	사용하고 있습니다.
	SEL4 의 검증기를 병렬 프로그래밍 책에 더하는게 말이 되는 날도 올거라
	믿습니다만, 불행히도, 지금은 그때가 아닙니다.
	\iffalse

	There can be no doubt that the verifiers used by the SEL4
	project are quite capable.
	However, it has been only in the past couple of years
	(as of 2017) that SEL4 has been anything other than
	a single-CPU project.
	And although SEL4 is starting to gain multi-processor
	capabilities, it is currently using very coarse-grained
	locking that is similar to the Linux kernel's old
	Big Kernel Lock (BKL).
	There will hopefully come a day when it makes sense to add
	SEL4's verifiers to a book on parallel programming, but
	unfortunately, this is not yet that day.
	\fi
} \QuickQuizEnd

하지만, 소스나 바이너리로부터 직접 검증을 하는게 모두 사람의 변환 과정에서의
에러를 제거하는, 안정적인 regression test 에 매우 중요한 장점을 갖습니다.
\iffalse

However, verifying directly from either the source or binary both have the
advantage of eliminating human translation errors, which is critically
important for reliable regression testing.
\fi

\subsection{Environment}
\label{sec:formal:Environment}

Formal-verification 도구들이 각자의 환경을 올바르게 모델링 하는 것은 매우
중요합니다.
모두에게 너무나 흔한 생략은 메모리 모델로, Promela/spin 을 포함한 매우 많은
formal-verification 도구들이 순차적 일관성에 국한되어 있습니다.
Section~\ref{sec:formal:Is QRCU Really Correct?}
에 연관된 QRCU 의 경험은 중요한 교훈적 이야기입니다.

Promela 와 \co{spin} 은 순차적 일관성을 가정하는데, 이는
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering} 에서 볼 수 있듯이
최근의 컴퓨터 시스템에서는 좋은 맞춤이 아닙니다.
대조적으로, PPCMEM 과 \co{herd} 의 큰 강점 중 하나는 x86, ARM, Power, 그리고,
\co{herd} 의 경우에는 심지어 프로토타입 리눅스 커널 메모리
모델~\cite{JadeAlglave2017LWN-LKMM-1,JadeAlglave2017LWN-LKMM-2} 을 포함해
다양한 CPU 제품군들의 메모리 모델에 대한 모델링입니다.
\iffalse

It is critically important that formal-verification tools correctly
model their environment.
One all-too-common omission is the memory model, where a great
many formal-verification tools, including Promela/spin, are
restricted to sequential consistency.
The QRCU experience related in
Section~\ref{sec:formal:Is QRCU Really Correct?}
is an important cautionary tale.

Promela and \co{spin} assume sequential consistency, which is not a
good match for modern computer systems, as will be seen in
Chapter~\ref{chp:Advanced Synchronization: Memory Ordering}.
In contrast, one of the great strengths of PPCMEM and \co{herd}
is their detailed modeling of various CPU families memory models,
including x86, ARM, Power, and, in the case of \co{herd},
even a prototype Linux-kernel memory
model~\cite{JadeAlglave2017LWN-LKMM-1,JadeAlglave2017LWN-LKMM-2}.
\fi

\co{cbmc} 와 Nidhugg 도구들은 메모리 모델을 선택할 수 있는 기능을 일부
제공합니다만, PPCMEM 과 \co{herd} 만큼 다양한 기능은 아닙니다.
하지만, 시간이 흐름에 따라 더 커다란 규모의 도구들이 더 많은 메모리 모델을
받아들일 가능성이 있습니다.

장기적으로 보면, formal-verification 도구들이 I/O 를 포함하는게 도움이
될겁니다~\cite{PaulEMcKenney2016LinuxKernelMMIO}만, 그렇게 되기전에 많은 시간이
필요할 겁니다.
\iffalse

The \co{cbmc} and Nidhugg tools provide some ability to select
memory models, but do not provide the variety that PPCMEM and
\co{herd} do.
However, it is likely that the larger-scale tools will adopt
a greater variety of memory models as time goes on.

In the longer term, it would be helpful for formal-verification
tools to include I/O~\cite{PaulEMcKenney2016LinuxKernelMMIO},
but it may be some time before this comes to pass.
\fi

\subsection{Overhead}
\label{sec:formal:Overhead}

거의 모든 하드코어 formal-verification 도구들이 기본적으로 기하급수적으로
증가하는 오버헤드를 갖습니다만, 그 정도에는 차이가 있습니다.

PPCMEM 은 설계적으로 최적화 되어 있지 않은데, 관심의 메모리 모델이 실제로
정확하게 표현되었음에 대한 확증을 주기 위해서입니다.
\co{herd} 는 더 적극적으로 최적화를 하며, 따라서
Section~\ref{sec:formal:Axiomatic Approaches} 에서 설명한대로, PPCMEM 보다 열배
이상 빠릅니다.
그러나, PPCMEM 과 \co{herd} 모두 커다란 코드보다는 매우 작은 리트머스 테스트를
목표로 삼습니다.
\iffalse

Almost all hard-core formal-verification tools are exponential
in nature, however, there are differences in degree.

PPCMEM by design is unoptimized, in order to provide greater assurance
that the memory models of interest are in fact accurately represented.
The \co{herd} tool optimizes more aggressively, and so as described in
Section~\ref{sec:formal:Axiomatic Approaches}, is orders of magnitude
faster than PPCMEM.
Nevertheless, both PPCMEM and \co{herd} target very small litmus tests
rather than larger bodies of code.
\fi

반면에, Promela/\co{spin}, \co{cbmc}, 그리고 Nidhugg 는 더 커다란 코드 (무언가)
를 위해 설계되었습니다.
Promela/\co{spin} 은 Curiosity rover 의
파일시스템~\cite{DBLP:journals/amai/GroceHHJX14} 을 검증하기 위해 사용되었고,
앞에서도 이야기되었듯 \co{cbmc} 와 Nidhugg 는 모두 리눅스 커널 RCU 에
적용되었습니다.

휴리스틱의 발전이 지난 25년간의 속도로 지속된다면, 우리는 formal verification
의 오버헤드의 많은 감소를 예상할 수 있습니다.
그렇다고는 해도, 오버헤드의 조합적인 증가는 여전히 조합적 증가인데, 이는
휴리스틱의 개선이 지속되든 안되든 검증될 수 있는 프로그램의 크기는 여전히 대폭
제한할 겁니다.
\iffalse

In contrast, Promela/\co{spin}, \co{cbmc}, and Nidhugg are designed for
(somewhat) larger bodies of code.
Promela/\co{spin} was used to verify the Curiosity rover's
filesystem~\cite{DBLP:journals/amai/GroceHHJX14} and, as noted earlier,
both \co{cbmc} and Nidhugg were appled to Linux-kernel RCU.

If advances in heuristics continue at the rate of the past quarter
century, we can look forward to large reductions in overhead for
formal verification.
That said, combinatorial explosion is still combinatorial explosion,
which would be expected to sharply limit the size of programs that
could be verified, with or without continued improvements in
heuristics.
\fi

하지만, 조합적 폭증의 반대면은 Macedon 의 Philip II 의 영원한 충고입니다:
``분할하고 지배하라.''
커다란 프로그램이 분할될 수 있고 그 조각들이 검증될 수 있다면, 그 결과는 조합적
\emph{폭발}~\cite{PaulEMcKenney2011Verico} 일 겁니다.
분할을 할 자연스러운 장소는 API 경계인데, 예를 들어, 락킹 기능들의 그것입니다.
그러면 하나의 검증 패스는 락킹 구현이 올바른지 검증하고, 추가적인 검증 패스들은
락킹 API 들의 올바른 사용을 검증할 수 있을 겁니다.
\iffalse

However, the flip side of combinatorial explosion is Philip II of
Macedon's timeless advice: ``Divide and rule.''
If a large program can be divided and the pieces verified, the result
can be combinatorial \emph{implosion}~\cite{PaulEMcKenney2011Verico}.
One natural place to divide is on API boundaries, for example, those
of locking primitives.
One verification pass can then verify that the locking implementation
is correct, and additional verification passes can verify correct
use of the locking APIs.
\fi

\begin{listing}[tbp]
{ \scriptsize
\begin{verbbox}[\LstLineNo]
C C-SB+l-o-o-u+l-o-o-u-C

{
}

P0(int *sl, int *x0, int *x1)
{
  int r2;
  int r1;

  r2 = cmpxchg_acquire(sl, 0, 1);
  WRITE_ONCE(*x0, 1);
  r1 = READ_ONCE(*x1);
  smp_store_release(sl, 0);
}

P1(int *sl, int *x0, int *x1)
{
  int r2;
  int r1;

  r2 = cmpxchg_acquire(sl, 0, 1);
  WRITE_ONCE(*x1, 1);
  r1 = READ_ONCE(*x0);
  smp_store_release(sl, 0);
}

filter (0:r2=0 /\ 1:r2=0)
exists (0:r1=0 /\ 1:r1=0)
\end{verbbox}
}
\centering
\theverbbox
\caption{Emulating Locking with \tco{cmpxchg_acquire()}}
\label{lst:formal:Emulating Locking with cmpxchg}
\end{listing}

\begin{table}[tbh]
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]}
	\toprule
	\multicolumn{1}{c}{\# Threads} & \multicolumn{1}{c}{Locking} &
			\multicolumn{1}{c}{\tco{cmpxchg_acquire}} \\
	\midrule
	2 & 0.004 &  0.022 \\
	3 & 0.041 &  0.743 \\
	4 & 0.374 & 59.565 \\
	5 & 4.905 &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance (s)}
\label{tab:formal:Emulating Locking: Performance (s)}
\end{table}

이 접근법의 성능 이득은 리눅스 커널 메모리
모델~\cite{JadeAlglave2017LWN-LKMM-1,JadeAlglave2017LWN-LKMM-2} 을 통해 보여질
수 있습니다.
이 모델은 \co{spin_lock()} 과 \co{spin_unlock()} 기능을 제공하지만, 이 기능들은
또한
Listing~\ref{lst:formal:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} 와 \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus})
에 보인대로 \co{cmpxchg_acquire()} 와 \co{smp_store_release()} 를 사용해서도
에뮬레이션 될 수 있습니다.
Table~\ref{tab:formal:Emulating Locking: Performance (s)}
은 이 모델의 \co{spin_lock()} 과 \co{spin_unlock()} 을 사용할 때의 성능과
확장성을 이 기능들을 앞의 리스트에 보인대로 에뮬레이션 했을 때와 비교합니다.
이 차이는 무의미하지 않습니다: 네개 프로세스에서, 이 모델은 에뮬레이션보다 백배
이상 빠릅니다!
\iffalse

The performance benefits of this approach can be demonstrated using
the Linux-kernel memory
model~\cite{JadeAlglave2017LWN-LKMM-1,JadeAlglave2017LWN-LKMM-2}.
This model provides \co{spin_lock()} and \co{spin_unlock()}
primitives, but these primitives can also be emulated using
\co{cmpxchg_acquire()} and \co{smp_store_release()}, as shown in
Listing~\ref{lst:formal:Emulating Locking with cmpxchg}
(\path{C-SB+l-o-o-u+l-o-o-*u.litmus} and \path{C-SB+l-o-o-u+l-o-o-u*-C.litmus}).
Table~\ref{tab:formal:Emulating Locking: Performance (s)}
compares the performance and scalability of using the model's
\co{spin_lock()} and \co{spin_unlock()} against emulating these
primitives as shown in the listing.
The difference is not insignificant: At four processes, the model
is more than two orders of magnitude faster than emulation!
\fi

\QuickQuiz{}
	Listing~\ref{lst:formal:Emulating Locking with cmpxchg}
	의 line~26 에서 해당 컨디션을 그냥 \co{exists} 절에 넣는 대신 왜 별개의
	\co{filter} 커맨드를  사용하나요?
	그리고 \co{cmpxchg_acquire()} 대신 \co{xchg_acquire()} 를 사용하는게 더
	간단하지 않겠어요?
	\iffalse

	Why bother with a separate \co{filter} command on line~28 of
	Listing~\ref{lst:formal:Emulating Locking with cmpxchg}
	instead of just adding the condition to the \co{exists} clause?
	And wouldn't it be simpler to use \co{xchg_acquire()} instead
	of \co{cmpxchg_acquire()}?
	\fi
\QuickQuizAnswer{
	이 \co{filter} 절은 \co{herd} 툴이 \co{exists} 절보다 더 이른 처리
	단계에서 수행을 멈추게 해주는데, 이는 상당한 속도 향상을 가져다 줍니다.
	\iffalse

	The \co{filter} clause causes the \co{herd} tool to discard
	executions at an earlier stage of processing than does
	the \co{exists} clause, which provides significant speedups.
	\fi

\begin{table}[tbh]
\rowcolors{7}{lightgray}{}
\renewcommand*{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{S[table-format=1.0]S[table-format=1.3]S[table-format=2.3]
		S[table-format=3.3]S[table-format=2.3]S[table-format=3.3]}
	\toprule
	& & \multicolumn{2}{c}{\tco{cmpxchg_acquire()}}
		& \multicolumn{2}{c}{\tco{xchg_acquire()}} \\
	\cmidrule(l){3-4} \cmidrule(l){5-6}
	\multicolumn{1}{c}{\#} & \multicolumn{1}{c}{Lock}
		& \multicolumn{1}{c}{\tco{filter}}
			& \multicolumn{1}{c}{\tco{exists}}
				& \multicolumn{1}{c}{\tco{filter}}
					& \multicolumn{1}{c}{\tco{exists}} \\
	\cmidrule{1-1} \cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-6}
	2 & 0.004 &  0.022 &   0.039 &  0.027 &  0.058 \\
	3 & 0.041 &  0.743 &   1.653 &  0.968 &  3.203 \\
	4 & 0.374 & 59.565 & 151.962 & 74.818 & 500.96 \\
	5 & 4.905 &        &         &        &        \\
	\bottomrule
\end{tabular}
\caption{Emulating Locking: Performance Comparison (s)}
\label{tab:formal:Emulating Locking: Performance Comparison (s)}
\end{table}

	\co{xchg_acquire()} 어토믹 오퍼레이션은 락 획득이 성공하든 실패하든
	쓰기를 하게 되는데, 이는 \co{xchg_acquire()} 를 사용하는 모델은 락 획득
	실패의 경우에는 쓰기를 하지 않을 \co{cmpxchg_acquire()} 를 사용하는
	모델보다 많은 오퍼레이션을 갖게 될 것을 의미합니다.
	더 많은 쓰기는
	Table~\ref{tab:formal:Emulating Locking: Performance Comparison (s)}
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, 그리고
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}) 에서 보이는 것과 같이 더 많은
	조합의 폭증을 의미합니다.
	이 테이블은 \co{cmpxchg_acquire()} 가 \co{xchg_acquire()} 보다 성능이
	높음을 , 그리고 \co{filter} 절의 사용이 \co{exists} 절의 사용보다
	성능이 높음을 보입니다.
	\iffalse

	As for \co{xchg_acquire()}, this atomic operation will do a
	write whether or not lock acquisition succeeds, which means
	that a model using \co{xchg_acquire()} will have more operations
	than one using \co{cmpxchg_acquire()}, which won't do a write
	in the failed-acquisition case.
	More writes means more combinatorial to explode, as shown in
	Table~\ref{tab:formal:Emulating Locking: Performance Comparison (s)}
	(\path{C-SB+l-o-o-u+l-o-o-*u.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-C.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-CE.litmus},
	\path{C-SB+l-o-o-u+l-o-o-u*-X.litmus}, and
	\path{C-SB+l-o-o-u+l-o-o-u*-XE.litmus}).
	This table clearly shows that \co{cmpxchg_acquire()}
	outperforms \co{xchg_acquire()} and that use of the
	\co{filter} clause outperforms use of the \co{exists} clause.
	\fi
} \QuickQuizEnd

도구들이 자동으로 커다란 프로그램들을 쪼개고, 그 조각들을 검증하고, 그 조각들의
조합을 검증한다면 물론 매우 유용할 겁니다.
그렇게 되기 전까지는, 커다란 프로그램의 검증은 상당한 수작업을 필요로 할
겁니다.
이 작업은 매 릴리즈마다 반복된 검증을 안정적으로 해내기에 낫고 결국 continuous
integration 에 잘 맞는 방식인 스크립팅으로 중개를 하는게 바람직할겁니다.

어떤 경우든, 우린 formal-verification 기능이 시간에 따라 증가하길 계속할 것을
기대할 수 있습니다.
\iffalse

It would of course be quite useful for tools to automatically divide
up large programs, verify the pieces, and then verify the combinations
of pieces.
In the meantime, verification of large programs will require significant
manual intervention.
This intervention will preferably mediated by scripting, the better to
reliably carry out repeated verifications on each release, and
preferably eventually in a manner well-suited for continuous integration.

In any case, we can expect formal-verification capabilities to continue
to increase over time.
\fi

\subsection{Locate Bugs}
\label{sec:formal:Locate Bugs}

Any software artifact of any size contains bugs.
Therefore, a formal-verification tool that reports only the
presence or absence of bugs is not particularly useful.
What is needed is a tool that gives at least \emph{some} information
as to where the bug is located and the nature of that bug.

The \co{cbmc} output includes a traceback mapping back to the source
code, similar to Promela/spin's, as does Nidhugg.
Of course, these tracebacks can be quite long, however, it is almost
always worthwhile to analyze them.
Although doing so can be tedious, it is usually quite a bit faster
and more pleasant than locating bugs the old-fashioned way.

\subsection{Minimal Scaffolding}
\label{sec:formal:Minimal Scaffolding}

In the old days, formal-verification researchers demanded a full
specification against which the software would be verified.
Unfortunately, a mathematically rigorous specification might well
be larger than the actual code, and each line of specification
is just as likely to contain bugs as is each line of code.
A formal verification effort proving that the code faithfully
implemented the specification would be a proof of bug-for-bug
compatibility between the two, which might not be the intended
result.

Worse yet, the requirements for a number of software artifacts,
including Linux-kernel RCU, are empirical in
nature~\cite{PaulEMcKenney2015RCUreqts1,PaulEMcKenney2015RCUreqts2,PaulEMcKenney2015RCUreqts3}.
For this common type of software, a complete specification is a
polite fiction.

This situation might cause one to give up all hope of formal verification
of real-world software artifacts, but it turns out that there is
quite a bit that can be done.
For example, design and coding rules can act as a partial specification,
as can assertions contained in the code.
And in fact formal-verification tools such as \co{cbmc} and Nidhugg
both check for assertions that can be triggered, implicitly treating
these assertions as part of the specification.
However, the assertions are also part of the code, which makes it less
likely that they will become obsolete, especially if the code is
also subjected to stress tests.\footnote{
	And you \emph{do} stress-test your code, don't you?}
The \co{cbmc} tool also checks for array-out-of-bound references,
thus implicitly adding this to the specification.

This implicit-specification approach makes quite a bit of sense, particularly
if you look at formal verification not as a full proof of correctness,
but rather an alternative form of validation with a different set of
strengths and weaknesses that other forms of validation, such as testing.
From this viewpoint, software will always have bugs, and therefore any
tool of any kind that helps to find those bugs is a very good thing
indeed.

\subsection{Relevant Bugs}
\label{sec:formal:Relevant Bugs}

Finding bugs---and fixing them---is of course the whole point of any
type of validation effort.
Clearly, false positives are to be avoided.
But even in the absense of false positives, there are bugs and there are bugs.

For example, suppose that a software artifact had exactly 100 remaining
bugs, each of which manifested on average once every million years
of runtime.
Suppose further that an omniscient formal-verification tool located
all 100 bugs, which the developers duly fixed.
What happens to the reliability of this software artifact?

The perhaps surprising answer is that the reliability \emph{decreases}.

To see this, keep in mind that historical experience indicates that
about 7\,\% of fixes introduce a new bug~\cite{RexBlack2012SQA}.
Therefore, fixing the 100 bugs, which had a combined mean time to failure
(MTBF) of about 10,000 years, will introduce seven more bugs.
Historical statistics indicate that each new bug will have an MTBF
much less than 70,000 years.
This in turn suggests that the combined MTBF of these seven new bugs
will most likely be much less than 10,000 years, which in turn means
that the well-intentioned fixing of the original 100 bugs actually
decreased the reliability of the overall software.

\QuickQuiz{}
	How do we know that the MTBFs of known bugs is a good estimate
	of the MTBFs of bugs that have not yet been located?
\QuickQuizAnswer{
	We don't, but it does not matter.

	To see this, note that the 7\,\% figure only applies to injected
	bugs that were subsequently located: It necessarily ignores
	any injected bugs that were never found.
	Therefore, the MTBF statistics of known bugs is likely to be
	a good approximation of that of the injected bugs that are
	subsequently located.

	A key point in this whole section is that we should be more
	concerned about bugs that inconvenience users than about
	other bugs that never actually manifest.
	This of course is \emph{not} to say that we should completely
	ignore bugs that have not yet inconvenienced users, just that
	we should properly prioritize our efforts so as to fix the
	most important and urgent bugs first.
} \QuickQuizEnd

\QuickQuiz{}
	But the formal-verification tools should immediately find all the
	bugs introduced by the fixes, so why is this a problem?
\QuickQuizAnswer{
	It is a problem because real-world formal-verification tools
	(as opposed to those that exist only in the imaginations of
	the more vociferous proponents of formal verification) are
	not omniscient, and thus are only able to locate certain types
	of bugs.
	For but one example, formal-verification tools are unlikely to
	spot a bug corresponding to an omitted assertion or, equivalently,
	a bug corresponding to an omitted portion of the specification.
} \QuickQuizEnd

Worse yet, imagine another software artifact with one bug that fails
once every day on average and 99 more that fail every million years
each.
Suppose that a formal-verification tool located the 99 million-year
bugs, but failed to find the one-day bug.
Fixing the 99 bugs located will take time and effort, likely slightly
decrease reliability, and do nothing at all about the pressing
each-day failure that is likely causing much embarrassment and perhaps
much worse besides.

Therefore, it would be best to have a validation tool that
preferentially located the most troublesome bugs.

This might sound like too much to ask, but it is what is really
required if we are to actually increase software reliability.

\subsection{Formal Regression Scorecard}
\label{sec:formal:Formal Regression Scorecard}

\begin{table*}[tbh]
% \rowcolors{6}{}{lightgray}
%\renewcommand*{\arraystretch}{1.1}
\small
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccccccccc}
	\toprule
	& & Promela & & PPCMEM & & \tco{herd} & & \tco{cbmc} & & Nidhugg \\
	\midrule
	(1) Automated &
		& \cellcolor{red!50} &
			& \cellcolor{orange!50} &
				& \cellcolor{orange!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(2) Environment &
		& \cellcolor{red!50} (MM) &
			& \cellcolor{green!50} &
				& \cellcolor{blue!50} &
					& \cellcolor{yellow!50} (MM) &
						& \cellcolor{orange!50} (MM) \\
	\addlinespace[3pt]
	(3) Overhead &
		& \cellcolor{yellow!50} &
			& \cellcolor{red!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{yellow!50} (SAT) &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(4) Locate Bugs &
		& \cellcolor{yellow!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{green!50} &
						& \cellcolor{green!50} \\
	\addlinespace[3pt]
	(5) Minimal Scaffolding &
		& \cellcolor{green!50} &
			& \cellcolor{yellow!50} &
				& \cellcolor{yellow!50} &
					& \cellcolor{blue!50} &
						& \cellcolor{blue!50} \\
	\addlinespace[3pt]
	(6) Relevant Bugs &
		& \cellcolor{yellow!50} ??? &
			& \cellcolor{yellow!50} ??? &
				& \cellcolor{yellow!50} ??? &
					& \cellcolor{yellow!50} ??? &
						& \cellcolor{yellow!50} ??? \\
	\bottomrule
\end{tabular}
\caption{Formal Regression Scorecard}
\label{tab:formal:Formal Regression Scorecard}
\end{table*}

Table~\ref{tab:formal:Formal Regression Scorecard}
shows a rough-and-ready scorecard for the formal-verification tools
covered in this chapter.
Shorter wavelengths are better than longer wavelengths.

Promela requires hand translation and supports only sequential
consistency, so its first two cells are red.
It has reasonable overhead (for formal verification, anyway)
and provides a traceback, so its next two cells are yellow.
Despite requiring hand translation, Promela handles assertions
in a natural way, so its fifth cell is green.

PPCMEM usually requires hand translation due to the small size of litmus
tests that it supports, so its first cell is orange.
It handles several memory models, so its second cell is green.
Its overhead is quite high, so its third cell is red.
It provides a graphical display of relations among operations, which
is not as helpful as a traceback, but is still quite useful, so its
fourth cell is yellow.
It requires constructing an \co{exists} clause and cannot take
intra-process assertions, so its fifth cell is also yellow.

The \co{herd} tool has size restrictions similar to those of PPCMEM,
so \co{herd}'s first cell is also orange.
It supports a wide variety of memory models, so its second cell is blue.
It has reasonable overhead, so its third cell is yellow.
Its bug-location and assertion capabilities are quite similar to those
of PPCMEM, so \co{herd} also gets yellow for the next two cells.

The \co{cbmc} tool inputs C code directly, so its first cell is blue.
It supports a few memory models, so its second cell is yellow.
It has reasonable overhead, so its third cell is also yellow, however,
perhaps SAT-solver performance will continue improving.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

Nidhugg also inputs C code directly, so its first cell is also blue.
It supports only a couple of memory models, so its second cell is orange.
Its overhead is quite low (for formal-verification), so its
third cell is green.
It provides a traceback, so its fourth cell is green.
It takes assertions directly from the C code, so its fifth cell is blue.

So what about the sixth and final row?
It is too early to tell how any of the tools do at finding the right bugs,
so they are all yellow with question marks.

\QuickQuiz{}
	How would testing stack up in the scorecard shown in
	Table~\ref{tab:formal:Formal Regression Scorecard}?
\QuickQuizAnswer{
	It would be blue all the way down, with the possible
	exception of the third row (overhead) which might well
	be marked down for testing's difficulty finding
	improbable bugs.

	On the other hand, improbable bugs are often also
	irrelevant bugs, so your mileage may vary.

	Much depends on the size of your installed base.
	If your code is only ever going to run on (say) 10,000
	systems, Murphy can actually be a really nice guy.
	Everything that can go wrong, will.
	Eventually.
	Perhaps in geologic time.

	But if your code is running on 20~billion systems,
	Murphy can be a real jerk!
	Everything that can go wrong, will, and it can go wrong
	really quickly!!!
} \QuickQuizEnd

Once again, please note that this table rates these tools for use in
regression testing.
Just because many of them are poor fit for regression testing does
not at all mean that they are useless, in fact,
many of them have proven their worth many times over.\footnote{
	For but one example, Promela was used to verify the file system
	of none other than the Curiosty Rover.
	Was \emph{your} formal verification tool used on a Mars rover?}
Just not for regression testing.
