% formal/formal.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Formal Verification}{Formal Verification}{qqzformal}
%
\Epigraph{Beware of bugs in the above code; I have only proved it correct,
	  not tried it.}{\emph{Donald Knuth}}

\OriginallyPublished{Chapter}{chp:Formal Verification}{Formal Verification}{Linux Weekly News}{PaulEMcKenney2007QRCUspin,PaulEMcKenney2008dynticksRCU,PaulEMcKenney2011ppcmem}

병렬 알고리즘은 작성하기도 어렵지만 디버깅 하기는 더 어렵습니다.
테스트는 필수적이지만 치명적인 경주 조건이 극단적으로 낮은 발생확률을 가질 수도
있기 때문에 충분치 못합니다.
정확성 증명은 가치있을 수 있지만 종국에는 원래의 알고리즘만큼이나 사람의 오류에
취약합니다.
또한, 정확성 증명은 여러분의 가정, 요구사항 상의 부족함, 아래의 소프트웨어와
하드웨어 기능에 대한 잘못된 이해, 또는 여러분이 증명을 구성하는데 있어 생각치
않은 오류 등에 있는 오류는 찾을 거라 예상될 수 없습니다.
이는 정형적 방법들이 테스트를 대체할 수는 없음을 의미합니다.
그러나, 정형적 방법들은 여러분의 검증 도구상자에 가치있는 추가물품이 될 수
있습니다.

\iffalse

Parallel algorithms can be hard to write, and even harder to debug.
Testing, though essential, is insufficient, as fatal race conditions
can have extremely low probabilities of occurrence.
Proofs of correctness can be valuable, but in the end are just as
prone to human error as is the original algorithm.
In addition, a proof of correctness cannot be expected to find errors
in your assumptions, shortcomings in the requirements,
misunderstandings of the underlying software or hardware primitives,
or errors that you did not think to construct a proof for.
This means that formal methods can never replace testing.
Nevertheless, formal methods can be a valuable addition to your validation
toolbox.

\fi

모든 경주 조건을 어떻게든 찾아내는 도구를 갖는건 무척 도움이 될 겁니다.
그런 도구가 여럿 존재하는데, 예를 들어
\cref{sec:formal:State-Space Search} 는 범용 상태 공간 탐색 도구인 Promela 와
Spin 을 소개하고,
\cref{sec:formal:Special-Purpose State-Space Search}
은 비슷하게 특수 목적 ppcmem 도구를 소개하며,
\cref{sec:formal:Axiomatic Approaches}
는 공리적 접근법의 예를 알아보며,
\cref{sec:formal:SAT Solvers}
는 간단히 SAT solver 들을 살펴보며,
\cref{sec:formal:Stateless Model Checkers}
는 짧게 stateless 모델 검사기를 알아보고,
\cref{sec:formal:Summary}
는 병렬 알고리즘을 검증하기 위한 정형적 검증 도구들의 사용을 요약하며,
마지막으로
\cref{sec:formal:Choosing a Validation Plan}
에서는 얼마나 많은 그리고 어떤 종류의 검증을 주어진 소프트웨어 프로젝트에
적용해야 할지 이야기 합니다.

\iffalse

It would be very helpful to have a tool that could somehow locate
all race conditions.
A number of such tools exist, for example,
\cref{sec:formal:State-Space Search} provides an
introduction to the general-purpose state-space search tools Promela and Spin,
\cref{sec:formal:Special-Purpose State-Space Search}
similarly introduces the special-purpose ppcmem tool,
\cref{sec:formal:Axiomatic Approaches}
looks at an example axiomatic approach,
\cref{sec:formal:SAT Solvers}
briefly overviews SAT solvers,
\cref{sec:formal:Stateless Model Checkers}
briefly overviews stateless model checkers,
\cref{sec:formal:Summary}
sums up use of formal-verification tools for verifying parallel algorithms,
and finally
\cref{sec:formal:Choosing a Validation Plan}
discusses how to decide how much and what type of validation to apply
to a given software project.

\fi

\input{formal/spinhint}
\input{formal/dyntickrcu}
\input{formal/ppcmem}
\input{formal/axiomatic}
\input{formal/sat}
\input{formal/stateless}

\section{Summary}
\label{sec:formal:Summary}
%
\epigraph{Western thought has focused on True-False;
	  it is high time to shift to Robust-Fragile.}
	 {\emph{Nassim Nicholas Taleb, summarized}}
% Full quote:
% Since Plato, Western thought and the theory of knowledge has focused on
% the notions of True-False; as commendable as that was, it is high time
% to shift the concern to Robust-Fragile, and social epistemology to the
% more serious problem of Sucker-Nonsucker.

이 챕터에서 이야기한 정형 검증 기법들은 작은 병렬 알고리즘을 검증하는데 매우
강력한 도구입니다만 그게 여러분의 도구상자의 유일한 도구여선 안됩니다.
정형 검증에 전념한 수십년의 세월에도 불구하고 테스트는 거대 병렬 소프트웨어
시스템의 검증 주력 도구로
남아있습니다~\cite{JonathanCorbet2006lockdep,DaveJones2011Trinity,PaulEMcKenney2016Formal}.

그렇지만 이게 영원할 거라는 보장은 없습니다.
이를 위해, 2017년 기준으로 200억개가 넘는 리눅스 커널 인스턴스가 있을 것으로
추정됨을 생각해 보십시오.
리눅스 커널이 평균 백만년에 한번 일어나는 버그를 가지고 있다고 해봅시다.
앞 챕터의 끝에서 이야기 되었듯, 이 버그는 이 전체 설치 기반에서 \emph{매일}
50번씩 나타날 겁니다.
하지만 대부분의 정형 검증 기법은 매우 작은 코드기반에서만 사용될 수 있다는
사실도 여전합니다.
그래서 동시성 코드를 짜는 사람들은 뭘 해야 할까요?

첫번째 버그, 첫번째 연관된 버그, 마지막 연관된 버그, 그리고 마지막 버그를
찾는다는 관점에서 생각해 보세요.

\iffalse

The formal-verification techniques described in this chapter
are very powerful tools for validating small
parallel algorithms, but they should not be the only tools in your toolbox.
Despite decades of focus on formal verification, testing remains the
validation workhorse for large parallel software
systems~\cite{JonathanCorbet2006lockdep,DaveJones2011Trinity,PaulEMcKenney2016Formal}.

It is nevertheless quite possible that this will not always be the case.
To see this, consider that there is estimated to be more than twenty
billion instances of the Linux kernel as of 2017.
Suppose that the Linux kernel has a bug that manifests on average every million
years of runtime.
As noted at the end of the preceding chapter, this bug will be appearing
more than 50 times \emph{per day} across the installed base.
But the fact remains that most formal validation techniques can be used
only on very small codebases.
So what is a concurrency coder to do?

Think in terms of finding the first bug, the first relevant bug, the
last relevant bug, and the last bug.

\fi

첫번째 버그는 보통 조사나 컴파일러 진단에 의해 찾아집니다.
갈수록 정교해지는 컴파일러 진단 기능은 경량화된 일종의 정형 검증을 제공하긴
하지만, 그것들을 그런 관점에서 생각하는 건 흔치 않습니다.
이는 한편으로는 부분적으로 ``내가 그걸 사용한다면, 그건 정형 검증일 수 없다''
고 말하는 사용자의 이상한 편견 때문이고, 다른 한편으로는 컴파일러 진단과 검증
연구 사이의 큰 차이 때문입니다.

첫번째 연관된 버그는 조사나 컴파일러 진단을 통해 찾아질 수도 있지만, 이 두개의
단계가 오탈자나 거짓 양성만을 찾는건 일반적이지 않습니다.
어느 쪽이던, 많은 연관된 버그들, 즉, 실제로 제품 환경에서 발생할 수 있는
버그들은 테스트를 통해 발견됩니다.

테스트가 예상된 또는 실제 사용 시나리오에 의해 만들어지는 경우, 마지막 연관
버그가 테스트를 통해 찾아지는 건 드물지 않습니다.
이 상황은 정형 검증에 대한 완전한 각하의 동기를 유발할 수도 있겠으나, 연관 없는
버그들은 black-hat 공격 덕분에 가장 피곤한 순간에 연관되게 나타나는 짜증나는
습관을 갖고 있습니다.
따라서 계속해서 그 비중이 늘어나고 있는 보안에 치명적인 소프트웨어에서는 마지막
버그를 찾고 고치기 위한 강한 동기가 있습니다.
테스트는 마지막 버그를 찾을 수 없으며, 따라서 정형 검증의 역할이 존재할 것이며,
정형 검증은 그 역할에 있어서 성장을 지속할 것을 가정할 수 있습니다.
이 챕터가 보였듯, 현재의 정형 검증 시스템은 상당한 한계를 가지고 있습니다.

\iffalse

The first bug is normally found via inspection or compiler diagnostics.
Although the increasingly sophisticated compiler diagnostics comprise
a lightweight sort of formal verification, it is not common to think of
them in those terms.
This is in part due to an odd practitioner prejudice which says ``If I am
using it, it cannot be formal verification'' on the one hand, and a large
gap between compiler diagnostics and verification research on the other.

Although the first relevant bug might be located via inspection or
compiler diagnostics, it is not unusual for these two steps to find
only typos and false positives.
Either way, the bulk of the relevant bugs, that is, those bugs that
might actually be encountered in production, will often be found via testing.

When testing is driven by anticipated or real use cases, it is not
uncommon for the last relevant bug to be located by testing.
This situation might motivate a complete rejection of formal verification,
however, irrelevant bugs have an annoying habit of suddenly becoming relevant
at the least convenient moment possible, courtesy of black-hat attacks.
For security-critical software, which appears to be a continually
increasing fraction of the total, there can thus be strong motivation
to find and fix the last bug.
Testing is demonstrably unable to find the last bug, so there is a
possible role for formal verification, assuming, that is, that
formal verification proves capable of growing into that role.
As this chapter has shown, current formal verification systems are
extremely limited.

\fi

\QuickQuiz{
	하지만 충분히 낮은 단계의 소프트웨어는 모든 의도와 목적에 있어 black
	hat 에 착취당하는데 면역이 있어야 하지 않나요?

	\iffalse

	But shouldn't sufficiently low-level software be for all intents
	and purposes immune to being exploited by black hats?

	\fi

}\QuickQuizAnswer{
	불행히도, 아닙니다.

	한때 Paul E. McKenny 는 리눅스 커널 RCU 가 그런 공격으로부터 면역이
	있다고 느꼈으나, Row Hammer 의 발전은 그렇지 않음을 그에게 보였습니다.
	어쨌건, black hat 이 시스템의 DRAM 을 공격할 수 있다면, 모든 저수준
	소프트웨어를 공격할 수 있는데, RCU 조차도 포함됩니다.

	그리고 2018년, 이 가능성은 이론적 예측의 계곡에서 객관적 진실의 어렵고
	빠른 계곡으로 옮겨졌습니다~\cite{McKenney:2019:CRS:3319647.3325836}.

	\iffalse

	Unfortunately, no.

	At one time, Paul E. McKenny felt that Linux-kernel RCU
	was immune to such exploits, but the advent of Row Hammer
	showed him otherwise.
	After all, if the black hats can hit the system's DRAM,
	they can hit any and all low-level software, even including RCU\@.

	And in 2018, this possibility passed from the realm of
	theoretical speculation into the hard and fast realm of
	objective reality~\cite{McKenney:2019:CRS:3319647.3325836}.

	\fi

}\QuickQuizEnd

정형적 검증은 종종 테스트보다 이용되기가 훨씬 어려움을 유의하시기 바랍니다.
이는 부분적으로 통념적 발언이며, 정형 검증이 더 익숙해져서 사용하기 쉬워지는
날이 오길 바랄 이유가 됩니다.
그러나, 매우 간단한 테스트 도구는 임의의 거대 소프트웨어 시스템에 있는 심각한
버그를 찾을 수 있습니다.
대조적으로, 정형 검증을 적용하는데 필요한 노력은 시스템 크기가 커질수록
극적으로 증가하는 것으로 보입니다.

저는 더도 덜도 아니고 지난 30년간 때때로 정형 검증을 사용하며 그 강력함을
느꼈는데, 전체 소프트웨어를 구성하는 작은 부분에 대한 설계 시점 검증이 그런
때였습니다.
더 큰 전체 소프트웨어는 물론 테스트를 통해 검증되었습니다.

\iffalse

Please note that formal verification is often much harder to use than
is testing.
This is in part a cultural statement, and there is reason to hope
that formal verification will be perceived to be easier with increased
familiarity.
That said, very simple test harnesses can find significant bugs in arbitrarily
large software systems.
In contrast, the effort required to apply formal verification seems to
increase dramatically as the system size increases.

I have nevertheless made occasional use of formal verification
for almost 30 years by playing to formal verification's strengths,
namely design-time verification of small complex portions of the overarching
software construct.
The larger overarching software construct is of course validated by testing.

\fi

\QuickQuiz{
	L4 마이크로 커널의 전체 검증을 놓고 보면, 정형 검증에 대한 이 제한적
	시선은 약간 시대에 뒤쳐진 것 아닌가요?

	\iffalse

	In light of the full verification of the L4 microkernel,
	isn't this limited view of formal verification just a little
	bit obsolete?

	\fi

}\QuickQuizAnswer{
	불행히도, 아닙니다.

	L4 마이크로 커널의 전체 검증은 학생별로는 느린 속도로 진행된, 많은
	Ph.D. 학생들이 일일이 손을 통해 코드를 검증한 역작이었습니다.
	대부분의 소프트웨어 프로젝트는 변경이 너무 빨리 만들어지기 때문에 이런
	수준의 노력을 적용할 수 없습니다.
	더 나아가서, L4 마이크로커널이 정형 검증의 시선에서 보기에는 거대한
	소프트웨어 작품이지만, LLVM, \GCC, 리눅스 커널, Hadoop, MongoDB, 그
	외에도 수 많은 많은 프로젝트들에 비하면 아주 작습니다.
	또한, 이 검증 역시 한계가 있는데, 연구자들도 이를 인정합니다:
	\url{https://docs.sel4.systems/projects/sel4/frequently-asked-questions.html\#does-sel4-have-zero-bugs}.

	\iffalse

	Unfortunately, no.

	The first full verification of the L4 microkernel was a tour de force,
	with a large number of Ph.D.~students hand-verifying code at a
	very slow per-student rate.
	This level of effort could not be applied to most software projects
	because the rate of change is just too great.
	Furthermore, although the L4 microkernel is a large software
	artifact from the viewpoint of formal verification, it is tiny
	compared to a great number of projects, including LLVM,
	\GCC, the Linux kernel, Hadoop, MongoDB, and a great many others.
	In addition, this verification did have limits, as the researchers
	freely admit, to their credit:
	\url{https://docs.sel4.systems/projects/sel4/frequently-asked-questions.html\#does-sel4-have-zero-bugs}.

	\fi

	정형 검증이 더 큰 수준의 자동화를 적용한 더 최근의 L4 검증을 포함해
	어떤 성과를 마침내 거두기 시작하고 있지만 가시적 미래에 테스트를 완전히
	대체할 기회는 현재로썬 없습니다.
	그리고 전 이에 대해 틀렸기를 감히 바라겠습니다만, 그런 증명은 실제
	소프트웨어를 검증하는 진짜 도구에 의한 형태이어야지, 화려한 수사의 몸통
	형태여선 안될 겁니다.

	아마도 언젠가 정형 검증은 검증을 위해 널리 사용될텐데, 현재 회귀
	테스트라 알린 것들도 포함할 겁니다.
	Section~\ref{sec:future:Formal Regression Testing?} 은 이 가능성을
	현실로 만들기 위해 무엇이 필요할지 알아봅니다.

	\iffalse

	Although formal verification is finally starting to show some
	promise, including more-recent L4 verifications involving greater
	levels of automation, it currently has no chance of completely
	displacing testing in the foreseeable future.
	And although I would dearly love to be proven wrong on this point,
	please note that such proof will be in the form of a real tool
	that verifies real software, not in the form of a large body of
	rousing rhetoric.

	Perhaps someday formal verification will be used heavily for
	validation, including for what is now known as regression testing.
	Section~\ref{sec:future:Formal Regression Testing?} looks at
	what would be required to make this possibility a reality.

	\fi

}\QuickQuizEnd

마지막 한가지 방법은
\cref{sec:debugging:Required Mindset} 에서 소개된 다음의 두 정의와 그게
의미하는 결론을 고려하는 겁니다:

\begin{description}[itemsep=0pt,labelindent=1em]
\item[Definition:]	버그가 없는 프로그램은 사소한 프로그램이다.
\item[Definition:]	신뢰성 있는 프로그램은 알려진 버그가 없다.
\item[Consequences:]	모든 사소하지 않으며 신뢰성 있는 프로그램은 최소 하나의
			아직 알려지지 않은 버그를 가지고 있다.
\end{description}

이런 관점에서, 검증 분야에서의 발전은 두가지 효과를 가질 수 있습니다:
(1)~사소한 프로그램의 수의 증가 또는 (2)~신뢰성 있는 프로그램의 수의 감소.
물론, 인류의 멀티코어 시스템과 소프트웨어에 대한 의존의 증가는 사소한
프로그램의 수의 빠른 증가에 대한 강력한 동기 부여가 될 겁니다.

\iffalse

One final approach is to consider the following two definitions from
\cref{sec:debugging:Required Mindset}
and the consequence that they imply:

\begin{description}[itemsep=0pt,labelindent=1em]
\item[Definition:]	Bug-free programs are trivial programs.
\item[Definition:]	Reliable programs have no known bugs.
\item[Consequence:]	Any non-trivial reliable program contains at least
			one as-yet-unknown bug.
\end{description}

From this viewpoint, any advances in validation and verification can
have but two effects: (1)~An increase in the number of trivial programs or
(2)~A decrease in the number of reliable programs.
Of course, the human race's increasing reliance on multicore systems and
software provides extreme motivation for a very sharp increase in the
number of trivial programs.

\fi

하지만, 여러분의 코드가 너무 복잡해서 스스로가 정형 검증 도구에 크게 의존하고
있음을 발견하게 된다면 여러분의 설계를 주의 깊게 다시 생각해 봐야 하는데, 특히
여러분의 정형 검증 도구가 여러분의 코드를 특수 목적 언어로 손으로 번역될 것을
필요로 한다면 그렇습니다.
예를 들어,
Section~\ref{sec:formal:Promela Parable: dynticks and Preemptible RCU}
에서 보인 preemption 가능한 RCU 의 dynticks 인터페이스의 복잡한 구현은
Section~\ref{sec:formal:Simplicity Avoids Formal Verification}
에서 이야기 된 것처럼 훨씬 더 간단한 대안 구현을 갖게 되었습니다.
다른 모든게 동일하다면 더 간단한 구현이 올바름의 증명에 훨씬 낫습니다.

그리고 정형 검증 기법과 시스템에서 일하는 분들에게 열려 있는 도전 사항은 이
요약이 틀렸음을 증명하는 겁니다!
이 일을 돕기 위해, Verification Challenge~6 이
가능합니다~\cite{PaulEMcKenney2017VerificationChallenge6}.
해보세요!!!

\iffalse

However, if your code is so complex that you find yourself
relying too heavily on formal-verification
tools, you should carefully rethink your design, especially if your
formal-verification tools require your code to be hand-translated
to a special-purpose language.
For example, a complex implementation of the dynticks interface for
preemptible RCU that was presented in
Section~\ref{sec:formal:Promela Parable: dynticks and Preemptible RCU}
turned out to
have a much simpler alternative implementation, as discussed in
Section~\ref{sec:formal:Simplicity Avoids Formal Verification}.
All else being equal, a simpler implementation is much better than
a proof of correctness for a complex implementation.

And the open challenge to those working on formal verification techniques
and systems is to prove this summary wrong!
To assist in this task, Verification Challenge~6 is now
available~\cite{PaulEMcKenney2017VerificationChallenge6}.
Have at it!!!

\fi

\section{Choosing a Validation Plan}
\label{sec:formal:Choosing a Validation Plan}
%
\epigraph{Science is a first-rate piece of furniture for one's upper
	  chamber, but only given common sense on the ground floor.}
	 {\emph{Oliver Wendell Holmes, updated}}

What sort of validation should you use for your project?

As is often the case in software in particular and in engineering
in general, the answer is ``it depends''.

Note that neither running a test nor undertaking formal verification
will change your project.
At best, such effort have an indirect effect by locating a bug that
is later fixed.
Nevertheless, fixing a bug might prevent inconvenience, monetary loss,
property damage, or even loss of life.
Clearly, this sort of indirect effect can be extremely valuable.

Unfortunately, as we have seen, it is difficult to predict whether or
not a given validation effort will find important bugs.
It is therefore all too easy to invest too little---or even to fail
to invest at all, especially if development estimates proved overly
optimistic or budgets unexpectedly tight, conditions which almost
always come into play in real-world software projects.

The decision to nevertheless invest in validation is often forced by
experienced people with forceful personalities.
But this is no guarantee, given that other stakeholders might also
have forceful personalities.
Worse yet, these other stakeholders might bring stories of expensive
validation efforts that nevertheless allowed embarrassing bugs to
escape to the end users.
So although a scarred, grey-haired, and grouchy veteran might carry
the day, a more organized approach would perhaps be more useful.

Fortunately, there is a strictly financial analog to investments in
validation, and that is the insurance policy.

\IfTwoColumn{
\begin{figure*}[tb]
\centering
\resizebox{6in}{!}{\includegraphics{formal/RCU-test-ratio.pdf}}
\caption{Linux-Kernel RCU Test Code}
\label{fig:formal:Linux-Kernel RCU Test Code}
\end{figure*}
}{}

Both insurance policies and validation efforts require consistent
up-front investments, and both defend against disasters that might
or might not ever happen.
Furthermore, both have exclusions of various types.
For example, insurance policies for coastal areas might exclude
damages due to tidal waves, while on the other hand we have seen
that there is not yet any validation methodology that can find
each and every bug.

In addition, it is possible to over-invest in both insurance and
in validation.
For but one example, a validation plan that consumed the entire
development budget would be just as pointless as would an insurance
policy that covered the Sun going nova.

One approach is to devote a given fraction of the software budget to
validation, with that fraction depending on the criticality of the
software, so that safety-critical avionics software might grant a
larger fraction of its budget to validation than would a homework
assignment.
Where available, experience from prior similar projects should be
brought to bear.
However, it is necessary to structure the project so that the validation
investment starts when the project does, otherwise the inevitable overruns
in spending on coding will crowd out the validation effort.

Staffing start-up projects with experienced people can result in
overinvestment in validation efforts.
Just as it is possible to go broke buying too much insurance, it is
possible to kill a project by investing too much in testing.
This is especially the case for first-of-a-kind projects where it is
not yet clear which use cases will be important, in which case testing
for all possible use cases will be a possibly fatal waste of time,
energy, and funding.

However, as the tasks supported by a start-up project become more routine,
users often become less forgiving of failures, thus increasing the need
for validation.
Managing this shift in investment can be extremely challenging,
especially in the all-too-common case where the users are unwilling
or unable to disclose the exact nature of their use case.
It then becomes critically important to reverse-engineer the
use cases from bug reports and from discussions with the users.
As these use cases are better understood, use of continuous integration
can help reduce the cost of finding and fixing any bugs located.

\IfTwoColumn{}{
\begin{figure}[tb]
\centering
\resizebox{4.5in}{!}{\includegraphics{formal/RCU-test-ratio.pdf}}
\caption{Linux-Kernel RCU Test Code}
\label{fig:formal:Linux-Kernel RCU Test Code}
\end{figure}
}

One example evolution of a software project's use of validation is
shown in
\cref{fig:formal:Linux-Kernel RCU Test Code}.
As can be seen in the \lcnamecref{fig:formal:Linux-Kernel RCU Test Code},
Linux-kernel RCU didn't have any validation code whatsoever until Linux
kernel v2.6.15, which was released more than two years after RCU was
accepted into the kernel.
The test suite achieved its peak fraction of the total lines of code
in Linux kernel v2.6.19--v2.6.21.
This fraction decreased sharply with the acceptance of preemptible RCU
for real-time applications in v2.6.25.
This decrease was due to the fact that the RCU API was identical
in the preemptible and non-preemptible variants of RCU\@.
This in turn meant that the existing test suite applied to both variants,
so that even though the Linux-kernel RCU code expanded significantly,
there was no need to expand the tests.

Subsequent bars in \cref{fig:formal:Linux-Kernel RCU Test Code} show
that the RCU code base expanded significantly, but that the
corresponding validation code expanded even more dramatically.
Linux kernel v3.5 added tests for the \co{rcu_barrier()} API, closing
a long-standing hole in test coverage.
Linux kernel v3.14 added automated testing and analysis of test results,
moving RCU towards continuous integration.
Linux kernel v4.7 added a performance validation suite for RCU's update-side
primitives.
Linux kernel v4.12 added Tree SRCU, featuring improved update-side
scalability, and v4.13 removed the old less-scalable SRCU implementation.
Linux kernel v5.0 briefly hosted the \path{nolibc} library within
the rcutorture scripting directory before it moved to its long-term
home in \path{tools/include/nolibc}.
Linux kernel v5.8 added the Tasks Trace and Rude flavors of RCU\@.
Linux kernel v5.9 added the \path{refscale.c} suite of read-side performance
tests.
Numerous other changes may be found in the Linux kernel's \co{git} archives.
% rcutorture
% v2.6.15: First torture test
% v2.6.19: SRCU: Plugin architecture avoids test-code explosion.
% v2.6.19-21: Peak test fraction.
% v2.6.25: preemptible RCU, consistent API avoids added test code.
% v3.4: Add tests for RCU CPU stall warnings.
% v3.5: Add tests for rcu_barrier(). *
% v3.14: Add rcutorture scripting automating tests and results analysis. *
% v3.15: Add support for multiple torture-tests suites for locktorture.
% v3.16: Add support for conditional grace-period primitives.
% v4.7: Add update-side performance validation suite. *
% v4.12: Added Tree SRCU.
% v4.13: Removed non-Tree SRCU.
% v5.0: nolibc was briefly in the rcutorture scripting directory.
% v5.8: Added Tasks Trace RCU and Rude RCU.
% v5.9: Added refscale.c.

We have established that the validation budget varies from one project
to the next, and also over the lifetime of any given project.
But how should the validation investment be split between testing and
formal verification?

This question is being answered naturally as compilers adopt increasingly
aggressive formal-verification techniques into their diagnostics and
as formal-verification tools continue to mature.
In addition, the Linux-kernel lockdep and KCSAN tools illustrate the
advantages of combining formal verification techniques with run-time
analysis, as discussed in \cref{sec:debugging:Assertions}.
Other combined techniques analyze traces gathered from
executions~\cite{DanielBristot2019RTtrace}.
For the time being, the best practice is to focus first on testing and to
reserve explicit work on formal verification for those portions of the
project that are not well-served by testing, and that have exceptional
needs for robustness.
For example, Linux-kernel RCU relies primarily on testing, but has
made occasional use of formal verification as discussed in this chapter.

In short, choosing a validation plan for concurrent software remains
more an art than a science, let alone a field of engineering.
However, there is every reason to expect that increasingly rigorous
approaches will continue to become more prevalent.

\QuickQuizAnswersChp{qqzformal}
