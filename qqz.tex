\QuickQAC{chp:How To Use This Book}{How To Use This Book}
\QuickQ{}
	이 Quick Quiz 들의 답은 어디에 있을까요?

\iffalse
	Where are the answers to the Quick Quizzes found?
\fi
\QuickA{}
	페이지~\pageref{chp:Answers to Quick Quizzes} 에서 시작하는
	Appendix~\ref{chp:Answers to Quick Quizzes}.

\iffalse
	In Appendix~\ref{chp:Answers to Quick Quizzes} starting on
	page~\pageref{chp:Answers to Quick Quizzes}.
\fi

	쉽죠?
\iffalse
	Hey, I thought I owed you an easy one!
\fi

\QuickQ{}
	몇몇 퀴즈는 저자의 입장이 아니라 독자의 입장에서 쓰인 것 같은데요.
	그런 의도가 맞나요?

\iffalse
	Some of the Quick Quiz questions seem to be from the viewpoint
	of the reader rather than the author.
	Is that really the intent?
\fi
\QuickA{}
	실제로 그렇답니다!  많은 질문들은 Paul E. Mckenny 가 이 내용들을 다루는
	수업을 듣는 학생이었다면 질문했을 법한 것들입니다.  Paul 은 이 내용들을
	교수님으로부터가 아니라 병렬 하드웨어와 소프트웨어로부터 배웠다는
	내용도 짚어둬야 할 것 같네요.  Paul 의 경험에 의하면, 교수님들은 Watson
	같은 실제 병렬 시스템과는 달리 말로 이야기되는 문제에 대해 답을 주려
	하곤 합니다.  물론, 어떤 교수님들이나 병렬 시스템들이 이런 종류의
	문제들에 대해 가장 유용한 답을 주는지에 대해서는 많은 토론이
	가능하겠습니다만, 지금은 일단 실제 교수님들과 병렬 시스템들에 따라
	그들이 주는 답의 유용성이 다를 수 있다는 점만 동의하고 넘어갑시다.

\iffalse
	Indeed it is!
	Many are questions that Paul E. McKenney would probably have
	asked if he was a novice student in a class covering this material.
	It is worth noting that Paul was taught most of this material by
	parallel hardware and software, not by professors.
	In Paul's experience, professors are much more likely to provide
	answers to verbal questions than are parallel systems,
	Watson notwithstanding.
	Of course, we could have a lengthy debate over which of professors
	or parallel systems provide the most useful answers to these sorts
	of questions,
	but for the time being let's just agree that usefulness of
	answers varies widely across the population both of professors
	and of parallel systems.
\fi

	그 외의 퀴즈들은 컨퍼런스 발표 중에, 그리고 이 책에서 다루는 내용을
	다루는 수업 중에 받은 실제 질문과 유사합니다.
	그리고 일부 퀴즈는 저자의 관점에서 쓰이기도 했습니다.

\iffalse
	Other quizzes are quite similar to actual questions that have been
	asked during conference presentations and lectures covering the
	material in this book.
	A few others are from the viewpoint of the author.
\fi

\QuickQ{}
	전 퀵 퀴즈를 좋아하지 않아요.
	어떡하죠?

\iffalse
	These Quick Quizzes are just not my cup of tea.
	What can I do about it?
\fi
\QuickA{}

여기 몇가지 전략이 있습니다:

\iffalse
Here are a few possible strategies:
\fi

\begin{enumerate}
\item	그냥 퀵 퀴즈를 무시하고 책을 읽으세요.  퀵 퀴즈의 흥미로운 내용들을
	놓치게 되겠지만 이 책의 퀵 퀴즈를 제외한 부분도 훌륭한 내용을 많이 담고
	있습니다.  만약 당신의 목표가 일반적인 내용에 대한 이해를 얻는 것이거나
	이 책을 통해 특정 문제에 대한 해결책을 찾는 것이라면 충분히 합리적인
	접근법입니다.

\iffalse
\item	Just ignore the Quick Quizzes and read the rest of
	the book.
	You might miss out on the interesting material in
	some of the Quick Quizzes, but the rest of the book
	has lots of good material as well.
	This is an eminently reasonable approach if your main
	goal is to gain a general understanding of the material
	or if you are skimming through to book to find a
	solution to a specific problem.
\fi

\item	퀵 퀴즈가 집중을 방해하지만 무시하기엔 중요하다고 생각한다면, 언제든 이
	책의 소스를 git 저장소에서 클론할 수 있음을 기억하세요.
	그러고나서 \co{Makefile} 과 \co{qqz.sty} 를 수정해서 퀵 퀴즈가 PDF 에서
	사라지게 할 수 있습니다.
	또는, 해당 파일들을 수정해서 답변이 문제 바로 아래 나오도록 수정할 수도
	있습니다.

\iffalse
\item	If you find the Quick Quizzes distracting but impossible
	to ignore, you can always clone the \LaTeX{} source for
	this book from the git archive.
	You can then modify \co{Makefile} and \co{qqz.sty} to eliminate
	the Quick Quizzes from the PDF output.
	Alternatively, you could modify these two files so as
	to pull the answers inline, immediately following
	the questions.
\fi

\item	당신의 답을 구하느라 너무 많은 시간을 보내지 말고 곧바로 답을 보세요.
	현재 퀵 퀴즈의 답이 당신이 해결하려는 문제의 핵심을 쥐고 있는 게
	아니라면 이것도 합리적인 접근법입니다.  또한, 당신이 원하는게 해당
	내용에 대한 깊은 이해이지, 새로이 병렬성을 활용한 해결책을 맨바닥부터
	만들려 하는게 아닌 경우에도 이는 합리적인 접근법입니다.

\iffalse
\item	Look at the answer immediately rather than investing
	a large amount of time in coming up with your own
	answer.
	This approach is reasonable when a given Quick Quiz's
	answer holds the key to a specific problem you are
	trying to solve.
	This approach is also reasonable if you want a somewhat
	deeper understanding of the material, but when you do not
	expect to be called upon to generate parallel solutions given
	only a blank sheet of paper.
\fi
\end{enumerate}

\QuickQAC{chp:Introduction}{Introduction}
\QuickQ{}
	여봐요!!!
	병렬 프로그래밍은 수십년간 엄청나게 어렵다고 알려졌다구요.
	근데 당신은 그게 그렇게 어렵지 않다고 슬쩍 이야기하는 것 같네요.
	뭔 개수작이요?

\iffalse
	Come on now!!!
	Parallel programming has been known to be exceedingly
	hard for many decades.
	You seem to be hinting that it is not so hard.
	What sort of game are you playing?
\fi
\QuickA{}
	정말 병렬 프로그래밍이 엄청나게 어렵다고 믿는다면, ``왜 병렬
	프로그래밍이 어려운가?'' 라는 질문에 대답할 준비가 되어 있을 겁니다.
	누군가는 여러 이유를 댈 수 있겠죠, 데드락부터 레이스 컨디션, 테스팅
	커버리지 등등. 하지만 진짜 답은 {\em 그건 그렇게까지 어렵지는 않다}
	입니다.
	일단, 만약 병렬 프로그래밍이 정말로 그렇게 소름끼치도록 어렵다면,
	어떻게 Apache 나 MySQL, 리눅스 커널 같은 많은 오픈 소스 프로젝트들이
	그걸 잘 사용하고 있겠어요?

\iffalse
	If you really believe that parallel programming is exceedingly
	hard, then you should have a ready answer to the question
	``Why is parallel programming hard?''
	One could list any number of reasons, ranging from deadlocks to
	race conditions to testing coverage, but the real answer is that
	{\em it is not really all that hard}.
	After all, if parallel programming was really so horribly difficult,
	how could a large number of open-source projects, ranging from Apache
	to MySQL to the Linux kernel, have managed to master it?
\fi

	보다 나은 질문은 아마도 이렇겠죠: ''왜 병렬 프로그래밍은 그렇게
	어렵다고 {\em 알려져 있을까}?''
	답을 알기 위해, 1991년으로 돌아가 봅시다.
	Paul McKenney 는 주차장에서 6개의 dual-80486 Sequent Symmetry CPU
	보드를 들고 Sequent의 벤치마킹 센터로 걸어가던 중, 문득 자신이 집 몇채
	가격의 물건을 들고 있음을 깨달았습니다.\footnote{
	그래요, 이 갑작스런 깨달음은 그가 좀 더 조심히 걷게 만들었습니다. 왜
	그런걸 물어요?}
	이렇게 엄청난 병렬 시스템의 가격은 병렬 프로그래밍이 병렬 시스템을 직접
	제작하거나 --- 1991년의 미국 달러로 --- \$100,000 이상의 가격의 기계를
	구매할 수 있는 회사에서 일하는 제한된 일부 특권층의 일부에게만
	가능했음을 의미합니다.

\iffalse
	A better question might be: ''Why is parallel programming {\em
	perceived} to be so difficult?''
	To see the answer, let's go back to the year 1991.
	Paul McKenney was walking across the parking lot to Sequent's
	benchmarking center carrying six dual-80486 Sequent Symmetry CPU
	boards, when he suddenly realized that he was carrying several
	times the price of the house he had just purchased.\footnote{
		Yes, this sudden realization {\em did} cause him to walk quite
		a bit more carefully.
		Why do you ask?}
	This high cost of parallel systems meant that
	parallel programming was restricted to a privileged few who
	worked for an employer who either manufactured or could afford to
	purchase machines costing upwards of \$100,000 --- in 1991 dollars US.
\fi


	하지만, 2006년, Paul 은 자신이 이 글을 dual-core x86 노트북에서 쓰고
	있음을 발견합니다.
	앞서 이야기한 dual-80486 CPU 보드와 달리, 이 노트북은 2GB 메인 메모리,
	60GB 디스크 드라이브, 모니터, 이더넷, USB 포트, 무선랜, 그리고
	블루투스까지 달려 있습니다.
	그리고 그 노트북은 그간의 인플레이션을 고려하지 않더라도 dual-80486 CPU
	보드보다 열배가 넘게 쌉니다.

\iffalse
	In contrast, in 2006, Paul finds himself typing these words on a
	dual-core x86 laptop.
	Unlike the dual-80486 CPU boards, this laptop also contains
	2GB of main memory, a 60GB disk drive, a display, Ethernet,
	USB ports, wireless, and Bluetooth.
	And the laptop is more than an order of magnitude cheaper than
	even one of those dual-80486 CPU boards, even before taking inflation
	into account.
\fi

	병렬 시스템이 정말로 세상에 도래했습니다.
	병렬 시스템은 더이상 일부 특권층의 소유물이 아니라 거의 모든 사람에게
	가능한 물건입니다.

\iffalse
	Parallel systems have truly arrived.
	They are no longer the sole domain of a privileged few, but something
	available to almost everyone.
\fi

	기존의 제한적이었던 병렬 하드웨어 접근성이야말로 병렬 프로그래밍이
	그렇게 어렵다고 여겨지게 만들었던 \emph{진짜} 이유입니다.
	무엇보다, 아무리 단순한 기계라도 직접 만져볼 수가 없다면 프로그램하기는
	매우 어렵습니다.
	찾기 어렵고 비싼 패럴렐 머신들의 시대는 갔으니 병렬 프로그래밍이
	미치도록 어렵다고 생각하던 시대는 곧 지나갈 겁니다.\footnote{
		병렬 프로그래밍은 시퀀셜 프로그래밍보다는 어렵습니다. 예를
		들어, 병렬적으로 validation 을 하는 것은 더 어렵습니다.
		하지만 더이상 미칠듯이 어렵진 않아요.}

\iffalse
	The earlier restricted availability of parallel hardware is
	the \emph{real} reason that parallel programming is considered
	so difficult.
	After all, it is quite difficult to learn to program even the simplest
	machine if you have no access to it.
	Since the age of rare and expensive parallel machines is for the most
	part behind us, the age during which
	parallel programming is perceived to be mind-crushingly difficult is
	coming to a close.\footnote{
		Parallel programming is in some ways more difficult than
		sequential programming, for example, parallel validation
		is more difficult.
		But no longer mind-crushingly difficult.}
\fi

\QuickQ{}
	어떻게 병렬 프로그래밍이 시퀀셜 프로그래밍만큼 쉬운게 가능한가요?

\iffalse
	How could parallel programming \emph{ever} be as easy
	as sequential programming?
\fi
\QuickA{}
	그건 프로그래밍 환경에 달려 있습니다.  SQL~\cite{DIS9075SQL92} 는 잘
	알려지지 않은 성공 사례인데요, 병렬성에 대해 잘 모르는 프로그래머도
	거대한 병렬 시스템을 바삐 동작하게 만들 수 있도록 해주기 때문이죠.
	병렬 컴퓨터는 갈수록 싸고 어디서나 접할 수 있게 되어가고 있기 때문에
	이런 류의 다양한 예를 볼 수 있을 겁니다.  예를 들어, 과학 / 기술 컴퓨팅
	쪽에서의 가능할 법한 경쟁자는 흔한 행렬 연산을 자동으로 병렬화 시켜주는
	MATLAB*P 입니다.

\iffalse
	It depends on the programming environment.
	SQL~\cite{DIS9075SQL92} is an underappreciated success
	story, as it permits programmers who know nothing about parallelism
	to keep a large parallel system productively busy.
	We can expect more variations on this theme as parallel
	computers continue to become cheaper and more readily available.
	For example, one possible contender in the scientific and
	technical computing arena is MATLAB*P,
	which is an attempt to automatically parallelize common
	matrix operations.
\fi

	마지막으로, 리눅스와 유닉스 시스템에서의 다음 셸 커맨드를 생각해
	보세요:

\iffalse
	Finally, on Linux and UNIX systems, consider the following
	shell command:
\fi

	{\small \tt get\_input | grep "interesting" | sort}

	이 셸 파이프라인은 \co{get_input}, \co{grep}, 그리고 \co{sort} 를
	병렬적으로 처리합니다.
	어때요, 어렵지 않았죠, 됐죠?

\iffalse
	This shell pipeline runs the \co{get_input}, \co{grep},
	and \co{sort} processes in parallel.
	There, that wasn't so hard, now was it?
\fi

	요약하자면, 병렬 프로그래밍은 시퀀셜 프로그래밍 만큼이나 쉽습니다.
	적어도 병렬성을 사용자에게서 숨겨주는 환경에서는요!

\iffalse
	In short, parallel programming is just as easy as sequential
	programming---at least in those environments that hide the parallelism
	from the user!
\fi

\QuickQ{}
	헐, 진짜요? 정확성, 관리성, 내구성 같은 것들은요?

\iffalse
	Oh, really???
	What about correctness, maintainability, robustness, and so on?
\fi
\QuickA{}
	그것들도 중요한 목표들이죠, 하지만 병렬 프로그램에서 그런 목표들의
	중요도는 시퀀셜 프로그램에서의 그것 정도일 뿐입니다.  따라서, 그것들은
	중요한 목표들이긴 하지만 병렬 프로그래밍만의 목표에는 속하지 않습니다.

\iffalse
	These are important goals, but they are just as important for
	sequential programs as they are for parallel programs.
	Therefore, important though they are, they do not belong on
	a list specific to parallel programming.
\fi

\QuickQ{}
	그리고 정확성, 관리성, 내구성이 해당되지 않는데 왜 생산성과 Generality
	는 해당되는거죠?

\iffalse
	And if correctness, maintainability, and robustness don't
	make the list, why do productivity and generality?
\fi
\QuickA{}
	병렬 프로그래밍이 시퀀셜 프로그래밍보다 훨씬 어렵다고 인식되고
	있는만큼, 생산성도 달성하기 어려운 목표로 여겨지고 있고, 따라서 반드시
	이 목표를 이뤄야 합니다.
	또한, SQL 과 같이 높은 생산성을 갖는 병렬 프로그래밍 환경은 특정한
	용도에만 사용가능하기 때문에, Generality 도 반드시 목표에 들어가야
	합니다.

\iffalse
	Given that parallel programming is perceived to be much harder
	than sequential programming, productivity is tantamount and
	therefore must not be omitted.
	Furthermore, high-productivity parallel-programming environments
	such as SQL serve a special purpose, hence generality must
	also be added to the list.
\fi

\QuickQ{}
	병렬 프로그램은 정확성을 증명하기가 어렵다고 알고 있는데, \emph{정말}
	정확성도 그 목록에 올라갈 수 없는 건가요?

\iffalse
	Given that parallel programs are much harder to prove
	correct than are sequential programs, again, shouldn't
	correctness \emph{really} be on the list?
\fi
\QuickA{}
	엔지니어링 관점에서 형식적이든 비형식적이든 정확성을 증명하는건
	엔지니어링 관점에서의 최대 목표인 생산성에 어떤 영향을 미치느냐에 따라
	중요도가 정해집니다.
	따라서, 정확성 증명이 중요한 경우라면 ``생산성'' 아래 포함된다고 볼 수
	있겠죠.

\iffalse
	From an engineering standpoint, the difficulty in proving
	correctness, either formally or informally, would be important
	insofar as it impacts the primary goal of productivity.
	So, in cases where correctness proofs are important, they
	are subsumed under the ``productivity'' rubric.
\fi

\QuickQ{}
	그냥 재미를 목표로 하는건 어떤가요?

\iffalse
	What about just having fun?
\fi
\QuickA{}
	재미도 물론 중요하죠, 하지만 당신이 취미로만 사는 사람이 아니라면
	재미가 당신의 \emph{최우선} 목표는 아닐겁니다.  거꾸로 말하자면, 당신이
	취미로만 사는 사람이 \emph{맞다면} 좋은 자세입니다!

\iffalse
	Having fun is important as well, but, unless you are a hobbyist,
	would not normally be a \emph{primary} goal.
	On the other hand, if you \emph{are} a hobbyist, go wild!
\fi

\QuickQ{}
	성능 이외의 이유로 병렬 프로그래밍을 하는 경우도 있나요?

\iffalse
	Are there no cases where parallel programming is about something
	other than performance?
\fi
\QuickA{}
	풀어야 하는 문제가 본질적으로 병렬적인 경우가 있습니다. 예를 들어,
	Monte Carlo method 들과 일부 숫자 계산들이요.
	하지만, 이런 경우에도 병렬성을 관리하기 위해 많은 추가작업이
	필요합니다.

	병렬성은 가끔 신뢰성(reliability) 를 위해 사용되기도 합니다.
	일단 예를 하나들자면, triple-modulo redundancy 는 병렬로 동작하는
	세개의 시스템을 가지고 결과에 대해 투표를 합니다.
	극단적 경우에는 세개의 시스템이 서로 다른 알고리즘과 기술을 가지고
	독립적으로 구현될 수도 있습니다.

\iffalse
	There certainly are cases where the problem to be solved is
	inherently parallel, for example, Monte Carlo methods and
	some numerical computations.
	Even in these cases, however, there will be some amount of
	extra work managing the parallelism.

	Parallelism is also sometimes used for reliability.
	For but one example,
	triple-modulo redundancy has three systems run in parallel
	and vote on the result.
	In extreme cases, the three systems will be independently
	implemented using different algorithms and technologies.
\fi

\QuickQ{}
	왜 이런 비기술적인 문제를 이야기하는거죠???
	그저 비기술적일 뿐 아니라, 심지어 \emph{생산성}이라니요?
	누가 그런걸 신경써요?

\iffalse
	Why all this prattling on about non-technical issues???
	And not just \emph{any} non-technical issue, but \emph{productivity}
	of all things?
	Who cares?
\fi
\QuickA{}
	당신이 순수히 취미로만 사는 사람이라면 아마 당신은 신경쓰지 않아도
	될겁니다.
	하지만 설령 그렇다 해도 얼마나 빨리 그리고 얼마나 많이 일을 할 수
	있는지는 신경쓸겁니다.
	무엇보다, 가장 유명한 취미가용 도구는 보통 그 목적에 가장 적합한
	도구이고, ``가장 적합한'' 이란 말의 정의의 가장 중요한 부분은 생산성과
	연결되어 있죠.
	그리고 만약 누군가가 당신에게 병렬 코드를 작성하라고 돈을 준다면,
	그들은 당신의 생산성에 대해 매우 신경쓸겁니다.
	그리고 그 고용주가 뭔가에 신경쓴다면, 당신은 거기에 적어도 관심을
	가져야겠죠!

\iffalse
	If you are a pure hobbyist, perhaps you don't need to care.
	But even pure hobbyists will often care about how much they
	can get done, and how quickly.
	After all, the most popular hobbyist tools are usually those
	that are the best suited for the job, and an important part of
	the definition of ``best suited'' involves productivity.
	And if someone is paying you to write parallel code, they will
	very likely care deeply about your productivity.
	And if the person paying you cares about something, you would
	be most wise to pay at least some attention to it!
\fi

	그리고, 만약 당신이 \emph{정말로} 생산성에 신경쓰지 않는다면, 애초에
	컴퓨터를 사용하지 않고 손으로 일을 했겠죠!

\iffalse
	Besides, if you \emph{really} didn't care about productivity,
	you would be doing it by hand rather than using a computer!
\fi

\QuickQ{}
	병렬 시스템이 그렇게 싼 가격이 되었다면, 어떤 사람이 그걸 프로그램
	하라고 월급을 줘가며 프로그래머를 고용하겠어요?
\iffalse
	Given how cheap parallel systems have become, how can anyone
	afford to pay people to program them?
\fi
\QuickA{}
	이 질문에는 몇가지 답이 있습니다:
	\begin{enumerate}
	\item	거대한, 여러 병렬머신들로 구성된 클러스터가 있다고 하면, 이 클러스터의 전체 비용은 상당한 개발 노력을 정당화합니다. 개발 비용은 수많은 머신들 전체에게 적용되기 때문이죠.
	\item	수천만명이 넘는 사용자들이 사용하는 유명한 소프트웨어라면 상당한 개발 노력이 정당화 됩니다. 그 개발 노력은 수천만 사용자를 위한 거니까요.
		커널이나 시스템 라이브러리 같은 것들도 이 경우에 들어감을 참고하세요.
	\item	낮은 가격의 병렬 머신이 중요한 어떤 장비의 운영에 사용되고 있다면 그 장비의 가격 일부분이 상당한 개발 비용을 정당화 할 수 있습니다.
	\item	안전을 위해 사용되는 주요 시스템은 사람의 목숨을 보호합니다. 따라서 이 경우에는 매우 큰 개발 비용을 정당화 하죠.
	\item	취미가와 연구자들은 돈보다는 지식, 경험, 재미, 그리고 명예를 추구합니다.
	\end{enumerate}
	그러니까 하락하는 하드웨어 가격은 소프트웨어를 의미없게 만들지 않고,
	오히려 소프트웨어 개발 비용을 하드웨어 가격에 ``숨기는'' 것이
	불가능해진 겁니다. 적어도 엄청나게 많은 수의 하드웨어를 사용하는 경우가
	아니라면요.

\iffalse
	There are a number of answers to this question:
	\begin{enumerate}
	\item	Given a large computational cluster of parallel machines,
		the aggregate cost of the cluster can easily justify
		substantial developer effort, because the development
		cost can be spread over the large number of machines.
	\item	Popular software that is run by tens of millions of users
		can easily justify substantial developer effort,
		as the cost of this development can be spread over the tens
		of millions of users.
		Note that this includes things like kernels and system
		libraries.
	\item	If the low-cost parallel machine is controlling the operation
		of a valuable piece of equipment, then the cost of this
		piece of equipment might easily justify substantial
		developer effort.
	\item	If the software for the low-cost parallel machine produces an
		extremely valuable result (e.g., mineral exploration),
		then the valuable result might again justify substantial
		developer cost.
	\item	Safety-critical systems protect lives, which can clearly
		justify very large developer effort.
	\item	Hobbyists and researchers might seek knowledge, experience,
		fun, or glory rather than gold.
	\end{enumerate}
	So it is not the case that the decreasing cost of hardware renders
	software worthless, but rather that it is no longer possible to
	``hide'' the cost of software development within the cost of
	the hardware, at least not unless there are extremely large
	quantities of hardware.
\fi

\QuickQ{}
	이건 달성 불가한 이상에 불과해요!
	현실적으로 달성 가능한 무언가에 집중하는게 어때요?

\iffalse
	This is a ridiculously unachievable ideal!
	Why not focus on something that is achievable in practice?
\fi
\QuickA{}
	이건 분명 달성 가능합니다.
	휴대폰은 프로그래밍이나 환경구성 없이 최종 사용자가 전화 통화를 하고
	텍스트 메세지를 주고 받을 수 있게 해주는 컴퓨터입니다.

\iffalse
	This is eminently achievable.
	The cellphone is a computer that can be used to make phone
	calls and to send and receive text messages with little or
	no programming or configuration on the part of the end user.
\fi

	일견 사소한 예처럼 보일 수 있겠지만, 천천히 생각해보면 이건 간단하기도
	하고 심오하기도 한 이야기입니다.
	generality 를 희생하면 우리는 놀랍도록 높은 생산성 향상을 얻을 수
	있습니다.
	과한 generality 에 빠진 사람들은 그래서 소프트웨어 스택의 최대치까지
	성능을 끌어올리는데 실패하곤 합니다.
	이 삶의 진리는 약자도 있죠: YAGNI, 즉 ``You Ain't Gonna Need It.''

\iffalse
	This might seem to be a trivial example at first glance,
	but if you consider it carefully you will see that it is
	both simple and profound.
	When we are willing to sacrifice generality, we can achieve
	truly astounding increases in productivity.
	Those who indulge in excessive generality will therefore fail to set
	the productivity bar high enough to succeed near the top of the
	software stack.
	This fact of life even has its own acronym: YAGNI, or ``You
	Ain't Gonna Need It.''
\fi

\QuickQ{}
	잠깐만요!
	이런 접근법은 단순히 개발을 위한 노력을 당신으로부터 누군가 그
	존재한다는 병렬 소프트웨어를 만드는 사람에게 전가할 뿐인 거 아닌가요?
\iffalse
	Wait a minute!
	Doesn't this approach simply shift the development effort from
	you to whoever wrote the existing parallel software you are using?
\fi
\QuickA{}
	바로 그겁니다!
	그리고 그게 바로 이미 있는 소프트웨어를 쓰는 것의 요점이죠.
	한 팀의 작업물이 많은 다른 팀에 의해 사용되어서 모든 팀이 불필요하게
	바퀴를 재발명하는 것에 비해 훨씬 노력을 줄이게 되는것이요.
\iffalse
	Exactly!
	And that is the whole point of using existing software.
	One team's work can be used by many other teams, resulting in a
	large decrease in overall effort compared to all teams
	needlessly reinventing the wheel.
\fi

\QuickQ{}
	어떤 다른 병목지점들이 CPU 를 추가해도 성능을 개선되지 않게 할 수
	있을까요?

\iffalse
	What other bottlenecks might prevent additional CPUs from
	providing additional performance?
\fi
\QuickA{}
	잠재적 병목지점이 얼마든지 있습니다:
\iffalse
	There are any number of potential bottlenecks:
\fi
	\begin{enumerate}
	\item	메인 메모리. 싱글 쓰레드가 모든 가용한 메모리를 사용하고
		있다면, 추가된 쓰레드는 단순히 멍청하게 자신을 페이지 아웃
		시키겠죠.
	\item	캐시. 싱글 쓰레드의 캐시 사용량이 모든 공유 CPU 캐시(들)을 꽉
		채운다면, 쓰레드를 추가하는 것은 그저 영향받는 캐시들을 쓰래쉬
		하기만 할겁니다.
	\item	메모리 밴드위쓰. 싱글 쓰레드가 모든 메몰 밴드위쓰를 소모한다면,
		추가된 쓰레드들은 그저 메모리로의 시스템 접점에 줄을 서 있을
		겁니다.
	\item	I/O 밴드위쓰. 싱글쓰레드가 I/O 에 바운드 되어 있다면,
		쓰레드들을 추가하는 것은 그저 그들 모두 관련된 I/O 자원에 줄을
		서서 기다리고만 있게 될겁니다.
\iffalse
	\item	Main memory.  If a single thread consumes all available
		memory, additional threads will simply page themselves
		silly.
	\item	Cache.  If a single thread's cache footprint completely
		fills any shared CPU cache(s), then adding more threads
		will simply thrash those affected caches.
	\item	Memory bandwidth.  If a single thread consumes all available
		memory bandwidth, additional threads will simply
		result in additional queuing on the system interconnect.
	\item	I/O bandwidth.  If a single thread is I/O bound,
		adding more threads will simply result in them all
		waiting in line for the affected I/O resource.
\fi
	\end{enumerate}

	특정 하드웨어 시스템들은 추가적인 병목지점을 얼마든지 가지고 있을 수
	있습니다.
	다만 분명한 건 여러 CPU 들이나 쓰레드들 간에 공유되고 있는 자원은
	잠재적 병목지점입니다.

\iffalse
	Specific hardware systems might have any number of additional
	bottlenecks.
	The fact is that every resource which is shared between
	multiple CPUs or threads is a potential bottleneck.
\fi

\QuickQ{}
	CPU 캐시 용량 외에, 뭐가 동시에 수행되는 쓰레드들의 갯수를 제한해야
	하게 할 수 있을까요?
\iffalse
	Other than CPU cache capacity, what might require limiting the
	number of concurrent threads?
\fi
\QuickA{}
	쓰레드 갯수에 영향을 끼치는 여러 잠재적 요소들이 있습니다:
	\begin{enumerate}
	\item	메인 메모리. 각 쓰레드는 (최소한 스택을 위해) 메모리를 일부
		사용하므로, 너무 많은 쓰레드는 메모리를 모조리 사용해버려서
		엄청나게 과도한 페이징이나 메모리 할당 실패를 일으킬 수
		있습니다.
	\item	I/O 밴드위쓰. 각 쓰레드가 많은 스토리지 I/O 나 네트워크
		트래픽을 만든다면 너무 많은 수의 쓰레드는 과도한 I/O 큐잉
		딜레이를 일으키고, 결국 성능이 또 저하될 것입니다.
		일부 네트워킹 프로토콜은 너무 많은 쓰레드가 네트워킹 이벤트를
		만들어 시간 내에 그 응답을 받지 못할 경우 타임아웃이나 다른
		문제상황을 낼 수 있습니다.
	\item	동기화 오버헤드.
		많은 동기화 프로토콜에서 과도한 수의 쓰레드는 지나친 스피닝,
		블락킹, 또는 롤백을 일으켜서 성능을 떨어뜨릴 수 있습니다.
	\end{enumerate}

	특정한 어플리케이션이나 플랫폼에 따라서는 이외에도 추가적인 요소가
	얼마든지 있을 수 있습니다.
\iffalse
	There are any number of potential limits on the number of
	threads:
	\begin{enumerate}
	\item	Main memory.  Each thread consumes some memory
		(for its stack if nothing else), so that excessive
		numbers of threads can exhaust memory, resulting
		in excessive paging or memory-allocation failures.
	\item	I/O bandwidth.  If each thread initiates a given
		amount of mass-storage I/O or networking traffic,
		excessive numbers of threads can result in excessive
		I/O queuing delays, again degrading performance.
		Some networking protocols may be subject to timeouts
		or other failures if there are so many threads that
		networking events cannot be responded to in a timely
		fashion.
	\item	Synchronization overhead.
		For many synchronization protocols, excessive numbers
		of threads can result in excessive spinning, blocking,
		or rollbacks, thus degrading performance.
	\end{enumerate}

	Specific applications and platforms may have any number of additional
	limiting factors.
\fi

\QuickQ{}
	병렬 프로그래밍에 다른 어려움은 없나요?

	\iffalse
	Are there any other obstacles to parallel programming?
	\fi
\QuickA{}
	병렬 프로그래밍에의 수많은 잠재적 문제들이 존재합니다.
	여기 그 중 일부를 이야기 해보죠:

	\iffalse
	There are a great many other potential obstacles to parallel
	programming.
	Here are a few of them:
	\fi
	\begin{enumerate}
	\item	주어진 프로젝트의 하나 뿐인 알고리즘이 본질적으로 순차적일 수
		있습니다.
		이 경우에는 (당신의 프로젝트가 \emph{반드시} 병렬로 돌아야
		한다는 법적 조항이 없다면) 병렬 프로그래밍을 관두거나 새로운
		병렬 알고리즘을 고안해내야 합니다.

	\iffalse
	\item	The only known algorithms for a given project might
		be inherently sequential in nature.
		In this case, either avoid parallel programming
		(there being no law saying that your project \emph{has}
		to run in parallel) or invent a new parallel algorithm.
	\fi

	\item	프로젝트가 동일 어드레스 스페이스를 사용하지만 바이너리로만
		제공되는 플러그인을 허용해서 모든 개발자가 프로젝트의 모든 소스
		코드에 접근할 수는 없는 경우가 있을 수 있습니다.
		데드락을 포함해 많은 병렬성에 기인한 버그들이 여기저기 있기
		때문에, 그런 바이너리로 만 제공되는 플러그인은 현재의
		소프트웨어 개발 방법 하에서는 상당한 어려움을 안겨줄 수
		있습니다.
		물론 미래에는 상황이 바뀔 수도 있지만 현재로썬 주어진 어드레스
		스페이스를 공유하는 병렬 코드의 모든 개발자는 그 어드레스
		스페이스에서 돌아가는 \emph{모든} 모드를 들여다 볼 수 있어야
		합니다.

	\iffalse
	\item	The project allows binary-only plugins that share the same
		address space, such that no one developer has access to
		all of the source code for the project.
		Because many parallel bugs, including deadlocks, are
		global in nature, such binary-only plugins pose a severe
		challenge to current software development methodologies.
		This might well change, but for the time being, all
		developers of parallel code sharing a given address space
		need to be able to see \emph{all} of the code running in
		that address space.
	\fi

	\item	프로젝트가 병렬성을 고려하지 않은채 설계된
		API~\cite{HagitAttiya2011LawsOfOrder,Clements:2013:SCR:2517349.2522712}
		를 엄청나게 사용하는 경우.
		System V 메세지 큐 API 의 매우 화려한 기능들이 이 경우에
		속합니다.
		물론, 만약 당신의 프로젝트가 수십년 넘게 존속되었다면, 그리고
		그 개발자들이 병렬 하드웨어를 접해본 적 없었다면 그 프로젝트는
		분명 그런 API 들을 최소한 사용은 하고 있을 겁니다.

	\iffalse
	\item	The project contains heavily used APIs that were designed
		without regard to
		parallelism~\cite{HagitAttiya2011LawsOfOrder,Clements:2013:SCR:2517349.2522712}.
		Some of the more ornate features of the System V
		message-queue API form a case in point.
		Of course, if your project has been around for a few
		decades, and its developers did not have access to
		parallel hardware, it undoubtedly has at least
		its share of such APIs.
	\fi

	\item	프로젝트가 병렬성에 대한 고려 없이 구현된 경우.
		순차적 환경에서는 매우 잘 동작하지만 병렬 환경에서는 처참하게
		동작하는 기술이 있기 때문에, 만약 당신의 프로젝트가 순차적
		하드웨어에서만 그동안 사용되어왔다면 당신의 프로젝트는 분명
		병렬성에 친화적이지 못한 코드를 최소한 사용은 하고 있을 겁니다.

	\iffalse
	\item	The project was implemented without regard to parallelism.
		Given that there are a great many techniques that work
		extremely well in a sequential environment, but that
		fail miserably in parallel environments, if your project
		ran only on sequential hardware for most of its lifetime,
		then your project undoubtably has at least its share of
		parallel-unfriendly code.
	\fi

	\item	프로젝트가 좋은 소프트웨어 개발 관습에 대한 고려 없이 구현된
		경우.
		잔혹한 사실은, 공유 메모리 병렬 환경은 종종 순차적 환경에 비해
		대충 만들어진 개발 관습에 더 엄혹하다는 것입니다.
		이 경우에는 병렬성을 도입하기 전에 먼저 기존의 설계와 코드를
		재정리 해야할 겁니다.

	\iffalse
	\item	The project was implemented without regard to good
		software-development practice.
		The cruel truth is that shared-memory parallel
		environments are often much less forgiving of sloppy
		development practices than are sequential environments.
		You may be well-served to clean up the existing design
		and code prior to attempting parallelization.
	\fi

	\item	당신의 프로젝트를 처음 개발한 사람들이 여전히 관리 권한을 쥐고
		있거나 작은 기능 정도는 추가할 수 있는 기능을 가지고 있지만
		``커다란'' 변경은 할 수 없는 경우.
		이런 경우에는 당신이 매우 간단하게 당신의 프로젝트를 병렬화 할
		수 있다 해도, 순차적인 채로 놔두는게 최선일 수 있습니다.
		그렇다 해도 여러 인스턴스를 수행시킨다던지, 많이 사용하는
		라이브러리의 병렬적 구현체를 사용한다던지, database 와 같은
		다른 병렬 프로젝트의 사용을 하도록 한다던지와 같이 간단하게
		당신의 프로젝트를 병렬화 시킬 수 있는 방법이 있습니다.

	\iffalse
	\item	The people who originally did the development on your
		project have since moved on, and the people remaining,
		while well able to maintain it or add small features,
		are unable to make ``big animal'' changes.
		In this case, unless you can work out a very simple
		way to parallelize your project, you will probably
		be best off leaving it sequential.
		That said, there are a number of simple approaches that
		you might use
		to parallelize your project, including running multiple
		instances of it, using a parallel implementation of
		some heavily used library function, or making use of
		some other parallel project, such as a database.
	\fi
	\end{enumerate}

	이런 문제들은 비기술적인 요소들이라고 말할 수도 있겠죠, 하지만 그렇다고
	이것들이 비현실적이지도 않습니다.
	요약하자면, 커다란 코드의 병렬화는 크고 복잡한 노력을 필요로 할 수
	있습니다.
	그리고 크고 복잡한 노력이 필요하다면, 그 숙제를 가능한 빨리 해결하는게
	낫겠죠.

	\iffalse
	One can argue that many of these obstacles are non-technical
	in nature, but that does not make them any less real.
	In short, parallelization of a large body of code
	can be a large and complex effort.
	As with any large and complex effort, it makes sense to
	do your homework beforehand.
	\fi

\QuickQAC{chp:Hardware and its Habits}{Hardware and its Habits}
\QuickQ{}
	왜 병렬 프로그래머가 하드웨어의 로우 레벨 요소들까지 배워야 하죠?
	하이 레벨의 추상 계층만 보는게 더 쉽고, 낫고, 더 일반적이지 않겠어요?

	\iffalse
	Why should parallel programmers bother learning low-level
	properties of the hardware?
	Wouldn't it be easier, better, and more general to remain at
	a higher level of abstraction?
	\fi
\QuickA{}
	하드웨어의 세세한 내용들은 무시하는게 더 쉬울 수 있을 겁니다만,
	많은 경우 그건 바보같은 짓일 수 있습니다.
	병렬성의 모든 목적이 성능 향상일 뿐이란걸 인정하신다면, 그리고 성능은
	하드웨어의 디테일한 부분들에 의존적인 걸 인정하신다면, 논리적으로 병렬
	프로그래머들은 하드웨어에 대해 최소 조금은 알아야 한다는 결론을 얻을 수
	있을 겁니다.

	이건 대부분의 엔지니어링 교훈에서 나오는 이야기입니다.
	\emph{당신}이라면 콘크리트와 철강에 대해 이해하지 못하는 엔지니어가
	설계한 다리를 사용하시겠습니까?
	아니라면, 왜 병렬 프로그래머가 최소한 \emph{조금의} 하드웨어에 대한
	이해 없이 훌륭한 병렬 소프트웨어를 만들 수 있을 거라고 생각하시나요?

\iffalse
	It might well be easier to ignore the detailed properties of
	the hardware, but in most cases it would be quite foolish
	to do so.
	If you accept that the only purpose of parallelism is to
	increase performance, and if you further accept that
	performance depends on detailed properties of the hardware,
	then it logically follows that parallel programmers are going
	to need to know at least a few hardware properties.

	This is the case in most engineering disciplines.
	Would \emph{you} want to use a bridge designed by an
	engineer who did not understand the properties of
	the concrete and steel making up that bridge?
	If not, why would you expect a parallel programmer to be
	able to develop competent parallel software without at least
	\emph{some} understanding of the underlying hardware?
\fi

\QuickQ{}
	어떤 기계가 복수 데이터 요소에 대한 어토믹 오퍼레이션을 허용하겠어요?

	\iffalse
	What types of machines would allow atomic operations on
	multiple data elements?
	\fi
\QuickA{}
	이 질문에 대한 한가지 답은 종종 복수개의 데이터 요소를 어토믹하게
	다뤄질 수 있는, 단일 머신 워드 안에 모아넣을 수 있다는 겁니다.

	좀 더 트렌디한 답은 트랜잭셔널 메모리~\cite{DBLomet1977SIGSOFT} 를
	지원하는 기계가 되겠습니다.
	2014년 초에 이르러서는 일부 주요 시스템들이 제한되긴 했지만 하드웨어
	트랜잭셔널 메모리 구현을 제공합니다. 더 자세한 내용은
	Section~\ref{sec:future:Hardware Transactional Memory} 에서 다루고
	있습니다.
	소프트웨어 트랜잭셔널
	메모리~\cite{McKenney2007PLOSTM,DonaldEPorter2007TRANSACT,
	ChistopherJRossbach2007a,CalinCascaval2008tmtoy,
	AleksandarDragovejic2011STMnotToy,AlexanderMatveev2012PessimisticTM}
	에 대해서는 아직 적합하지 않다는 평가입니다.
	소프트웨어 트랜잭셔널 메모리에 대한 더 많은 내용은
	Section~\ref{sec:future:Transactional Memory} 에서 볼 수 있을 겁니다.

	\iffalse
	One answer to this question is that it is often possible to
	pack multiple elements of data into a single machine word,
	which can then be manipulated atomically.

	A more trendy answer would be machines supporting transactional
	memory~\cite{DBLomet1977SIGSOFT}.
	As of early 2014, several mainstream systems provide limited
	hardware transactional memory implementations, which is covered
	in more detail in
	Section~\ref{sec:future:Hardware Transactional Memory}.
	The jury is still out on the applicability of software transactional
	memory~\cite{McKenney2007PLOSTM,DonaldEPorter2007TRANSACT,
	ChistopherJRossbach2007a,CalinCascaval2008tmtoy,
	AleksandarDragovejic2011STMnotToy,AlexanderMatveev2012PessimisticTM}.
	Additional information on software transactional memory may be
	found in
	Section~\ref{sec:future:Transactional Memory}.
	\fi

\QuickQ{}
	그래서, CPU 설계자들은 캐시 미스 오버헤드 역시 많이 개선 했나요?
	\iffalse
	So have CPU designers also greatly reduced the overhead of
	cache misses?
	\fi
\QuickA{}
	안타깝지만, 그렇게 많은 개선은 하지 못했습니다.
	약간 오버헤드를 줄인 CPU 들도 있었습니다만, 빛의 속도의 한계와 물질의
	원자성의 자연 법칙이 큰 시스템에서 캐시 미스 오버헤드를 줄일 수 있는
	방법을 제한하고 있습니다.
	Section~\ref{sec:cpu:Hardware Free Lunch?} 에서 가능할 법한 미래의 개선
	방법들을 논의해 봅니다.

	\iffalse
	Unfortunately, not so much.
	There has been some reduction given constant numbers of CPUs,
	but the finite speed of light and the atomic nature of
	matter limits their ability to reduce cache-miss overhead
	for larger systems.
	Section~\ref{sec:cpu:Hardware Free Lunch?}
	discusses some possible avenues for possible future progress.
	\fi

\QuickQ{}
	이제 \emph{간략화된} 거라구요?
	이것보다 더 복잡한게 어떻게 \emph{가능하죠}?
	\iffalse
	This is a \emph{simplified} sequence of events?
	How could it \emph{possibly} be any more complex?
	\fi
\QuickA{}
	이 예는 다음을 포함해 몇가지 가능한 복잡한 경우를 뺐습니다:
	\begin{enumerate}
	\item	해당 캐시라인에 대해 다른 CPU 들도 동사에 CAS 오퍼레이션을
		수행하려 하고 있을 수 있습니다.
	\item	해당 캐시라인은 리드 온리로 다른 CPU 들의 캐시들에 복사되어
		있을 수 있는데, 이 경우엔 그 캐시들도 비워야 할 필요가
		생깁니다.
	\item	CPU~7 은 해당 요청이 도착했을 때 해당 캐시 라인에 뭔가 연산을
		수행하고 있었을 수 있고, 이 경우 CPU~7 은 자신의 연산이 끝날
		때까지 해당 요청을 잠시 대기하고 있게 해야 합니다.
	\item	CPU~7 은 (예를 들어, 다른 데이터를 위한 공간을 만들기 위해)
		해당 캐시라인을 캐시에서 없앴을 수 있고, 이로 인해 요청이
		도착한 시점에서는 캐시라인이 메모리에 있을 수 있습니다.
	\item	캐시라인에서 고칠 수 있는 에러가 났을 수 있는데, 그렇다면 해당
		데이터가 사용되기 전에 그 에러는 고쳐져야 합니다.
	\iffalse
	This sequence ignored a number of possible complications,
	including:

	\begin{enumerate}
	\item	Other CPUs might be concurrently attempting to perform
		CAS operations involving this same cacheline.
	\item	The cacheline might have been replicated read-only in
		several CPUs' caches, in which case, it would need to
		be flushed from their caches.
	\item	CPU~7 might have been operating on the cache line when
		the request for it arrived, in which case CPU~7 might
		need to hold off the request until its own operation
		completed.
	\item	CPU~7 might have ejected the cacheline from its cache
		(for example, in order to make room for other data),
		so that by the time that the request arrived, the
		cacheline was on its way to memory.
	\item	A correctable error might have occurred in the cacheline,
		which would then need to be corrected at some point before
		the data was used.
	\fi
	\end{enumerate}

	제품 품질의 캐시 일관성 메커니즘들은 이런 종류의 여러 복잡한
	경우~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}
	때문에 엄청나게 복잡합니다.

	\iffalse
	Production-quality cache-coherence mechanisms are extremely
	complicated due to these sorts of
	considerations~\cite{Hennessy95a,DavidECuller1999,MiloMKMartin2012scale,DanielJSorin2011MemModel}.
	\fi


\QuickQ{}
	왜 CPU~7 의 캐시에서 해당 캐시라인을 비워야 하죠?

	\iffalse
	Why is it necessary to flush the cacheline from CPU~7's cache?
	\fi
\QuickA{}
	만약 해당 캐시라인이 CPU~7 의 캐시에서 비워지지 않는다면, CPU~0 과
	CPU~7 은 같은 변수에 대해 서로 다른 값을 보게 될 겁니다.
	이런 종류의 비일관성은 병렬 소프트웨어를 매우 복잡하게 만들 수 있고,
	때문에 하드웨어 설계자들은 그런 문제를 없애려 노력해 왔습니다.
	\iffalse
	If the cacheline was not flushed from CPU~7's cache, then
	CPUs~0 and 7 might have different values for the same set
	of variables in the cacheline.
	This sort of incoherence would greatly complicate parallel
	software, and so hardware architects have been convinced to
	avoid it.
	\fi

\QuickQ{}
	하드웨어 설계자들은 분명 이 상황을 개선하려 노력할 수 있었을 거예요!
	왜 그들은 이 단일 인스트럭션 오퍼레이션들의 끔찍한 성능을 만족하고
	있는거죠?
	\iffalse
	Surely the hardware designers could be persuaded to improve
	this situation!
	Why have they been content with such abysmal performance
	for these single-instruction operations?
	\fi
\QuickA{}
	하드웨어 설계자들은 이 문제를 해결하려 노력\emph{했었}습니다만,
	물리학자 스티븐 호킹 정도의 권위자에게만 조언을 얻었습니다.
	호킹은 하드웨어 설계자들이 두개의 기본 문제~\cite{BryanGardiner2007}를
	가지고 있음을 발견했습니다:

	\begin{enumerate}
	\item	빛의 한계 속도, 그리고
	\item	물질의 원자적 본성.
	\end{enumerate}

	\iffalse
	The hardware designers \emph{have} been working on this
	problem, and have consulted with no less a luminary than
	the physicist Stephen Hawking.
	Hawking's observation was that the hardware designers have
	two basic problems~\cite{BryanGardiner2007}:

	\begin{enumerate}
	\item	the finite speed of light, and
	\item	the atomic nature of matter.
	\end{enumerate}
	\fi

\begin{table}
\centering
\begin{tabular}{l||r|r}
				& 	 	& Ratio \\
	Operation		& Cost (ns) 	& (cost/clock) \\
	\hline
	\hline
	Clock period		&           0.4	&           1.0 \\
	\hline
	``Best-case'' CAS	&          12.2	&          33.8 \\
	\hline
	Best-case lock		&          25.6	&          71.2 \\
	\hline
	Single cache miss	&          12.9	&          35.8 \\
	\hline
	CAS cache miss		&           7.0	&          19.4 \\
	\hline
	Off-Core		&		&		\\
	\hline
	Single cache miss	&          31.2	&          86.6 \\
	\hline
	CAS cache miss		&          31.2	&          86.5 \\
	\hline
	Off-Socket		&		&		\\
	\hline
	Single cache miss	&          92.4	&         256.7 \\
	\hline
	CAS cache miss		&          95.9	&         266.4 \\
	\hline
	Comms Fabric		&       4,500	&       7,500 \\
	\hline
	Global Comms		& 195,000,000	& 324,000,000 \\
\end{tabular}
\caption{Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
\label{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
\end{table}

	첫번째 문제는 기본 속도를 제한하고, 두번째 문제는 동작의 소형화를
	제한하여 결과적으로 단위시간당 가능한 오퍼레이션의 갯수를 제한합니다.
	그리고 이 문제는 설령, 현재 상품화된 CPU 들의 속도를 10 GHz 아래로
	제한하고 있는, 에너지 소비 문제를 피해간다 해도 존재합니다.

	Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU
	2.8GHz Intel X5550 (Nehalem) System} 과
	페이지~\pageref{tab:cpu:Performance of Synchronization Mechanisms on
	4-CPU 1.8GHz AMD Opteron 844 System} 의 Table~\ref{tab:cpu:Performance
	of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}
	를 비교해 보면 알 수 있겠지만, 분명 개선은 이루어지고 있습니다.
	하드웨어 쓰레드들을 단일 코어에 집어넣고 복수의 코어들을 하나의 다이에
	넣는 것은 적어도 싱글 코어내에서 또는 단일 다이 내에서의 반응속도는
	엄청나게 개선했습니다.
	전체 시스템 반응속도에서도 일부 개선이 있었습니다만, 겨우 대략 두배
	정도입니다.
	안타깝지만, 지난 몇년간 빛의 속도나 물질의 원자성의 본질은 변하지
	않았습니다.

	Section~\ref{sec:cpu:Hardware Free Lunch?} 에서는 병렬 프로그래머들의
	곤경을 완화시키기 위해 어떤 일을 해줄 수 있을지 알아봅니다.

	\iffalse
	The first problem limits raw speed, and the second limits
	miniaturization, which in turn limits frequency.
	And even this sidesteps the power-consumption issue that
	is currently holding production frequencies to well below
	10 GHz.

	Nevertheless, some progress is being made, as may be seen
	by comparing
	Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 16-CPU 2.8GHz Intel X5550 (Nehalem) System}
	with
	Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}
	on
	page~\pageref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}.
	Integration of hardware threads in a single core and multiple
	cores on a die have improved latencies greatly, at least within the
	confines of a single core or single die.
	There has been some improvement in overall system latency,
	but only by about a factor of two.
	Unfortunately, neither the speed of light nor the atomic nature
	of matter has changed much in the past few years.

	Section~\ref{sec:cpu:Hardware Free Lunch?}
	looks at what else hardware designers might be
	able to do to ease the plight of parallel programmers.
	\fi

\QuickQ{}
	숫자가 미친듯이 크군요!
	어떡해야 제 머리로 이걸 이해할 수 있을까요?
	\iffalse
	These numbers are insanely large!
	How can I possibly get my head around them?
	\fi
\QuickA{}
	휴지 한 롤을 가져오세요.
	미제 휴지라면, 한 롤의 휴지는 약 350-500 조각의 휴지로 구성됩니다.
	한개 조각이 하나의 클락 사이클이라고 생각하고, 휴지를 모두 풀어 보세요.

	그게 하나의 CAS 캐시 미스를 의미한다고 보면 됩니다.

	더 비싼 시스템간 커뮤니케이션 대기시간을 보려면, 몇개의 휴지 롤 (또는
	여러 휴지 케이스들) 이 필요할 겁니다.

	중요한 팁 하나: 당신이 살아가면서 필요한 휴지가 얼마나 되는지도
	생각해보세요!

	\iffalse
	Get a roll of toilet paper.
	In the USA, each roll will normally have somewhere around 350-500
	sheets.
	Tear off one sheet to represent a single clock cycle, setting it aside.
	Now unroll the rest of the roll.

	The resulting pile of toilet paper will likely represent a single
	CAS cache miss.

	For the more-expensive inter-system communications latencies,
	use several rolls (or multiple cases) of toilet paper to represent
	the communications latency.

	Important safety tip: make sure to account for the needs of
	those you live with when appropriating toilet paper!
	\fi

\QuickQ{}
	하지만 개별의 전자들은 컨덕터 내에서조차도 그렇게 빠르지 않아요!!!
	세미컨덕터에서 발견된 저전력의 컨덕터 안에서의 전자 이동 속도는 초당
	겨우 1 \emph{밀리미터} 정도라구요.
	뭔가요???

	\iffalse
	But individual electrons don't move anywhere near that fast,
	even in conductors!!!
	The electron drift velocity in a conductor under the low voltages
	found in semiconductors is on the order of only one \emph{millimeter}
	per second.
	What gives???
	\fi
\QuickA{}
	전자 이동 속도는 긴 시간동안의 개별 전자들의 이동을 추적합니다.
	개별 전자들은 꽤 무작위적으로 튀어다니고, 따라서 그들의 순간 속도는
	매우 빠르지만 긴 시간으로 보게 되면 그렇게 멀리 이동하지는 않습니다.
	여기서, 전자들은 대부분의 시간을 고속으로 이동하는데 소모하지만 긴
	시간으로 보면 어디에도 가지 않는 통근자와도 같습니다.
	이런 통근자들의 속도는 시속 70 마일(113 킬로미터) 정도지만, 지구의
	표면에 비교해 보는 긴 시간동안의 이동 속도는 제로에 가까울 겁니다.

	\iffalse
	Electron drift velocity tracks the long-term movement of individual
	electrons.
	It turns out that individual electrons bounce around quite
	randomly, so that their instantaneous speed is very high, but
	over the long term, they don't move very far.
	In this, electrons resemble long-distance commuters, who
	might spend most of their time traveling at full highway
	speed, but over the long term going nowhere.
	These commuters' speed might be 70 miles per hour
	(113 kilometers per hour), but their long-term drift velocity
	relative to the planet's surface is zero.
	\fi

	따라서, 우리는 전자의 이동속도가 아니라 순간적 속도에 주의를 기울여야
	합니다.
	하지만, 전자의 순간적 속도라 하더라도 빛의 속도에는 발끝도 따라가지
	못합니다.
	컨덕터에서 측정된 전자파의 속도는 더도 아니고 덜도 아니고 빛의 속도의
	발끝은 따라가고 있는데, 이 때문에 여전히 미스테리는 풀리지 않습니다.

	\iffalse
	Therefore, we should pay attention not to the electrons'
	drift velocity, but to their instantaneous velocities.
	However, even their instantaneous velocities are nowhere near
	a significant fraction of the speed of light.
	Nevertheless, the measured velocity of electric waves
	in conductors \emph{is} a substantial fraction of the
	speed of light, so we still have a mystery on our hands.
	\fi

	하나 더 있는 트릭은 전자는 그 음극의 성질로 인해 다른 전자와
	상당히(원자적 관점에서요) 상호착용을 한다는 것입니다.
	이 상호작용은 광자에 의해 이끌어지는데, 광자는 \emph{바로} 빛의 속도로
	움직입니다.
	따라서 전기학에서의 전자라 해도, 대부분의 일을 하는건 광자입니다.

	통근자 비유를 이어가 보자면, 운전자는 다른 운전자에게 사고나 교통
	혼잡들을 알리는데 스마트폰을 사용할 수 있고, 이로 인해 교통 상황의
	변화를 개별 차들의 순간 속도보다 훨씬 빠르게 전파할 수 있는 겁니다.
	이 전기학과 교통상황 사이의 비유를 요약하자면 다음과 같습니다:

	\iffalse
	The other trick is that electrons interact with each other at
	significant distances (from an atomic perspective, anyway),
	courtesy of their negative charge.
	This interaction is carried out by photons, which \emph{do}
	move at the speed of light.
	So even with electricity's electrons, it is photons
	doing most of the fast footwork.

	Extending the commuter analogy, a driver might use a smartphone
	to inform other drivers of an accident or congestion, thus
	allowing a change in traffic flow to propagate much faster
	than the instantaneous velocity of the individual cars.
	Summarizing the analogy between electricity and traffic flow:
	\fi

	\begin{enumerate}
	\item	전자의 (매우 낮은) 이동 속도는 통근자의 장시간 속도와 비슷해서,
		둘 다 제로에 가깝습니다.
	\item	전자의 (여전히 낮은) 순간 속도는 통행 중인 차의 순간 속도와 비슷합니다.
		둘 다 이동 속도에 비해선 높지만, 변화가 전달되는 속도에 비교하면 굉장이 작습니다.
	\item	전자파의 (훨씬 높은) 전달 속도는 대부분 전자들 사이에서
		전자기력을 전달하는 광자의 덕분입니다.
		유사하게, 교통 상황은 운전자 사이의 커뮤니케이션으로 인해 훨씬
		빠르게 바뀔 수 있습니다.
		이것은 이미 교통 혼잡에 빠져 있는 운전자에겐 큰 도움이 되지
		않듯이, 이미 주어진 캐퍼시터에 잡혀 있는 전자들에겐 큰 도움이
		되지 않습니다.
	\end{enumerate}

	\iffalse
	\begin{enumerate}
	\item	The (very low) drift velocity of an electron is similar
		to the long-term velocity of a commuter, both being
		very nearly zero.
	\item	The (still rather low) instantaneous velocity of
		an electron is similar to the instantaneous velocity
		of a car in traffic.
		Both are much higher than the drift velocity, but
		quite small compared to the rate at which changes
		propagate.
	\item	The (much higher) propagation velocity of an electric
		wave is primarily due to photons transmitting
		electromagnetic force among the electrons.
		Similarly, traffic patterns can change quite quickly
		due to communication among drivers.
		Not that this is necessarily of much help to the
		drivers already stuck in traffic, any more than it
		is to the electrons already pooled in a given capacitor.
	\end{enumerate}
	\fi

	물론, 이 주제를 완전히 이해하려면 전자기학을 공부해야 할겁니다.

	\iffalse
	Of course, to fully understand this topic, you should read
	up on electrodynamics.
	\fi

\QuickQ{}
	분산 시스템에서 통신이 그렇게까지 비싸다면 누가, 그리고 왜 그걸 쓰려
	하는 건가요?
	\iffalse
	Given that distributed-systems communication is so horribly
	expensive, why does anyone bother with them?
	\fi
\QuickA{}
	몇가지 이유가 있지요:
	\iffalse
	There are a number of reasons:
	\fi

	\begin{enumerate}
	\item	공유 메모리 멀티 프로세서 시스템은 크기 제한이 있습니다.
		수천개 이상의 CPU가 필요하다면, 분산 시스템을 사용하는 것밖에
		선택지가 없습니다.
	\item	극단적으로 거대한 공유 메모리 시스템은 매우 비싸고,
		Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on
		4-CPU 1.8GHz AMD Opteron 844 System} 에 나타난 것처럼 작은 네개
		CPU 로 구성된 시스템에서보다도 긴 캐시 미스 대기시간을 갖는
		경향을 보입니다..
	\item	분산 시스템에서의 통신 대기시간은 CPU 를 사용하지 않고, 따라서
		메세지가 전달되는 동안 컴퓨팅 연산을 병렬적으로 수행할 수
		있습니다.
	\item	많은 중요한 문제들은 ``당황스럽도록 병렬적'' 이라서 극단적일
		정도로 거대한 연산 단위들이 매우 작은 수의 메세지만으로 가능해
		질수도 있습니다.
		SETI@HOME~\cite{SETIatHOME2008} 은 그런 어플리케이션의 한
		예입니다.
		이런 부류의 어플리케이션들은 극단적으로 긴 통신 대기시간에도
		불구하고 컴퓨터 네트워크를 훌륭하게 사용할 수 있습니다.

	\iffalse
	\item	Shared-memory multiprocessor systems have strict size limits.
		If you need more than a few thousand CPUs, you have no
		choice but to use a distributed system.
	\item	Extremely large shared-memory systems tend to be
		quite expensive and to have even longer cache-miss
		latencies than does the small four-CPU system
		shown in
		Table~\ref{tab:cpu:Performance of Synchronization Mechanisms on 4-CPU 1.8GHz AMD Opteron 844 System}.
	\item	The distributed-systems communications latencies do
		not necessarily consume the CPU, which can often allow
		computation to proceed in parallel with message transfer.
	\item	Many important problems are ``embarrassingly parallel'',
		so that extremely large quantities of processing may
		be enabled by a very small number of messages.
		SETI@HOME~\cite{SETIatHOME2008}
		is but one example of such an application.
		These sorts of applications can make good use of networks
		of computers despite extremely long communications
		latencies.
	\fi
	\end{enumerate}

	병렬 어플리케이션에서의 향후 노력은 긴 통신 대기시간을 가진 기계와,
	또는 클러스터에서 잘돌아갈 수 있는 당황스럽도록 병렬적인 어플리케이션의
	수를 늘려가는 것을 계속할 것입니다.
	그렇다곤 해도, 하드웨어 대기시간을 크게 줄이는 것은 개발에 크게 도움이
	될겁니다.

	\iffalse
	It is likely that continued work on parallel applications will
	increase the number of embarrassingly parallel applications that
	can run well on machines and/or clusters having long communications
	latencies.
	That said, greatly reduced hardware latencies would be an
	extremely welcome development.
	\fi

\QuickQ{}
	좋아요, 우리가 분산 프로그래밍 기법들을 공유 메모리 병렬 프로그램에
	적용하려 한다면, 항상 이런 분산 기법들을 사용하고 공유 메모리 없이 살면
	안되나요?

	\iffalse
	OK, if we are going to have to apply distributed-programming
	techniques to shared-memory parallel programs, why not just
	always use these distributed techniques and dispense with
	shared memory?
	\fi
\QuickA{}
	많은 경우 프로그램의 작은 부분만이 성능에 민감하기 때문입니다.
	공유 메모리 병렬성은 우리가 그 작은 부분에의 분산 프로그래밍에
	집중하고, 성능에 민감하지 않은 프로램의 대붑분의 영역은 간단한 공유
	메모리 기법을 사용하도록 해줍니다.

	\iffalse
	Because it is often the case that only a small fraction of
	the program is performance-critical.
	Shared-memory parallelism allows us to focus distributed-programming
	techniques on that small fraction, allowing simpler shared-memory
	techniques to be used on the non-performance-critical bulk of
	the program.
	\fi

\QuickQAC{chp:Tools of the Trade}{Tools of the Trade}
\QuickQ{}
	하지만 이 간단한 셸 스크립트는 \emph{진짜} 병렬 프로그램이 아니잖아요!
	왜 이런 별거아닌 걸 신경쓰는거죠???

	\iffalse
	But this silly shell script isn't a \emph{real} parallel program!
	Why bother with such trivia???
	\fi
\QuickA{}
	당신은 \emph{결코} 이 간단한 것을 잊을 수 없을 것이기 때문입니다!

	이 책의 제목이 ``Is Parallel Programming Hard, And, If So, What Can You
	Do About It?'' 이란 걸 마음에 새겨 두십시오.
	당신이 할 수 있는 가장 효과적인 일은 그 간단한 것을 잊지 않도록 하는
	것입니다!
	무엇보다, 당신이 병렬 프로그래밍을 어려운 방법으로 하기로 선택했다면,
	당신의 선택이니, 당신은 당신 자신 외의 누구에게도 불평 할 수 없습니다.

	\iffalse
	Because you should \emph{never} forget the simple stuff!

	Please keep in mind that the title of this book is
	``Is Parallel Programming Hard, And, If So, What Can You Do About It?''.
	One of the most effective things you can do about it is to
	avoid forgetting the simple stuff!
	After all, if you choose to do parallel programming the hard
	way, you have no one but yourself to blame.
	\fi

\QuickQ{}
	병렬 셸 스크립트를 작성하는 좀 더 간단한 방법은 없나요?
	만약 있다면, 어떻게 하나요? 없다면, 왜 없죠?

	\iffalse
	Is there a simpler way to create a parallel shell script?
	If so, how?  If not, why not?
	\fi
\QuickA{}
	가장 직관적인 방법은 셸 파이프라인입니다:

	\iffalse
	One straightforward approach is the shell pipeline:
	\fi
\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
grep $pattern1 | sed -e 's/a/b/' | sort
\end{verbatim}
\end{minipage}
\vspace{5pt}
	충분히 커다란 입력 파일에 대해서, \co{grep} 의 패턴 매칭, \co{sed} 의
	수정과 \co{sort} 의 입력물 처리는 병렬적으로 수행될 겁니다.
	\co{parallel.sh} 파일에 셸 스크립트 병렬성과 파이프라인에 대한 데모가
	있습니다.

	\iffalse
	For a sufficiently large input file,
	\co{grep} will pattern-match in parallel with \co{sed}
	editing and with the input processing of \co{sort}.
	See the file \co{parallel.sh} for a demonstration of
	shell-script parallelism and pipelining.
	\fi

\QuickQ{}
	하지만 스크립트 기반 병렬 프로그래밍이 그렇게 쉽다면, 왜 다른 것들을
	신경쓰는거죠?

	\iffalse
	But if script-based parallel programming is so easy, why
	bother with anything else?
	\fi
\QuickA{}
	사실 오늘날 사용되는 병렬 프로그램들의 매우 많은 부분들이 스크립트에
	기반합니다.
	하지만, 스크립트 기반 병렬성은 한계점도 지니고 있습니다:
	\begin{enumerate}
	\item	새 프로세스의 생성은 보통 비싼 시스템콜인 \co{fork()} 와
		\co{exec()} 를 포함하기 때문에 상당히 무거운 작업입니다.
	\item	파이프라이닝을 포함해서 데이터의 공유는 일반적으로 비싼 file
		I/O 를 포함합니다.
	\item	스크립트에서 믿고 쓸 수 있는 사용 가능한 동기화 기본 도구들
		역시 일반적으로 비싼 file I/O 를 포함합니다.
	\end{enumerate}
	이런 제한점들은 스크립트 기반 병렬성이 coarse-grained 병렬성을 사용하고
	각 일의 단위들은 최소 수십 밀리세컨드, 그리고 가능하다면 그보다도 훨씬
	긴 시간을 가질 것을 요구합니다.

	\iffalse
	In fact, it is quite likely that a very large fraction of
	parallel programs in use today are script-based.
	However, script-based parallelism does have its limitations:
	\begin{enumerate}
	\item	Creation of new processes is usually quite heavyweight,
		involving the expensive \co{fork()} and \co{exec()}
		system calls.
	\item	Sharing of data, including pipelining, typically involves
		expensive file I/O.
	\item	The reliable synchronization primitives available to
		scripts also typically involve expensive file I/O.
	\end{enumerate}
	These limitations require that script-based parallelism use
	coarse-grained parallelism, with each unit of work having
	execution time of at least tens of milliseconds, and preferably
	much longer.
	\fi

	finer-grained 병렬성을 필요로 하는 작업들은 그 작업의 문제가
	coarse-grained 형태로 표현될 수는 없을지 좀 고민해 보도록 추천됩니다.
	만약 불가능하다면, Section~\ref{sec:toolsoftrade:POSIX Multiprocessing}
	에서 다루는 것과 같은 다른 병렬 프로그래밍 환경을 고려해 봐야 합니다.

	\iffalse
	Those requiring finer-grained parallelism are well advised to
	think hard about their problem to see if it can be expressed
	in a coarse-grained form.
	If not, they should consider using other parallel-programming
	environments, such as those discussed in
	Section~\ref{sec:toolsoftrade:POSIX Multiprocessing}.
	\fi

\QuickQ{}
	왜 이 \co{wait()} 함수는 그렇게 복잡해야만 하는거죠?
	왜 그냥 셸 스크립트의 \co{wait} 같이 동작하도록 만들지 않는 거예요?

	\iffalse
	Why does this \co{wait()} primitive need to be so complicated?
	Why not just make it work like the shell-script \co{wait} does?
	\fi
\QuickA{}
	일부 병렬 어플리케이션은 특정 자식 프로세스가 끝났을 때 특별한 행동을
	취해야 할 수 있고, 그 때문에 각 자식 프로세스에 대해 개별적으로 대기를
	할 필요가 있습니다.
	또한, 일부 병렬 어플리케이션들은 자식 프로세스가 종료된 이유를 알
	필요도 있습니다.
	Figure~\ref{fig:toolsoftrade:Using the wait() Primitive} 에서 본
	것처럼, \co{wait()} 함수를 가지고 \co{waitall()} 함수를 만드는 건
	어렵지 않습니다만 그 반대는 불가능하겠지요.
	한번 특정 자식 프로세스에 대한 정보를 잃어버리면, 그건 복구될 수
	없습니다.

	\iffalse
	Some parallel applications need to take special action when
	specific children exit, and therefore need to wait for each
	child individually.
	In addition, some parallel applications need to detect the
	reason that the child died.
	As we saw in Figure~\ref{fig:toolsoftrade:Using the wait() Primitive},
	it is not hard to build a \co{waitall()} function out of
	the \co{wait()} function, but it would be impossible to
	do the reverse.
	Once the information about a specific child is lost, it is lost.
	\fi

\QuickQ{}
	여기서 이야기한 것 외에도 \co{fork()} 와 \co{wait()} 에 대해 이야기할
	것들이 많지 않나요?

	\iffalse
	Isn't there a lot more to \co{fork()} and \co{wait()}
	than discussed here?
	\fi
\QuickA{}
	맞습니다, 그리고 그리고 이 섹션은 나중에 메세징 기능 (UNIX 파이프,
	TCP/IP, 그리고 공유 파일 I/O 같은) 과 메모리 매핑 (\co{mmap()} 과
	\co{shmget()} 같은) 기능을 포함하도록 확장될 수도 있을 겁니다.
	그 전까지는, 이런 기능들에 대해 훨씬 자세하게 설명하는 다른 교재들이
	많이 있고, 정말 잘 알고 싶다면 man 페이지(역주: UNIX 커맨드
	\co{man})나, 이런 기능을 사용하며 현존하는 병렬 어플리케이션들, 또는
	리눅스 커널 구현의 소스 코드를 참고해도 될 것입니다.

	Figure~\ref{fig:toolsoftrade:Processes Created Via fork() Do Not Share
	Memory} 의 부모 프로세스는 자식 프로세스가 종료될 때까지 자신의
	\co{printf()} 를 위해 기다리고 있다는 것을 기억해 둘 필요가 있습니다.
	\co{printf()} 의 buffered I/O 를 같은 파일에 대해 여러 프로세스에서
	동시적으로 사용하는 것은 일반적이지 않고, 그러지 않는게 최선입니다.
	정말로 동시적으로 buffered I/O 를 해야만 한다면, 당신의 OS 의 문서를
	보세요.
	UNIX/Linux 시스템에서는 Stewart Weiss 의 강의 노트가
	예제~\cite{StewartWeiss2013UNIX} 와 함께 좋은 소개를 제공합니다.

	\iffalse
	Indeed there is, and
	it is quite possible that this section will be expanded in
	future versions to include messaging features (such as UNIX
	pipes, TCP/IP, and shared file I/O) and memory mapping
	(such as \co{mmap()} and \co{shmget()}).
	In the meantime, there are any number of textbooks that cover
	these primitives in great detail,
	and the truly motivated can read manpages, existing parallel
	applications using these primitives, as well as the
	source code of the Linux-kernel implementations themselves.

	It is important to note that the parent process in
	Figure~\ref{fig:toolsoftrade:Processes Created Via fork() Do Not Share Memory}
	waits until after the child terminates to do its \co{printf()}.
	Using \co{printf()}'s buffered I/O concurrently to the same file
	from multiple processes is non-trivial, and is best avoided.
	If you really need to do concurrent buffered I/O,
	consult the documentation for your OS.
	For UNIX/Linux systems, Stewart Weiss's lecture notes provide
	a good introduction with informative
	examples~\cite{StewartWeiss2013UNIX}.
	\fi

\QuickQ{}
	Figure~\ref{fig:toolsoftrade:Threads Created Via pthread-create() Share
	Memory} 의 \co{mythread()} 함수가 그냥 리턴해도 된다면, 왜
	\co{pthread_exit()} 를 신경써야하죠?

	\iffalse
	If the \co{mythread()} function in
	Figure~\ref{fig:toolsoftrade:Threads Created Via pthread-create() Share Memory}
	can simply return, why bother with \co{pthread_exit()}?
	\fi
\QuickA{}
	이 간단한 예제에서는 \co{pthread_exit()} 를 신경 쓸 이유가 없는게
	맞습니다.
	하지만, \co{mythread()} 가 별도로 컴파일된 다른 함수를 호출하는 경우를
	생각해 봅시다.
	그런 경우, \co{pthread_exit()} 는 이런 다른 함수들에서도 별도의 다른
	에러들을 리턴하거나 해서 실행 흐름을 \co{mythread()} 에 되돌리거나 할
	필요 없이 곧바로 쓰레드의 실행을 종료시킬 수 있게 합니다.

	\iffalse
	In this simple example, there is no reason whatsoever.
	However, imagine a more complex example, where \co{mythread()}
	invokes other functions, possibly separately compiled.
	In such a case, \co{pthread_exit()} allows these other functions
	to end the thread's execution without having to pass some sort
	of error return all the way back up to \co{mythread()}.
	\fi

\QuickQ{}
	C 언어가 데이터 레이스에 대해 어떤 보장도 하지 않는다면, 왜 리눅스
	커널은 그렇게 많은 데이터 레이스들을 가지고 있는거죠?
	지금 리눅스 커널이 완전 엉망이라고 이야기 하려는 거예요???

	\iffalse
	If the C language makes no guarantees in presence of a data
	race, then why does the Linux kernel have so many data races?
	Are you trying to tell me that the Linux kernel is completely
	broken???
	\fi
\QuickA{}
	아, 하지만 리눅스 커널은 조심스럽게 선택된, 데이터 레이스 상황에서도
	안전한 실행을 가능하게 하는 asm 과 같은 gcc 의 특수한 확장 기능을
	포함하는 C 언어의 슈퍼셋으로 작성되었습니다.
	또한, 리눅스 커널은 데이터 레이스가 특히나 문제가 되는 플랫폼들
	위에서는 동작하지 않습니다.
	예를 들어, 32 비트 포인터와 16 비트 버스를 갖는 임베디드 시스템을
	생각해 보세요.
	그런 시스템에서는 하나의 포인터에 값을 저장하고 읽어오는 데이터
	레이스에서 읽기는 아래쪽 16 비트는 예전 값이고 위쪽 16 비트는 새 값인
	값을 읽어올 수도 있을 겁니다.

	\iffalse
	Ah, but the Linux kernel is written in a carefully selected
	superset of the C language that includes special gcc
	extensions, such as asms, that permit safe execution even
	in presence of data races.
	In addition, the Linux kernel does not run on a number of
	platforms where data races would be especially problematic.
	For an example, consider embedded systems with 32-bit pointers
	and 16-bit busses.
	On such a system, a data race involving a store to and a load
	from a given pointer might well result in the load returning the
	low-order 16 bits of the old value of the pointer concatenated
	with the high-order 16 bits of the new value of the pointer.
	\fi

\QuickQ{}
	제가 여러 쓰레드들이 한번에 같은 락을 쥐고 있게 하고 싶으면 어떻게
	하죠?

	\iffalse
	What if I want several threads to hold the same lock at the
	same time?
	\fi
\QuickA{}
	가장 먼저 당신이 해야할 일은 왜 그러길 원하는지 스스로에게 물어보는
	겁니다.
	만약 답이 ``나는 많은 쓰레드에 의해 읽혀지고 아주 가끔 수정되는 많은
	데이터를 가지고 있기 때문'' 이라면, POSIX 리더-라이터 락이 당신이 찾고
	있는 것일 수 있습니다.
	이것들은 Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking} 에
	소개되어 있습니다.

	여러 쓰레드가 같은 락을 잡고 있는 것과 같은 효과를 얻는 또다른 방법은
	한 쓰레드가 락을 획득하고 나서 \co{pthread_create()} 함수를 이용해 다른
	쓰레드들을 생성하는 것입니다.
	왜 이게 좋은 방법인지는 독자 여러분께서 생각해 보시기 바랍니다.

	\iffalse
	The first thing you should do is to ask yourself why you would
	want to do such a thing.
	If the answer is ``because I have a lot of data that is read
	by many threads, and only occasionally updated'', then
	POSIX reader-writer locks might be what you are looking for.
	These are introduced in
	Section~\ref{sec:toolsoftrade:POSIX Reader-Writer Locking}.

	Another way to get the effect of multiple threads holding
	the same lock is for one thread to acquire the lock, and
	then use \co{pthread_create()} to create the other threads.
	The question of why this would ever be a good idea is left
	to the reader.
	\fi

\QuickQ{}
	왜 그냥 Figure~\ref{fig:toolsoftrade:Demonstration of Exclusive Locks}
	라인~5 에서 \co{lock_reader()} 가 곧바로 \co{pthread_mutex_t} 포인터를
	받도록 하지 않는거죠?

	\iffalse
	Why not simply make the argument to \co{lock_reader()}
	on line~5 of
	Figure~\ref{fig:toolsoftrade:Demonstration of Exclusive Locks}
	be a pointer to a \co{pthread_mutex_t}?
	\fi
\QuickA{}
	\co{lock_reader()} 를 \co{pthread_create()} 에 넘겨야 하기 때문이죠.
	물론 함수를 \co{pthread_create()} 에 넘길 때 캐스팅을 해서 넘길 수도
	있지만, 함수 캐스팅은 좀 보기도 안좋고 간단한 포인터 캐스팅에 비해
	잘 하기가 어렵습니다.

	\iffalse
	Because we will need to pass \co{lock_reader()} to
	\co{pthread_create()}.
	Although we could cast the function when passing it to
	\co{pthread_create()}, function casts are quite a bit
	uglier and harder to get right than are simple pointer casts.
	\fi

\QuickQ{}
	\co{pthread_mutex_t} 의 획득과 해제에 매번 4줄이나 써야한다니 좀
	고통스러울 것 같군요!
	더 나은 방법은 없나요?

	\iffalse
	Writing four lines of code for each acquisition and release
	of a \co{pthread_mutex_t} sure seems painful!
	Isn't there a better way?
	\fi
\QuickA{}
	실로 그렇습니다!
	그리고 그런 이유로, \co{pthread_mutex_lock()} 과
	\co{pthread_mutex_unlock()} 함수들은 보통 이 에러 체킹을 해주는 함수로
	감싸져서 사용되곤 합니다.
	뒤에서, 우리는 이들을 리눅스 커널의 \co{spin_lock()} 과
	\co{spin_unlock()} API 들로 감싸서 사용할 겁니다.

	\iffalse
	Indeed!
	And for that reason, the \co{pthread_mutex_lock()} and
	\co{pthread_mutex_unlock()} primitives are normally wrapped
	in functions that do this error checking.
	Later on, we will wrapper them with the Linux kernel
	\co{spin_lock()} and \co{spin_unlock()} APIs.
	\fi

\QuickQ{}
	``x = 0'' 만이 Figure~\ref{fig:toolsoftrade:Demonstration of Same
	Exclusive Lock} 의 코드에서 발생 가능한 오로지 하나의 결과인가요?
	만약 그렇다면, 왜죠?
	아니라면, 어떤 다른 결과가 가능할까요, 그리고 왜일까요?

	\iffalse
	Is ``x = 0'' the only possible output from the code fragment
	shown in
	Figure~\ref{fig:toolsoftrade:Demonstration of Same Exclusive Lock}?
	If so, why?
	If not, what other output could appear, and why?
	\fi
\QuickA{}
	아닙니다.
	``x = 0'' 가 나온 이유는 \co{lock_reader()} 가 락을 먼저 잡았기
	때문입니다.
	\co{lock_writer()} 가 먼저 락을 잡았다면, 결과는 ``x = 3'' 가 되었을
	것입니다.
	하지만, 해당 코드에서는 \co{lock_reader()} 를 먼저 시작시키고 이 실행은
	멀티프로세서에서 이루어졌기 때문에, 대부분은 일반적으로
	\co{lock_reader()} 가 락을 먼저 잡을 것으로 예상할 수 있을 겁니다.
	하지만, 보장된 건 아니지요, 특히나 바쁜 시스템에서는요.

	\iffalse
	No.
	The reason that ``x = 0'' was output was that \co{lock_reader()}
	acquired the lock first.
	Had \co{lock_writer()} instead acquired the lock first, then
	the output would have been ``x = 3''.
	However, because the code fragment started \co{lock_reader()} first
	and because this run was performed on a multiprocessor,
	one would normally expect \co{lock_reader()} to acquire the
	lock first.
	However, there are no guarantees, especially on a busy system.
	\fi

\QuickQ{}
	서로 다른 락을 사용하는건 쓰레드가 서로 상대의 중간 상태를 볼 수 있는등
	혼란스럽게 할 수 있는 것같은데요.
	잘 짜여진 병렬 프로그램은 이런 혼란을 막기 위해서는 하나의 락만을
	사용해야만 하는 건가요?

	\iffalse
	Using different locks could cause quite a bit of confusion,
	what with threads seeing each others' intermediate states.
	So should well-written parallel programs restrict themselves
	to using a single lock in order to avoid this kind of confusion?
	\fi
\QuickA{}
	가끔은 프로그램을 하나의 전역적인 락만을 사용하면서 잘 동작하고
	확장성도 좋게 작성하는 것도 가능하지만, 그런 프로그램은 좀 예외적인
	경우입니다.
	당신은 좋은 성능과 확장성을 위해선 보통은 여러개의 락을 사용해야
	할겁니다.

	이 규칙에 대해 하나의 가능한 예외는 아직은 연구 단계에 머물러 있는,
	``트랜잭셔널 메모리'' 입니다.
	트랜잭셔널 메모리는 하나의 전역 락을 사용하면서 허용된 최적화를
	사용하고, 추가적으로 롤백을 지원하는 케이스~\cite{HansJBoehm2009HOTPAR}
	로 간략히 생각할 수 있습니다.

	\iffalse
	Although it is sometimes possible to write a program using a
	single global lock that both performs and scales well, such
	programs are exceptions to the rule.
	You will normally need to use multiple locks to attain good
	performance and scalability.

	One possible exception to this rule is ``transactional memory'',
	which is currently a research topic.
	Transactional-memory semantics can be loosely thought of as those
	of a single global lock with optimizations permitted and
	with the addition of rollback~\cite{HansJBoehm2009HOTPAR}.
	\fi

\QuickQ{}
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive
	Locks} 에 보여진 코드에서, \co{lock_reader()} 는 \co{lock_writer()} 가
	생성하는 값 모두를 보도록 보장되어 있나요?
	그렇다면, 또 그렇지 않다면, 왜죠?

	\iffalse
	In the code shown in
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive Locks},
	is \co{lock_reader()} guaranteed to see all the values produced
	by \co{lock_writer()}?
	Why or why not?
	\fi
\QuickA{}
	아닙니다.
	바쁜 시스템에서라면, \co{lock_reader()} 는 \co{lock_writer()} 의 실행이
	완료될 때까지 CPU 를 선점당해 \co{lock_writer()} 의 \co{x} 중간 값을
	\emph{전혀} 볼 수 없을 수도 있습니다.

	\iffalse
	No.
	On a busy system, \co{lock_reader()} might be preempted
	for the entire duration of \co{lock_writer()}'s execution,
	in which case it would not see \emph{any} of \co{lock_writer()}'s
	intermediate states for \co{x}.
	\fi

\QuickQ{}
	잠깐만요!!!
	Figure~\ref{fig:toolsoftrade:Demonstration of Same Exclusive Lock}
	에서는 공유 변수 \co{x} 를 초기화 하지 않았는데, 
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive
	Locks} 에서는 왜 초기화 해야 했던거죠?

	\iffalse
	Wait a minute here!!!
	Figure~\ref{fig:toolsoftrade:Demonstration of Same Exclusive Lock}
	didn't initialize shared variable \co{x},
	so why does it need to be initialized in
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive Locks}?
	\fi
\QuickA{}
	Figure~\ref{fig:toolsoftrade:Demonstration of Exclusive Locks} 의
	라인~3 을 보세요.
	Figure~\ref{fig:toolsoftrade:Demonstration of Same Exclusive Lock} 의
	코드는 먼저 수행되었기 때문에, \co{x} 의 컴파일 타임 초기화에 의존할
	수도 있었습니다.
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive
	Locks} 는 그 다음에 돌았기 때문에, \co{x} 를 다시 초기화 해야 합니다.

	\iffalse
	See line~3 of
	Figure~\ref{fig:toolsoftrade:Demonstration of Exclusive Locks}.
	Because the code in
	Figure~\ref{fig:toolsoftrade:Demonstration of Same Exclusive Lock}
	ran first, it could rely on the compile-time initialization of
	\co{x}.
	The code in
	Figure~\ref{fig:toolsoftrade:Demonstration of Different Exclusive Locks}
	ran next, so it had to re-initialize \co{x}.
	\fi

\QuickQ{}
	여기 저기 모든 곳에서 \co{ACCESS_ONCE()} 를 쓰는 대신에,
	Figure~\ref{fig:toolsoftrade:Measuring Reader-Writer Lock Scalability}
	의 라인~10에서 \co{goflag} 를 \co{volatile} 로 선언하는게 어때요?

	\iffalse
	Instead of using \co{ACCESS_ONCE()} everywhere, why not just
	declare \co{goflag} as \co{volatile} on line~10 of
	Figure~\ref{fig:toolsoftrade:Measuring Reader-Writer Lock Scalability}?
	\fi
\QuickA{}
	이 경우에는 \co{volatile} 로의 선언도 합리적인 대안입니다.
	하지만, \co{ACCESS_ONCE()} 의 사용은 코드를 읽는 사람에게 \co{goflag}
	가 동시적 리드와 업데이트 동작에 연관되어 있음을 분명하게 보여줍니다.
	하지만, \co{ACCESS_ONCE()} 는 특히나 대부분의 접근이 락에 의해 보호되고
	있지만 (따라서 변화에 종속되지 \emph{않지만}) 락 바깥에서의 접근도 약간
	있는 경우에 유용합니다.
	volatile 선언을 이런 경우에 사용하는 것은 코드를 읽는 사람이 락
	바깥에서의 특수한 접근의 경우를 알아채기가 어렵게 만들고 컴파일러가 락
	아래의 코드에 대해 좋은 코드를 만들기 어렵게 할 수 있습니다.

	\iffalse
	A \co{volatile} declaration is in fact a reasonable alternative in
	this particular case.
	However, use of \co{ACCESS_ONCE()} has the benefit of clearly
	flagging to the reader that \co{goflag} is subject to concurrent
	reads and updates.
	However, \co{ACCESS_ONCE()} is especially useful in cases where
	most of the accesses are protected by a lock (and thus \emph{not}
	subject to change), but where a few of the accesses are made outside
	of the lock.
	Using a volatile declaration in this case would make it harder
	for the reader to note the special accesses outside of the lock,
	and would also make it harder for the compiler to generate good
	code under the lock.
	\fi

\QuickQ{}
	\co{ACCESS_ONCE()} 는 컴파일러에만 영향을 주지, CPU 에는 영향을 안주죠.
	Figure~\ref{fig:toolsoftrade:Measuring Reader-Writer Lock Scalability}
	의 \co{goflag} 의 값의 변화가 시간 순서대로 다른 CPU 에게도 전파되게
	하려면 메모리 배리어도 쳐야 하지 않나요?

	\iffalse
	\co{ACCESS_ONCE()} only affects the compiler, not the CPU.
	Don't we also need memory barriers to make sure
	that the change in \co{goflag}'s value propagates to the
	CPU in a timely fashion in
	Figure~\ref{fig:toolsoftrade:Measuring Reader-Writer Lock Scalability}?
	\fi
\QuickA{}
	아니오, 메모리 배리어는 여기선 필요하지도 않고 도움을 주지도 않습니다.
	메모리 배리어들은 그저 여러 메모리 참조들 사이의 순서만을 잡아줍니다:
	그것들은 시스템의 한 부분에서 다른 곳으로 데이터 전파를 촉진시키는 어떤
	일도 하지 않습니다.
	이것이 하나의 경험적 법칙을 일깨웁니다:  여러 쓰레드들 사이에 두개
	이상의 변수를 사용해 통신하고 있지 않다면 메모리 배리어는 필요하지
	않습니다.

	하지만 \co{nreadersrunning} 의 경우는 어떨까요?
	통신에 사용되는 두번째 변수 아닌가요?
	맞습니다, 그리고 \co{__sync_fetch_and_add()} 아래에 해당 쓰레드가
	시작해야 하는지 보기 전에 자신의 존재를 분명히 알리기 위해 필요한
	메모리 배리어가 있습니다.

	\iffalse
	No, memory barriers are not needed and won't help here.
	Memory barriers only enforce ordering among multiple
	memory references:  They do absolutely nothing to expedite
	the propagation of data from one part of the system to
	another.
	This leads to a quick rule of thumb:  You do not need
	memory barriers unless you are using more than one
	variable to communicate between multiple threads.

	But what about \co{nreadersrunning}?
	Isn't that a second variable used for communication?
	Indeed it is, and there really are the needed memory-barrier
	instructions buried in \co{__sync_fetch_and_add()},
	which make sure that the thread proclaims its presence
	before checking to see if it should start.
	\fi

\QuickQ{}
	예를 들어 \co{gcc} \co{__thread} 스토리지 클래스를 사용해 선언된
	쓰레드별 변수에 접근할 때에도 \co{ACCESS_ONCE()} 가 필요할까요?

	\iffalse
	Would it ever be necessary to use \co{ACCESS_ONCE()} when accessing
	a per-thread variable, for example, a variable declared using
	the \co{gcc} \co{__thread} storage class?
	\fi
\QuickA{}
	경우에 따라 다릅니다.
	만약 그 쓰레드별 변수가 그 쓰레드에서만 접근되었다면, 그리고 시그널
	핸들러에서 접근되지 않았다면, 필요하지 않습니다.
	하지만 그렇지 않다면, \co{ACCESS_ONCE()} 가 필요할 수 있습니다.
	각 상황을 모두 Section~\ref{sec:count:Signal-Theft Limit Counter
	Implementation} 에서 보겠습니다.

	이 이야기는 어떻게 한 쓰레드가 다른 쓰레드의 \co{__thread} 변수에
	접근할 수 있는지 질문을 가져오는데, 답은 두번째 쓰레드가 자신의
	\co{__thread} 변수로의 포인터를 첫번째 쓰레드가 접근할 수 있는 곳에
	저장해 둠으로써 가능하다 입니다.
	이런 코드를 작성하는 흔한 경우 중 하나는 쓰레드당 한개씩의 원소를 갖는
	링크드 리스트에 대해 각 쓰레드의 \co{__thread} 변수를 해당하는 원소에
	저장하는 경우입니다.

	\iffalse
	It depends.
	If the per-thread variable was accessed only from its thread,
	and never from a signal handler, then no.
	Otherwise, it is quite possible that \co{ACCESS_ONCE()} is needed.
	We will see examples of both situations in
	Section~\ref{sec:count:Signal-Theft Limit Counter Implementation}.

	This leads to the question of how one thread can gain access to
	another thread's \co{__thread} variable, and the answer is that
	the second thread must store a pointer to its \co{__thread}
	pointer somewhere that the first thread has access to.
	One common approach is to maintain a linked list with one 
	element per thread, and to store the address of each thread's
	\co{__thread} variable in the corresponding element.
	\fi

\QuickQ{}
	단일 CPU 성능에 비교하는건 좀 심한 거 아닌가요?

	\iffalse
	Isn't comparing against single-CPU throughput a bit harsh?
	\fi
\QuickA{}
	전혀요.
	사실, 이 비교는 지나치게 관대한 편입니다.
	더 균형잡힌 비교를 위해선 락을 전혀 사용하지 않는 단일 CPU 성능과
	비교해야 하겠죠.

	\iffalse
	Not at all.
	In fact, this comparison was, if anything, overly lenient.
	A more balanced comparison would be against single-CPU
	throughput with the locking primitives commented out.
	\fi

\QuickQ{}
	하지만 1,000 개의 인스트럭션은 크리티컬 섹션 치고 그렇게 작은 크기는
	아니예요. 수십 개의 인스트럭션 정도만을 가지는 훨씬 작은 크리티컬
	섹션이 필요하면 어떻게 해야하죠?

	\iffalse
	But 1,000 instructions is not a particularly small size for
	a critical section.
	What do I do if I need a much smaller critical section, for
	example, one containing only a few tens of instructions?
	\fi
\QuickA{}
	읽혀지는 데이터가 \emph{절대} 변하지 않는다면, 거기 접근하는데 어떤
	락도 잡을 필요가 없습니다.
	만약 데이터가 충분히 가끔만 변경된다면, 실행된 단계를 기록해두고, 모든
	쓰레드를 종료시키고, 데이터를 변경한 후, 기록된 단계부터 쓰레드들을
	다시 실행시키면 됩니다.

	다른 방법은 쓰레드당 하나씩의 명시적 락을 두고, 자신의 락을
	획득함으로써 커다란 리더-라이터 락의 읽기 락 획득을 하고, 모든 쓰레드의
	락을 획득함으로써 쓰기 락 획득을 하는 것~\cite{WilsonCHsieh92a}과 같은
	효과를 얻는 것입니다.
	이 방법은 리더들을 위해선 상당히 잘 동작합니다만, 라이터들은 쓰레드의
	수가 늘어날수록 큰 오버헤드를 갖게 만들 수 있습니다.

	그 외의 매우 작은 크리티컬 섹션을 처리하기 위한 방법들은
	Section~\ref{sec:defer:Read-Copy Update (RCU)} 에서 다루고 있습니다.

	\iffalse
	If the data being read \emph{never} changes, then you do not
	need to hold any locks while accessing it.
	If the data changes sufficiently infrequently, you might be
	able to checkpoint execution, terminate all threads, change
	the data, then restart at the checkpoint.

	Another approach is to keep a single exclusive lock per
	thread, so that a thread read-acquires the larger aggregate
	reader-writer lock by acquiring its own lock, and write-acquires
	by acquiring all the per-thread locks~\cite{WilsonCHsieh92a}.
	This can work quite well for readers, but causes writers
	to incur increasingly large overheads as the number of threads
	increases.

	Some other ways of handling very small critical sections are
	described in Section~\ref{sec:defer:Read-Copy Update (RCU)}.
	\fi

\QuickQ{}
	Figure~\ref{fig:intro:Reader-Writer Lock Scalability} 에서 100M 에서의
	경우 이외의 값들은 이상적인 선에서 부드럽게 멀어집니다.
	반면, 100M 에서의 값은 64 CPU 에서 갑자기 이상적인 선으로부터
	멀어지는군요.
	또, 100M 값과 10M 값 사이의 거리는 10M 값과 1M 값 사이의 거리보다
	작아요.
	왜 100M 값은 이렇게 남들과 다른거죠?

	\iffalse
	In
	Figure~\ref{fig:intro:Reader-Writer Lock Scalability},
	all of the traces other than the 100M trace deviate gently
	from the ideal line.
	In contrast, the 100M trace breaks sharply from the ideal
	line at 64 CPUs.
	In addition, the spacing between the 100M trace and the 10M
	trace is much smaller than that between the 10M trace and the
	1M trace.
	Why does the 100M trace behave so much differently than the
	other traces?
	\fi
\QuickA{}
	첫번째 단서는 64 CPU 는 정확히 기계의 128 CPU 의 절반이란 겁니다.
	그 차이는 하드웨어 쓰레딩의 영향입니다.
	이 시스템은 코어당 2개 하드웨어 쓰레드를 가지며 총 64 코어를 갖습니다.
	64 쓰레드보다 적은 쓰레드가 돌 때 까지는 각 쓰레드가 자신의 코어를
	하나씩 갖고 동작합니다.
	하지만 쓰레드의 수가 64를 넘는 순간, 일부 쓰레드들은 코어를 공유해야만
	합니다.
	한 코어를 공유하는 쓰레드들은 일부 하드웨어 자원을 공유해야 하기
	때문에, 한개 코어를 공유하는 두 쓰레드의 성능은 각자 코어 하나씩 가지고
	도는 두 쓰레드의 것에 비해 낮을 수밖에 없습니다.
	따라서 100M 경우의 성능은 리더-라이터 락에 의해 제한되는게 아니라 싱글
	코어에서의 하드웨어 쓰레드간의 하드웨어 자원 공유에 의해 제한되는
	것입니다.

	이건 10M 경우에도 볼 수 있는데, 64 쓰레드까지 일관되게 떨어지던 성능
	폭은 이후 급격하게 떨어져서 100M 경우와 비슷하죠.
	64 쓰레드까지는 10M 경우도 리더-라이터 락의 확장성에 의해 성능이
	제한되지만, 그 이후부터는 싱글 코어에서의 하드웨어 쓰레드간의 하드웨어
	자원 공유로 인해 제한되는 것입니다.

	\iffalse
	Your first clue is that 64 CPUs is exactly half of the 128
	CPUs on the machine.
	The difference is an artifact of hardware threading.
	This system has 64 cores with two hardware threads per core.
	As long as fewer than 64 threads are running, each can run
	in its own core.
	But as soon as there are more than 64 threads, some of the threads
	must share cores.
	Because the pair of threads in any given core share some hardware
	resources, the throughput of two threads sharing a core is not
	quite as high as that of two threads each in their own core.
	So the performance of the 100M trace is limited not by the
	reader-writer lock, but rather by the sharing of hardware resources
	between hardware threads in a single core.

	This can also be seen in the 10M trace, which deviates gently from
	the ideal line up to 64 threads, then breaks sharply down, parallel
	to the 100M trace.
	Up to 64 threads, the 10M trace is limited primarily by reader-writer
	lock scalability, and beyond that, also by sharing of hardware
	resources between hardware threads in a single core.
	\fi

\QuickQ{}
	Power-5 는 나온지 몇년이 넘었고, 최신 하드웨어는 분명 더 빠를 거예요.
	그런데 왜 리더-라이터 락의 느린 속도에 걱정해야 하죠?

	\iffalse
	Power-5 is several years old, and new hardware should
	be faster.
	So why should anyone worry about reader-writer locks being slow?
	\fi
\QuickA{}
	일반적으로, 최신 하드웨어에선 개선되어 있습니다.
	하지만, 128 CPU 에서 리더-라이터 락이 이상적인 성능을 달성하기 위해선
	100배 이상의 개선이 필요합니다.
	게다가, CPU 의 갯수가 커질수록, 필요한 성능 향상 정도도 커집니다.
	따라서 리더-라이터 락의 성능 문제는 당분간은 존재할 것입니다.

	\iffalse
	In general, newer hardware is improving.
	However, it will need to improve more than two orders of magnitude
	to permit reader-writer lock to achieve ideal performance on
	128 CPUs.
	Worse yet, the greater the number of CPUs, the larger the
	required performance improvement.
	The performance problems of reader-writer locking are therefore
	very likely to be with us for quite some time to come.
	\fi

\QuickQ{}
	정말로 이것들이 다 필요한 거 맞나요?
	\iffalse
	Is it really necessary to have both sets of primitives?
	\fi
\QuickA{}
	엄격하게 말하면, 아닙니다.
	필요하면 첫번째 분류의 것들을 이용해서 두번째 분류의 것들을 구현할 수가
	있습니다.
	예를 들어, 누군가는 \co{__sync_fetch_and_nand()} 를 이용해서
	아래와 같이 \co{__sync_nand_and_fetch()} 를 구현할 수 있겠죠.

	\iffalse
	Strictly speaking, no.
	One could implement any member of the second set using the
	corresponding member of the first set.
	For example, one could implement \co{__sync_nand_and_fetch()}
	in terms of \co{__sync_fetch_and_nand()} as follows:
	\fi

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\scriptsize
\begin{verbatim}
tmp = v;
ret = __sync_fetch_and_nand(p, tmp);
ret = ~ret & tmp;
\end{verbatim}
\end{minipage}
\vspace{5pt}

	비슷하게 \co{__sync_fetch_and_add()}, \co{__sync_fetch_and_sub()},
	그리고 \co{__sync_fetch_and_xor()} 를 그들의 나중값 리턴하는 대응
	함수들을 이용해 구현하는 것도 가능합니다.

	하지만, 이를 대신해주는 기능이 있는 게 프로그래머에게도
	컴파일러/라이브러리를 구현하는 사람에게도 편리할 것입니다.

	\iffalse
	It is similarly possible to implement \co{__sync_fetch_and_add()},
	\co{__sync_fetch_and_sub()}, and \co{__sync_fetch_and_xor()}
	in terms of their post-value counterparts.

	However, the alternative forms can be quite convenient, both
	for the programmer and for the compiler/library implementor.
	\fi

\QuickQ{}
	이 어토믹 오퍼레이션들은 기계의 인스트럭션 셋에서 바로 지원되는
	한개짜리 어토믹 인스트럭션으로 변환될테니, 이것들이 일을 돌아가게 할 수
	있는 가장 빠른 방법 아닌가요?

	\iffalse
	Given that these atomic operations will often be able to
	generate single atomic instructions that are directly
	supported by the underlying instruction set, shouldn't
	they be the fastest possible way to get things done?
	\fi
\QuickA{}
	안타깝지만, 아닙니다.
	극명한 반례를 위해 Chapter~\ref{chp:Counting} 을 보시기 바랍니다.
	\iffalse
	Unfortunately, no.
	See Chapter~\ref{chp:Counting} for some stark counterexamples.
	\fi

\QuickQ{}
	리눅스 커널의 \co{fork()} 와 \co{wait()} 대체물은 어디갔죠?

	\iffalse
	What happened to the Linux-kernel equivalents to \co{fork()}
	and \co{wait()}?
	\fi
\QuickA{}
	그런건 없습니다.
	리눅스 커널 내에서 실행되는 모든 태스크들은 메모리를 공유합니다. 당신이
	거대한 메모리 매핑을 손으로 일일히 할 생각이 아니라면 말이죠.

	\iffalse
	They don't really exist.
	All tasks executing within the Linux kernel share memory,
	at least unless you want to do a huge amount of memory-mapping
	work by hand.
	\fi

\QuickQ{}
	셸은 기본적으로 \co{fork()} 가 아니라 \co{vfork()} 를 사용하지 않나요?

	\iffalse
	Wouldn't the shell normally use \co{vfork()} rather than
	\co{fork()}?
	\fi
\QuickA{}
	아마 그럴겁니다만, 확인해보는건 독자의 몫입니다.
	하지만 그렇다 해도, 전 우리가 \co{vfork()} 는 \co{fork()} 의 변종일
	뿐이고, 따라서 \co{fork()} 를 둘 다를 이야기하는 일반적 용어로 사용해도
	된다는데 합의했으면 합니다.

	\iffalse
	It might well do that, however, checking is left as an exercise
	for the reader.
	But in the meantime, I hope that we can agree that \co{vfork()}
	is a variant of \co{fork()}, so that we can use \co{fork()}
	as a generic term covering both.
	\fi

\QuickQAC{chp:Counting}{Counting}
\QuickQ{}
	대체 왜 효과적이고 확장성 있는 카운팅이 어려운가요?
	무엇보다, 컴퓨터들은 카운팅, 더하기, 빼기, 그 외에도 여러가지를 위한
	전용 하드웨어도 가지고 있는데, 그걸 못하나요???

	\iffalse
	Why on earth should efficient and scalable counting be hard?
	After all, computers have special hardware for the sole purpose
	of doing counting,
	addition, subtraction, and lots more besides, don't they???
	\fi
\QuickA{}
	공유된 카운터에 대한 어토믹 오퍼레이션과 같은 기본적인 카운팅
	알고리즘들은 Section~\ref{sec:count:Why Isn't Concurrent Counting
	Trivial?} 에서 이야기하듯 느리고 확장성이 나쁘거나 정확도가 떨어지기
	때문입니다.

	\iffalse
	Because the straightforward counting algorithms, for example,
	atomic operations on a shared counter, either are slow and scale
	badly, or are inaccurate, as will be seen in
	Section~\ref{sec:count:Why Isn't Concurrent Counting Trivial?}.
	\fi

\QuickQ{}
	{ \bfseries 네트워크 패킷 카운팅 문제. }
	당신이 송수신된 네트워크 패킷의 갯수 (또는 전체 용량) 에 대한 통계를
	구해야 한다고 생각해 봅시다.
	패킷들은 시스템의 어떤 CPU 를 통해서든 송신 / 수신될 수 있을 겁니다.
	나아가서 이 커다란 기계가 초당 백만개의 패킷을 다룰 수 있고, 그 갯수를
	매 5초마다 읽어내야 하는 시스템 모니터링 패키지가 있다고 가정해 봅시다.
	당신이라면 이 통계 카운터를 어떻게 구현하시겠어요?

	\iffalse
	{ \bfseries Network-packet counting problem. }
	Suppose that you need to collect statistics on the number
	of networking packets (or total number of bytes) transmitted
	and/or received.
	Packets might be transmitted or received by any CPU on
	the system.
	Suppose further that this large machine is capable of
	handling a million packets per second, and that there
	is a systems-monitoring package that reads out the count
	every five seconds.
	How would you implement this statistical counter?
	\fi
\QuickA{}
	힌트: 카운터의 업데이트는 엄청 빨라야 합니다만, 카운터는 500만번의
	업데이트마다 한번만 일어나기 때문에, 카운터를 읽어내는 행동은 꽤 느려도
	될 겁니다.
	또한, 일반적으로 읽어지는 값은 완전히 정교하진 않아도
	될겁니다---무엇보다, 카운터는 1밀리세컨드당 1000번 업데이트되기 때문에,
	우린 ``진짜 값''에서 수천정도는 오차값을 가질 수밖에 없을 겁니다.
	``진짜 값'' 이란게 이 문맥에서 뭘 의미하던지요.
	하지만, 읽혀지는 값은 어느정도는 절대적인 오차를 유지해야 할겁니다.
	예를 들어, 카운트가 수백만 정도일 때 1\% 오차는 문제없지만, 조단위가
	된다면 문제가 있겠죠.
	Section~\ref{sec:count:Statistical Counters} 를 참고하세요.

	\iffalse
	Hint: The act of updating the counter must be blazingly
	fast, but because the counter is read out only about once
	in five million updates, the act of reading out the counter can be
	quite slow.
	In addition, the value read out normally need not be all that
	accurate---after all, since the counter is updated a thousand
	times per millisecond, we should be able to work with a value
	that is within a few thousand counts of the ``true value'',
	whatever ``true value'' might mean in this context.
	However, the value read out should maintain roughly the same
	absolute error over time.
	For example, a 1\% error might be just fine when the count
	is on the order of a million or so, but might be absolutely
	unacceptable once the count reaches a trillion.
	See Section~\ref{sec:count:Statistical Counters}.
	\fi

\QuickQ{}
	{ \bfseries 대략적 구조체 할당 한계 문제. }
	할당된 구조체의 갯수가 어떤 한계(한 10,000 정도)를 넘어가면 추가 할당을
	막기 위해 할당된 구조체의 갯수를 유지해야 한다고 생각해 봅시다.
	또, 이 구조체들은 할당되고 나서 곧바로 해제되고, 한계치를 넘기는 일은
	매우 드물고, ``대략적인'' 한계치 설정이 가능하다고 생각해 봅시다.

	\iffalse
	{ \bfseries Approximate structure-allocation limit problem. }
	Suppose that you need to maintain a count of the number of
	structures allocated in order to fail any allocations
	once the number of structures in use exceeds a limit
	(say, 10,000).
	Suppose further that these structures are short-lived,
	that the limit is rarely exceeded, and that a ``sloppy''
	approximate limit is acceptable.
	\fi
\QuickA{}
	힌트: 카운터의 업데이트 동작은 여기서도 매우 빨라야 합니다만, 카운터는
	카운터가 증가될 때마다 읽혀야 합니다.
	하지만, 읽혀지는 값은 그 값이 한계치 아래인지 위인지를 대략적으로는
	구분해 내야 한다는 점을 \emph{제외하고는} 정교하지 않아도 됩니다.

	\iffalse
	Hint: The act of updating the counter must again be blazingly
	fast, but the counter is read out each time that the
	counter is increased.
	However, the value read out need not be accurate
	\emph{except} that it must distinguish approximately
	between values below the limit and values greater than or
	equal to the limit.
	See Section~\ref{sec:count:Approximate Limit Counters}.
	\fi

\QuickQ{}
	{ \bfseries 정교한 구조체 할당 한계 문제. }
	할당된 구조체의 갯수가 어떤 정확한 한계(여기서도, 한 10,000 정도)를
	넘어가면 추가 할당을 막기 위해 할당된 구조체의 갯수를 유지해야 한다고
	생각해 봅시다.
	이 구조체들은 할당되고 얼마 안되 해제되고, 그 한계는 드물게 초과되며,
	거의 항상 최소 한개의 구조체는 사용중이 됩니다. 또한 예를 들어, 하나의
	구조체도 사용되지 않고 있다면 해제 할 수 있는 어떤 메모리를 위해
	카운터가 0이 되는 시점을 정확히 알 필요가 있습니다.
	\iffalse

	{ \bfseries Exact structure-allocation limit problem. }
	Suppose that you need to maintain a count of the number of
	structures allocated in order to fail any allocations
	once the number of structures in use exceeds an exact limit
	(again, say 10,000).
	Suppose further that these structures are short-lived,
	and that the limit is rarely exceeded, that there is almost
	always at least one structure in use, and suppose further
	still that it is necessary to know exactly when this counter reaches
	zero, for example, in order to free up some memory
	that is not required unless there is at least one structure
	in use.
	\fi
\QuickA{}
	힌트: 카운터의 업데이트 동작은 역시 엄청 빨라야 합니다만, 카운터의 값이
	증가될 때마다 그 값 역시 읽혀야 합니다.
	하지만, 그 값은 한계치 경계를 넘어서는지와 0인지를 분명하게 체크해야
	한다는 점을 \emph{제외하고는} 정교할 필요가 없습니다.
	Section~\ref{sec:count:Exact Limit counters} 를 참고하세요.
	\iffalse

	Hint: The act of updating the counter must once again be blazingly
	fast, but the counter is read out each time that the
	counter is increased.
	However, the value read out need not be accurate
	\emph{except} that it absolutely must distinguish perfectly
	between values between the limit and zero on the one hand,
	and values that either are less than or equal to zero or
	are greater than or equal to the limit on the other hand.
	See Section~\ref{sec:count:Exact Limit Counters}.
	\fi

\QuickQ{}
	{ \bfseries 제거될 수 있는 I/O 디바이스 접속 카운트 문제. }
	매우 빈번하게 사용되는 제거 가능한 대용량 디바이스에 대해 사용자에게
	해당 디바이스를 제거해도 안전한지 알려주기 위해 그 참조 횟수를 관리해야
	한다고 가정해 봅시다.
	이 디바이스는 사용자가 디바이스를 제거하고 싶을 때 그 의사를 알려주며,
	시스템은 사용자에게 언제 디바이스를 제거해도 안전한지 알려주는 일반적
	디바이스 제거 절차를 따릅니다.
	\iffalse

	{ \bfseries Removable I/O device access-count problem. }
	Suppose that you need to maintain a reference count on a
	heavily used removable mass-storage device, so that you
	can tell the user when it is safe to remove the device.
	This device follows the usual removal procedure where
	the user indicates a desire to remove the device, and
	the system tells the user when it is safe to do so.
	\fi
\QuickA{}
	힌트: 여기서도 카운터의 업데이트 동작은 I/O 오퍼레이션이 느려지지 않게
	매우 빠르고 확장성 있어야 합니다. 하지만 카운터는 사용자가 디바이스를
	제거하고자 할 때에만 읽혀지기 때문에, 카운터의 읽기 동작은 매우 느려도
	됩니다.
	또한, 사용자가 디바이스를 제거하고자 하는 의사를 밝히지 않았다면 그
	카운터는 읽혀질 필요가 아예 없습니다.
	또한, 그 값은 디바이스가 제거를 위한 절차 중일 때로 한정해서 0인지 0이
	아닌지만 분명히 구분할 수 있어야 한다는 점을 \emph{제외하고는} 정교할
	필요가 없습니다.
	하지만, 한번 0이라는 값을 읽었다면, 차후에 다른 쓰레드가 제거중인 해당
	디바이스에 접근을 하거나 하는 일을 막기 위해 해당 값을 0으로 유지해야
	합니다.
	Section~\ref{sec:count:Applying Specialized Parallel Counters} 를
	참고하세요.
	\iffalse

	Hint: Yet again, the act of updating the counter must be blazingly
	fast and scalable in order to avoid slowing down I/O operations,
	but because the counter is read out only when the
	user wishes to remove the device, the counter read-out
	operation can be extremely slow.
	Furthermore, there is no need to be able to read out
	the counter at all unless the user has already indicated
	a desire to remove the device.
	In addition, the value read out need not be accurate
	\emph{except} that it absolutely must distinguish perfectly
	between non-zero and zero values, and even then only when
	the device is in the process of being removed.
	However, once it has read out a zero value, it must act
	to keep the value at zero until it has taken some action
	to prevent subsequent threads from gaining access to the
	device being removed.
	See Section~\ref{sec:count:Applying Specialized Parallel Counters}.
	\fi

\QuickQ{}
	하지만 \co{++} 연산자는 x86 의 add-to-memory 명령어를 만들지 않나요?
	그리고 CPU 캐시는 그걸 어토믹하게 수행하지 않나요?
	\iffalse

	But doesn't the \co{++} operator produce an x86 add-to-memory
	instruction?
	And won't the CPU cache cause this to be atomic?
	\fi
\QuickA{}
	\co{++} 연산자는 어토믹할 수\emph{도 있지만}, 그래야만 한다는 규칙은
	없습니다.
	그리고 실제로, \co{gcc} 는 값을 레지스터에 읽어오고, 레지스터의 값을
	증가시킨 후, 메모리에 그 값을 저장하는, 어토믹하지 않은 방법을 종종
	택합니다.
	\iffalse

	Although the \co{++} operator \emph{could} be atomic, there
	is no requirement that it be so.
	And indeed, \co{gcc} often
	chooses to load the value to a register, increment
	the register, then store the value to memory, which is
	decidedly non-atomic.
	\fi

\QuickQ{}
	실패 횟수의 8-figure 정확도는 당신이 진짜로 이 테스트를 한 것을
	보여주는군요.
	왜 이런 사소한 프로그램을, 특히나 버그가 이렇게 쉽게 직관적으로
	보이는데도 굳이 테스트 해야 하나요?
	\iffalse

	The 8-figure accuracy on the number of failures indicates
	that you really did test this.
	Why would it be necessary to test such a trivial program,
	especially when the bug is easily seen by inspection?
	\fi
\QuickA{}
	사소한 병렬 프로그램이 아주 조금만 존재하지는 않고, 순차적 프로그램에도
	마찬가지라고 저는 생각합니다.

	프로그램이 얼마나 작거나 간단한지와는 상관 없이, 테스트 해보지
	않았다면, 그건 동작하지 않는 것입니다.
	그리고 설령 테스트 해봤다 해도, 머피의 법칙에 의하면 여전히 숨어있는
	버그가 몇개는 있을 수 있을 수 있습니다.
	\iffalse

	Not only are there very few
	trivial parallel programs, and most days I am
	not so sure that there are many trivial sequential programs, either.

	No matter how small or simple the program, if you haven't tested
	it, it does not work.
	And even if you have tested it, Murphy's Law says that there will
	be at least a few bugs still lurking.
	\fi

	또한, 정확성의 증명은 분명 그 의미를 갖지만, 여기 사용된
	\url{counttorture.h} 테스트를 포홤해 테스트를 대체하는 일은 결코 없을
	겁니다.
	무엇보다, 증명은 그것이 바닥에 깔고 있는 가정에 국한됩니다.
	게다가, 증명은 프로그램이 그렇듯 버그를 가지고 있기 쉽습니다!
	\iffalse

	Furthermore, while proofs of correctness certainly do have their
	place, they never will replace testing, including the
	\url{counttorture.h} test setup used here.
	After all, proofs are only as good as the assumptions that they
	are based on.
	Furthermore, proofs can have bugs just as easily as programs can!
	\fi

\QuickQ{}
	왜 x~축의 점선은 $x=1$ 에서 대각선의 선과 만나지 않죠?
	\iffalse

	Why doesn't the dashed line on the x~axis meet the 
	diagonal line at $x=1$?
	\fi
\QuickA{}
	어토믹 오퍼레이션의 오버헤드 때문입니다.
	x~축의 점선은 싱글 \emph{어토믹하지 않은} 증가 연산의 오버헤드를
	나타냅니다.
	\emph{이상적인} 알고리즘은 선형적으로 확장될 뿐만 아니라,
	싱글 쓰레드 코드에 비해서도 성능 하락이 없어야 할 것입니다.

	이런 수준의 이상론은 좀 지나쳐 보일 수 있습니다, 다만 리누스 토발즈에게
	충분하다면, 당신에게도 충분할 겁니다.
	\iffalse

	Because of the overhead of the atomic operation.
	The dashed line on the x~axis represents the overhead of
	a single \emph{non-atomic} increment.
	After all, an \emph{ideal} algorithm would not only scale
	linearly, it would also incur no performance penalty compared
	to single-threaded code.

	This level of idealism may seem severe, but if it is good
	enough for Linus Torvalds, it is good enough for you.
	\fi

\QuickQ{}
	하지만 어토믹 증가 연산은 여전히 꽤 빠릅니다.
	그리고 빡빡한 루프에서 하나의 변수를 증가시키는 건 제겐 꽤 비현실적인
	것 같아 보이구요, 무엇보다, 프로그램의 실행은 실제로 일을 하는데
	쓰여야지, 자기가 한 일을 세는데 쓰여야 하는게 아니라구요!
	왜 제가 이걸 빠르게 하는걸 고민해야 하나요?
	\iffalse

	But atomic increment is still pretty fast.
	And incrementing a single variable in a tight loop sounds
	pretty unrealistic to me, after all, most of the program's
	execution should be devoted to actually doing work, not accounting
	for the work it has done!
	Why should I care about making this go faster?
	\fi
\QuickA{}
	많은 경우에 어토믹 증가 연산은 분명히 당신에겐 충분히 빠를 겁니다.
	그런 경우에 당신은 당연히 어토믹 증가 연산을 사용해야죠.
	그렇지만, 더 나은 카운팅 알고리즘이 필요한 실제 상황도 상당히 많이
	존재합니다.
	그런 상황의 예는 고도로 최적화된 네트워킹 스택에서의 패킷과 용량
	카운팅으로, 이런 예에서는, 특히나 커다란 멀티프로세서에서는 대부분의
	실행시간을 이런류의 카운팅 작업에 보내게 됩니다.

	게다가, 이 챕터의 시작에서 이야기했듯, 카운팅은 공유 메모리 병렬
	프로그램에서 마주칠 수 있는 문제들을 보여줍니다.
	\iffalse

	In many cases, atomic increment will in fact be fast enough
	for you.
	In those cases, you should by all means use atomic increment.
	That said, there are many real-world situations where
	more elaborate counting algorithms are required.
	The canonical example of such a situation is counting packets
	and bytes in highly optimized networking stacks, where it is
	all too easy to find much of the execution time going into
	these sorts of accounting tasks, especially on large
	multiprocessors.

	In addition, as noted at the beginning of this chapter,
	counting provides an excellent view of the
	issues encountered in shared-memory parallel programs.
	\fi

\QuickQ{}
	그런데 왜 CPU 설계자들은 단순히 데이터에의 증가 연산을 추가해서
	담고 있는 캐시 라인의 순회가 증가하는걸 막지 않는거죠?
	\iffalse

	But why can't CPU designers simply ship the addition operation to the
	data, avoiding the need to circulate the cache line containing
	the global variable being incremented?
	\fi
\QuickA{}
	어떤 경우에는 그런 방법도 가능하겠죠.
	하지만, 그러려면 좀 복잡합니다:
	\iffalse

	It might well be possible to do this in some cases.
	However, there are a few complications:
	\fi
	\begin{enumerate}
	\item	만약 그 변수의 값이 필요하다면, 그 쓰레드는 해당 오퍼레이션이
		그 데이터로 향할 때까지 기다리고, 그러고나선 돌아오기까지
		기다려야 합니다.
	\item	그 어토믹 증가 오퍼레이션이 이전의, 그리고 이후의
		오퍼레이션들과 순서를 맞춰야 한다면, 해당 쓰레드는 오퍼레이션이
		데이터까지 가고, 돌아올 준비가 완료될 때까지 기다려야 합니다.
	\item	오퍼레이션들을 CPU들 사이에서 보내게 되면 시스템 접속부를
		지나야 하고, 이는 더 많은 다이 공간과 전력을 소모하게 될겁니다.
	\iffalse

	\item	If the value of the variable is required, then the
		thread will be forced to wait for the operation
		to be shipped to the data, and then for the result
		to be shipped back.
	\item	If the atomic increment must be ordered with respect
		to prior and/or subsequent operations, then the thread
		will be forced to wait for the operation to be shipped
		to the data, and for an indication that the operation
		completed to be shipped back.
	\item	Shipping operations among CPUs will likely require
		more lines in the system interconnect, which will consume
		more die area and more electrical power.
	\fi
	\end{enumerate}
	하지만 앞의 두가지 조건이 없다면?
	그럼 당신은 Section~\ref{sec:count:Statistical Counters} 에서 논의되는,
	실제 상품화된 하드웨어에서 이상적 상황에 근접하는 성능을 보이는
	알고리즘을 잘 고려해 봐야 할 겁니다.
	\iffalse

	But what if neither of the first two conditions holds?
	Then you should think carefully about the algorithms discussed
	in Section~\ref{sec:count:Statistical Counters}, which achieve
	near-ideal performance on commodity hardware.
	\fi

\begin{figure}[tb]
\begin{center}
\resizebox{3in}{!}{\includegraphics{count/GlobalTreeInc}}
\end{center}
\caption{Data Flow For Global Combining-Tree Atomic Increment}
\label{fig:count:Data Flow For Global Combining-Tree Atomic Increment}
\end{figure}

	앞의 두개 조건이 하나라도 걸려 있다면, 개선된 하드웨어에 \emph{약간}의
	희망이 있습니다.
	컴바이닝 트리(combining tree) 를 하드웨어에서 구현해서, 여러 CPU 에서의
	증가 요청이 하드웨어에 의해 결합되어 하나의 더하기 연산으로 변환되는
	방법을 생각해 볼 수 있을 것입니다.
	해당 하드웨어는 또한 요청들에 순서를 잡아줄 수도 있으므로, 각 CPU 에게
	각자의 어토믹 증가에 의한 값의 반환도 가능할 겁니다.
	Figure~\ref{fig:count:Data Flow For Global Combining-Tree Atomic Increment}
	에서 보여지듯, 이는 인스트럭션의 대기시간을 $O(log N)$ 로 만들어
	줍니다. 여기서 $N$ 은 CPU 의 갯수입니다.
	그리고 이런 하드웨어 최적화를 포함하는 CPU 는 2011년부터 나오기
	시작했습니다.

	Figure~\ref{fig:count:Data Flow For Global Atomic Increment} 에 보여진
	현재 하드웨어의  $O(N)$ 성능에 비하면 엄청난 향상이고, 하드웨어
	대기시간은 3차원 제조 공정이 현실화되면 더욱 낮아질 수 있을 것입니다.
	물론, 일부 중요한 특수 케이스에서는 소프트웨어가 \emph{훨씬} 나은 일을
	할 수 있을 것입니다.
	\iffalse

	If either or both of the first two conditions hold, there is
	\emph{some} hope for improved hardware.
	One could imagine the hardware implementing a combining tree,
	so that the increment requests from multiple CPUs are combined
	by the hardware into a single addition when the combined request
	reaches the hardware.
	The hardware could also apply an order to the requests, thus
	returning to each CPU the return value corresponding to its
	particular atomic increment.
	This results in instruction latency that varies as $O(log N)$,
	where $N$ is the number of CPUs, as shown in
	Figure~\ref{fig:count:Data Flow For Global Combining-Tree Atomic Increment}.
	And CPUs with this sort of hardware optimization are starting to
	appear as of 2011.

	This is a great improvement over the $O(N)$ performance
	of current hardware shown in
	Figure~\ref{fig:count:Data Flow For Global Atomic Increment},
	and it is possible that hardware latencies might decrease
	further if innovations such as three-dimensional fabrication prove
	practical.
	Nevertheless, we will see that in some important special cases,
	software can do \emph{much} better.
	\fi

\QuickQ{}
	하지만 C 의 ``정수들'' 은 크기와 관련한 복잡한 문제들이 있지 않나요?
	\iffalse

	But doesn't the fact that C's ``integers'' are limited in size
	complicate things?
	\fi
\QuickA{}
	아닙니다, 모듈로 더하기도 역시 상호성과 결합성을 가지니까요.
	적어도 부호 없는 정수형을 사용한다면 말입니다.
	오버플로우가 났을 때 넘쳐난 값을 감추는 것 외에 별다른 일을 하는
	기계들은 요즘 거의 없지만, C 표준에 의하면 부호를 갖는 정수형의
	오버플로우는 예상외 동작을 유발할 수 있으므로, 요즘 기계들은 대부분
	안전하다는 사실은 신경쓰지 마십시오.
	불행히도, 컴파일러들은 종종 부호 있는 정수형들은 오버플로우 나지 않을
	것이라는 가정 하에 최적화를 하기 때문에, 당신의 코드가 부호 있는
	정수형을 오버플로우 나게 한다면, 2의 보수를 사용하는 하드웨어를
	사용하고 있다 해도 문제에 직면할 수 있습니다.

	그렇다곤 해도, 32-비트 쓰레드별 카운터에서 (대략) 64-비트 합을
	모으는데에도 추가적인 복잡한 일이 숨어있습니다.
	이걸 처리하는 것은 독자 여러분들에게 연습문제로 남겨두겠습니다만, 이
	챕터의 뒤에 소개될 기법들이 큰 도움이 될 수도 있습니다.
	\iffalse

	No, because modulo addition is still commutative and associative.
	At least as long as you use unsigned integers.
	Recall that in the C standard, overflow of signed integers results
	in undefined behavior, never mind the fact that machines that
	do anything other than wrap on overflow are quite rare these days.
	Unfortunately, compilers frequently carry out optimizations that
	assume that signed integers will not overflow, so if your code
	allows signed integers to overflow, you can run into trouble
	even on twos-complement hardware.

	That said, one potential source of additional complexity arises
	when attempting to gather (say) a 64-bit sum from 32-bit
	per-thread counters.
	Dealing with this added complexity is left as
	an exercise for the reader, for whom some of the techniques
	introduced later in this chapter could be quite helpful.
	\fi

\QuickQ{}
	배열이요???
	하지만 그럼 쓰레드의 갯수가 제한되지 않나요?
	\iffalse

	An array???
	But doesn't that limit the number of threads?
	\fi
\QuickA{}
	그럴 수 있고, 이 간단한 구현에서는 그렇습니다.
	하지만 임의의 갯수의 쓰레드를 지원하는 구현을 만드는 건 그렇게 어렵지
	않은데요, 예를 들면,
	Section~\ref{sec:count:Per-Thread-Variable-Based Implementation}
	\co{gcc} \co{__thread} 를 사용하는 거죠.
	\iffalse

	It can, and in this toy implementation, it does.
	But it is not that hard to come up with an alternative
	implementation that permits an arbitrary number of threads,
	for example, using the \co{gcc} \co{__thread} facility,
	as shown in
	Section~\ref{sec:count:Per-Thread-Variable-Based Implementation}.
	\fi

\QuickQ{}
	근데, 그 외에 gcc 가 어떤 짓을 할 수 있죠???
	\iffalse

	What other choice does gcc have, anyway???
	\fi
\QuickA{}
	C 표준대로라면, 다른 쓰레드에 의해 동시에 수정되고 있을 수 있는 변수를
	읽어들이는 행동의 효과에 대해선 정의되어 있지 않습니다.
	C 는 어토믹하게 \co{long} 타입 변수를 읽어들일 수가 없는 (예를
	들면)8-비트 아키텍쳐를 지원해야만 했기에 C 표준은 다른 선택의 여지가
	없었습니다.
	다음 버전의 C 표준에서는 이 현실과의 격차를 해결해 보려 합니다만, 그
	전까지는 gcc 개발자들의 친절함에 의존해야 합니다.

	대신, 하드웨어가 한번의 메모리 참조 명령으로 필요한 값을 읽을 수 있는
	경우라면, \co{ACCESS_ONCE()}~\cite{JonCorbet2012ACCESS:ONCE} 같은
	volatile 접근법을 사용해서 컴파일러에 제약을 가할 수 있습니다.
	\iffalse

	According to the C standard, the effects of fetching a variable
	that might be concurrently modified by some other thread are
	undefined.
	It turns out that the C standard really has no other choice,
	given that C must support (for example) eight-bit architectures
	which are incapable of atomically loading a \co{long}.
	An upcoming version of the C standard aims to fill this gap,
	but until then, we depend on the kindness of the gcc developers.

	Alternatively, use of volatile accesses such as those provided
	by \co{ACCESS_ONCE()}~\cite{JonCorbet2012ACCESS:ONCE}
	can help constrain the compiler, at least
	in cases where the hardware is capable of accessing the value
	with a single memory-reference instruction.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters} 의
	쓰레드별 \co{counter} 변수는 어떻게 초기화 되나요?
	\iffalse

	How does the per-thread \co{counter} variable in
	Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters}
	get initialized?
	\fi
\QuickA{}
	C 표준은 전역 변수의 초기값은 명시적으로 초기화되지 않는 이상 0이라고
	명시합니다.
	그리고, 사용자가 통계적 카운터들의 연속되는 값 사이의 차이에
	관심있는게 아니라면, 초기값은 의미가 없습니다.
	\iffalse

	The C standard specifies that the initial value of
	global variables is zero, unless they are explicitly initialized.
	So the initial value of all the instances of \co{counter}
	will be zero.
	Furthermore, in the common case where the user is interested only
	in differences between consecutive reads
	from statistical counters, the initial value is irrelevant.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters}
	의 코드가 어떻게 복수의 카운터를 가능하게 할 수 있죠?
	\iffalse

	How is the code in
	Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters}
	supposed to permit more than one counter?
	\fi
\QuickA{}
	실제로, 이 예제는 한개 이상의 카운터는 지원하지 않습니다.
	이 예제를 수정해서 복수의 카운터를 제공하게 하는건 독자 여러분에게
	과제로 남겨 두겠습니다.
	\iffalse

	Indeed, this toy example does not support more than one counter.
	Modifying it so that it can provide multiple counters is left
	as an exercise to the reader.
	\fi

\QuickQ{}
	읽기 오퍼레이션은 쓰레드별 값을 모두 더하는 시간을 가져야 할 것이고,
	그동안도 카운터는 값이 변할 수 있어요.
	그럼 Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters}
	의  \co{read_count()} 는 정확하지 않다는 의미입니다.
	이 카운터는 단위시간당 $r$ 만큼 카운터 값을 증가하고 \co{read_count()}
	는 $\Delta$ 단위시간을 소모한다고 해봅시다.
	리턴되는 값의 예상 오류값은 얼마입니까?
	\iffalse

	The read operation takes time to sum up the per-thread values,
	and during that time, the counter could well be changing.
	This means that the value returned by
	\co{read_count()} in
	Figure~\ref{fig:count:Array-Based Per-Thread Statistical Counters}
	will not necessarily be exact.
	Assume that the counter is being incremented at rate
	$r$ counts per unit time, and that \co{read_count()}'s
	execution consumes $\Delta$ units of time.
	What is the expected error in the return value?
	\fi
\QuickA{}
	최악의 경우에 대한 분석부터 해보고, 좀 나은 경우들을 보죠.

	최악의 경우는, 읽기 오퍼레이션이 실제 동작은 금방 끝냈지만 리턴하기
	전에 $\Delta$ 단위시간동안 대기를 하는 경우로, 이 경우의 오류값은
	간단히 $r \Delta$ 입니다.
	\iffalse

	Let's do worst-case analysis first, followed by a less
	conservative analysis.

	In the worst case, the read operation completes immediately,
	but is then delayed for $\Delta$ time units before returning,
	in which case the worst-case error is simply $r \Delta$.
	\fi

	이 최악의 경우 동작은 별로 현실성이 없으니 각 $N$ 카운터들에 대한 각
	읽기 동작이 시간 간격 $\Delta$ 안에 동일한 간격으로 일어나는 경우를
	생각해보죠.
	$N$ 회의 읽기 사이에 $\frac{\Delta}{N+1}$ 길이의 $N+1$ 개 간격이 존재할
	겁니다.
	마지막 쓰레드의 카운터에서의 읽기 이후로의 지연으로 발생하는 에러치는
	$\frac{r \Delta}{N \left( N + 1 \right)}$ 이므로, 두번째에서 마지막
	쓰레드 사이의 카운터는
	$\frac{2 r \Delta}{N \left( N + 1 \right)}$,
	세번째에서 마지막 사이는
	$\frac{3 r \Delta}{N \left( N + 1 \right)}$,
	그리고 그렇게 계속 진행됩니다.
	전체 오류값은 각 쓰레드의 카운터에서의 읽기로 발생한 에러의 총 합으로
	주어지는데, 다음과 같습니다:

	\begin{equation}
		\frac{r \Delta}{N \left( N + 1 \right)}
			\sum_{i = 1}^N i
	\end{equation}
	\iffalse
	This worst-case behavior is rather unlikely, so let us instead
	consider the case where the reads from each of the $N$
	counters is spaced equally over the time period $\Delta$.
	There will be $N+1$ intervals of duration $\frac{\Delta}{N+1}$
	between the $N$ reads.
	The error due to the delay after the read from the last thread's
	counter will be given by $\frac{r \Delta}{N \left( N + 1 \right)}$,
	the second-to-last thread's counter by
	$\frac{2 r \Delta}{N \left( N + 1 \right)}$,
	the third-to-last by
	$\frac{3 r \Delta}{N \left( N + 1 \right)}$,
	and so on.
	The total error is given by the sum of the errors due to the
	reads from each thread's counter, which is:

	\begin{equation}
		\frac{r \Delta}{N \left( N + 1 \right)}
			\sum_{i = 1}^N i
	\end{equation}
	\fi

	합 부분은 다음과 같이 표현 가능하구요:

	\begin{equation}
		\frac{r \Delta}{N \left( N + 1 \right)}
			\frac{N \left( N + 1 \right)}{2}
	\end{equation}

	불필요한 부분을 제거하면 다음과 같이 직관적인 결과가 나옵니다:

	\begin{equation}
		\frac{r \Delta}{2}
	\label{eq:count:CounterErrorAverage}
	\end{equation}
	\iffalse

	Expressing the summation in closed form yields:

	\begin{equation}
		\frac{r \Delta}{N \left( N + 1 \right)}
			\frac{N \left( N + 1 \right)}{2}
	\end{equation}

	Cancelling yields the intuitively expected result:

	\begin{equation}
		\frac{r \Delta}{2}
	\label{eq:count:CounterErrorAverage}
	\end{equation}
	\fi

	읽기 오퍼레이션 호출자가 해당 오퍼레이션으로 리턴받은 수를 가지고 어떤
	일을 하는 코드를 수행하는 중에도 오류는 쌓여감을 기억해둘 필요가
	있습니다.
	예를 들어, 읽기 오퍼레이션 호출자가 리턴받은 값을 가지고 어떤 계산을
	하는데 $t$ 시간을 사용한다면, 최악의 경우 오류치는 $r \left(\Delta +
	t\right)$ 로 늘어날 겁니다.

	예상 오류치도 비슷하게 늘어나겠죠:

	\begin{equation}
		r \left( \frac{\Delta}{2} + t \right)
	\end{equation}
	\iffalse

	It is important to remember that error continues accumulating
	as the caller executes code making use of the count returned
	by the read operation.
	For example, if the caller spends time $t$ executing some
	computation based on the result of the returned count, the
	worst-case error will have increased to $r \left(\Delta + t\right)$.

	The expected error will have similarly increased to:

	\begin{equation}
		r \left( \frac{\Delta}{2} + t \right)
	\end{equation}
	\fi

	물론, 읽기 중에 카운터가 계속 증가하는건 용납될 수 없는 경우도
	있겠습니다.
	Section~\ref{sec:count:Applying Specialized Parallel Counters}
	에서는 이런 상황을 해결하는 방법을 알아봅니다.

	이렇게, 우리는 감소는 없고 증가만 이루어지는 카운터를 알아보았습니다.
	만약 카운터 값이 단위시간당 $r$ 만큼 감소로든 증가로든 바뀐다면,
	오류값은 줄어들 거라고 예상할 수 있을 겁니다.
	하지만, 카운터가 어느 쪽으로든 움직일 수 \emph{있을지 몰라도} 최악의
	경우는 해당 읽기 오퍼레이션이 금방 끝났지만 $\Delta$ 단위 시간동안
	기다려야 하는데, 그 시간 동안 카운터의 값은 같은 방향으로만 이동하는
	경우이므로 여전히 $r \Delta$ 오류값을 내므로 차이가 없습니다.
	\iffalse

	Of course, it is sometimes unacceptable for the counter to
	continue incrementing during the read operation.
	Section~\ref{sec:count:Applying Specialized Parallel Counters}
	discusses a way to handle this situation.

	Thus far, we have been considering a counter that is only
	increased, never decreased.
	If the counter value is being changed by $r$ counts per unit
	time, but in either direction, we should expect the error
	to reduce.
	However, the worst case is unchanged because although the
	counter \emph{could} move in either direction, the worst
	case is when the read operation completes immediately,
	but then is delayed for $\Delta$ time units, during which
	time all the changes in the counter's value move it in
	the same direction, again giving us an absolute error
	of $r \Delta$.
	\fi

	평균 에러치를 계산하는데는 값의 증감 패턴에 대한 다양한 가정을 바탕으로
	하는 여러가지 방법이 있습니다.
	일단은 간단하게, 1 의 오퍼레이션 중 $f$ 만큼이 감소 오퍼레이션이고,
	관심있는 오류값은 카운터의 장시간 추세선에서의 굴곡이라고 가정해
	봅시다.
	이 가정 하에서는 $f$ 가 0.5 이하라면, 각 감소는 증가에 의해 무효화
	되고, 따라서 $2f$ 오퍼레이션은 서로를 무효화 시키고, $1-2f$ 의
	오퍼레이션들은 무효화 되지 않은 증가가 됩니다.
	반면, $f$ 가 0.5 보다 크다면, $1-f$ 의 감소는 증가에 의해 무효화 되고,
	카운터는 음의 방향으로 $-1+2\left(1-f\right)$ 만큼 이동하는데, 이는
	$1-2f$ 로 정리되고, 따라서 카운터는 평균적으로 오퍼레이션당 $1-2f$
	만큼씩 어느 경우든 이동하게 됩니다.
	따라서, 긴 시점에서의 카운터의 변화는 $\left( 1-2f \right) r$ 로
	주어집니다.
	이걸 Equation~\ref{eq:count:CounterErrorAverage} 에 대입하면:

	\begin{equation}
		\frac{\left( 1 - 2 f \right) r \Delta}{2}
	\end{equation}
	\iffalse

	There are a number of ways to compute the average error,
	based on a variety of assumptions about the patterns of
	increments and decrements.
	For simplicity, let's assume that the $f$ fraction of
	the operations are decrements, and that the error of interest
	is the deviation from the counter's long-term trend line.
	Under this assumption, if $f$ is less than or equal to 0.5,
	each decrement will be cancelled by an increment, so that
	$2f$ of the operations will cancel each other, leaving
	$1-2f$ of the operations being uncancelled increments.
	On the other hand, if $f$ is greater than 0.5, $1-f$ of
	the decrements are cancelled by increments, so that the
	counter moves in the negative direction by $-1+2\left(1-f\right)$,
	which simplifies to $1-2f$, so that the counter moves an average
	of $1-2f$ per operation in either case.
	Therefore, that the long-term
	movement of the counter is given by $\left( 1-2f \right) r$.
	Plugging this into
	Equation~\ref{eq:count:CounterErrorAverage} yields:

	\begin{equation}
		\frac{\left( 1 - 2 f \right) r \Delta}{2}
	\end{equation}
	\fi

	그렇지만, 대부분의 통계적 카운터 사용에서 \co{read_count()} 에 의해
	리턴되는 값의 오류치는 별 의미가 없습니다.
	\co{read_count()} 가 수행하는데 필요한 시간은 일반적으로
	\co{read_count()} 호출 사이의 시간에 비하면 극단적으로 작기 때문입니다.
	\iffalse

	All that aside, in most uses of statistical counters, the
	error in the value returned by \co{read_count()} is
	irrelevant.
	This irrelevance is due to the fact that the time required
	for \co{read_count()} to execute is normally extremely
	small compared to the time interval between successive
	calls to \co{read_count()}.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent
	Counters} 의 \co{inc_count()} 는 왜 어토믹 명령을 사용하지 않죠?
	쓰레드별 카운터를 여러 쓰레드에서 접근하고 있잖아요!
	\iffalse

	Why doesn't \co{inc_count()} in
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent Counters}
	need to use atomic instructions?
	After all, we now have multiple threads accessing the per-thread
	counters!
	\fi
\QuickA{}
	두 쓰레드 중 하나는 읽기만 하고 있고, 변수는 정렬되어 있으며 기계가
	지원하는 워드 크기이기에, 어토믹하지 않은 명령들만으로도
	충분합니다.
	다만, \co{ACCESS_ONCE()} 매크로는 카운터 업데이트가 \co{eventual()}
	에게 보여지는 것을 막을 수도 있는~\cite{JonCorbet2012ACCESS:ONCE}
	컴파일러 최적화를 막기 위해 사용되었습니다.
	\iffalse

	Because one of the two threads only reads, and because the
	variable is aligned and machine-sized, non-atomic instructions
	suffice.
	That said, the \co{ACCESS_ONCE()} macro is used to prevent
	compiler optimizations that might otherwise prevent the
	counter updates from becoming visible to
	\co{eventual()}~\cite{JonCorbet2012ACCESS:ONCE}.
	\fi

	이 알고리즘의 예전 버전은 어토믹 명령을 사용했습니다만, 감사하게도
	Ersoy Bayramoglu 가 그것들이 필요 없다는 것을 지적해 줬습니다.
	그렇다곤 하나, 쓰레드별 \co{counter} 변수가 \co{global_counter} 보다
	작았다면 어토믹 명령이 필요했을 겁니다.
	하지만, 32-bit 시스템에서 쓰레드별 \co{counter} 변수는 정확하게 합을
	구하기 위해 32 비트로 제한되어야 하고 오버플로를 막기 위해
	\co{global_count} 변수는 64-bit 이 되어야 할겁니다.
	이 경우엔, 오버플로를 막기 위해 쓰레드별 \co{counter} 변수를 주기적으로
	0으로 초기화 시켜줘야 할 겁니다.
	0으로의 주기적 초기화는 너무 오래 지연되면 쓰레드별 변수의 오버플로가
	가능하단 것을 반드시 기억해 둬야만 합니다.
	따라서 이 방법은 프로그램이 돌아가는 시스템이 리얼-타임 속성을 가지고
	있어야 하며, 매우 조심스럽게 사용되어야 함을 의미합니다.

	대조적으로, 모든 변수가 같은 크기이면 어떤 변수에 오버플로가 나더라도
	최종적 합은 워드 크기로 절삭될테니 별 문제 없습니다.
	\iffalse

	An older version of this algorithm did in fact use atomic
	instructions, kudos to Ersoy Bayramoglu for pointing out that
	they are in fact unnecessary.
	That said, atomic instructions would be needed in cases where
	the per-thread \co{counter} variables were smaller than the
	global \co{global_count}.
	However, note that on a 32-bit system,
	the per-thread \co{counter} variables
	might need to be limited to 32 bits in order to sum them accurately,
	but with a 64-bit \co{global_count} variable to avoid overflow.
	In this case, it is necessary to zero the per-thread
	\co{counter} variables periodically in order to avoid overflow.
	It is extremely important to note that this zeroing cannot
	be delayed too long or overflow of the smaller per-thread
	variables will result.
	This approach therefore imposes real-time requirements on the
	underlying system, and in turn must be used with extreme care.

	In contrast, if all variables are the same size, overflow
	of any variable is harmless because the eventual sum
	will be modulo the word size.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent
	Counters} 의 단일 글로벌 쓰레드인 \co{eventual()} 함수는 글로벌 락처럼
	큰 병목이 되거나 하진 않나요?
	\iffalse

	Won't the single global thread in the function \co{eventual()} of
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent Counters}
	be just as severe a bottleneck as a global lock would be?
	\fi
\QuickA{}
	이 경우엔, 아닙니다.
	그 대신 쓰레드의 갯수가 늘어나면 \co{read_count()} 에 리턴되는 카운터
	값이 더 부정확해질 겁니다.
	\iffalse

	In this case, no.
	What will happen instead is that as the number of threads increases,
	the estimate of the counter
	value returned by \co{read_count()} will become more inaccurate.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent
	Counters} 의 \co{read_count()} 에서 리턴하는 추정값은 쓰레드의 갯수가
	늘어날수록 부정확해져 가지 않을까요?
	\iffalse

	Won't the estimate returned by \co{read_count()} in
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent Counters}
	become increasingly
	inaccurate as the number of threads rises?
	\fi
\QuickA{}
	맞습니다.
	이게 문제가 된다면, 여러 \co{eventual()} 쓰레드를 만들고, 각 쓰레드가
	일을 나눠서 해야 하는게 한가지 해결책이 될 수 있습니다.
	더 극단적인 경우에는, tree 같은 \co{eventual()} 쓰레드 계층 관리가
	필요할 수도 있습니다.
	\iffalse

	Yes.
	If this proves problematic, one fix is to provide multiple
	\co{eventual()} threads, each covering its own subset of
	the other threads.
	In more extreme cases, a tree-like hierarchy of
	\co{eventual()} threads might be required.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent
	Counters} 의 최종적 일관성 알고리즘은 읽기에도 쓰기에도 매우 적은
	오버헤드와 극단적인 확장성을 보이는데, 과연 누가
	Section~\ref{sec:count:Array-Based Implementation} 같이 읽기 쪽이 비싼
	구현을 사용하겠습니까?
	\iffalse

	Given that in the eventually-consistent algorithm shown in
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent Counters}
	both reads and updates have extremely low overhead
	and are extremely scalable, why would anyone bother with the
	implementation described in
	Section~\ref{sec:count:Array-Based Implementation},
	given its costly read-side code?
	\fi
\QuickA{}
	\co{eventual()} 쓰레드를 돌리는 것은 CPU 시간을 소모합니다. 이
	최종적으로 일관적인 카운터가 추가되어가면 언젠가는 \co{eventual()}
	쓰레드들이 모든 CPU 를 차지할 겁니다.
	따라서 이 구현은 확장성이 쓰레드나 CPU 의 갯수가 아니라 최종적으로
	일관적인 카운터의 갯수에 제한되는, 또다른 종류의 확장성 한계 문제를
	갖습니다.
	\iffalse

	The thread executing \co{eventual()} consumes CPU time.
	As more of these eventually-consistent counters are added,
	the resulting \co{eventual()} threads will eventually
	consume all available CPUs.
	This implementation therefore suffers a different sort of
	scalability limitation, with the scalability limit being in
	terms of the number of eventually consistent counters rather
	than in terms of the number of threads or CPUs.
	\fi

\QuickQ{}
	다른 쓰레드의 카운터를 찾는데 왜 별개의 배열이 필요하죠?
	왜 gcc 는 리눅스 커널의 \co{per_cpu()} 가 쓰레드들이 다른 쓰레드의
	쓰레드별 변수를 쉽게 접근할 수 있도록 하는 것처럼 \co{per_thread()}
	같은 인터페이스를 제공하지 않나요?
	\iffalse

	Why do we need an explicit array to find the other threads'
	counters?
	Why doesn't gcc provide a \co{per_thread()} interface, similar
	to the Linux kernel's \co{per_cpu()} primitive, to allow
	threads to more easily access each others' per-thread variables?
	\fi
\QuickA{}
	정말 왜일까요?

	gcc 에는 리눅스 커널은 무시할 수 있는 몇가지 문제들이 존재합니다.
	유저 레벨 쓰레드가 종료될 때, 그 쓰레드의 쓰레드별 변수는 모두
	사라지는데 이로 인해, 적어도 유저 레벨
	RCU(Section~\ref{sec:defer:Read-Copy Update
	(RCU)}) 가 충분히 개선되기 전까지는, 쓰레드별 변수에의 액세스 문제는
	복잡해집니다.
	반면, 리눅스 커널에서는 한 CPU 가 오프라인이 되더라도 그 CPU 의 CPU 별
	변수는 여전히 매핑 되어 있고 액세스 가능한 상태로 남습니다.
	\iffalse

	Why indeed?

	To be fair, gcc faces some challenges that the Linux kernel
	gets to ignore.
	When a user-level thread exits, its per-thread variables all
	disappear, which complicates the problem of per-thread-variable
	access, particularly before the advent of user-level RCU
	(see Section~\ref{sec:defer:Read-Copy Update (RCU)}).
	In contrast, in the Linux kernel, when a CPU goes offline,
	that CPU's per-CPU variables remain mapped and accessible.
	\fi

	비슷하게, 새 유저 레벨 쓰레드가 생성되면, 그 쓰레드의 쓰레드별 변수는
	갑자기 생겨나야 합니다.
	반면, 리눅스 커널에서는 특정 CPU 가 아직 존재하지 않거나 나중에도
	존재하게 되는 일이 없더라도 부팅 과정에서 모든 CPU 별 변수의 매핑과
	초기화를 합니다.

	리눅스 커널이 가지고 있는 중요 제약은 컴파일 시간의 길이가 CPU 갯수인
	\co{CONFIG_NR_CPUS} 에 바운드 되며, 부팅 타임의 길이 역시
	\co{nr_cpu_ids} 에 바운드 된다는 점입니다.
	반면, 유저 스페이스에서는 쓰레드의 갯수에 의한, 하드코딩된 제약이
	존재하지 않습니다.
	\iffalse

	Similarly, when a new user-level thread is created, its
	per-thread variables suddenly come into existence.
	In contrast, in the Linux kernel, all per-CPU variables are
	mapped and initialized at boot time, regardless of whether
	the corresponding CPU exists yet, or indeed, whether the
	corresponding CPU will ever exist.

	A key limitation that the Linux kernel imposes is a compile-time
	maximum bound on the number of CPUs, namely, \co{CONFIG_NR_CPUS},
	along with a typically tighter boot-time bound of \co{nr_cpu_ids}.
	In contrast, in user space, there is no hard-coded upper limit
	on the number of threads.
	\fi

	물론, 두 환경 모두 다이나믹하게 로드되는 코드 (유저 스페이스라면 동적
	라이브러리, 리눅스 커널에서는 커널 모듈) 가 쓰레드별 변수의 복잡도를
	증가시키므로 해당 경우도 처리해야 합니다.

	이런 복잡성이 유저 스페이스 환경에서 다른 쓰레드의 쓰레드별 변수에의
	접근을 제공하기 어렵게 합니다.
	하지만 분명한건, 그런 접근은 상당히 유용하고, 따라서 언젠가는 그런
	인터페이스가 생겨나면 좋겠죠.
	\iffalse

	Of course, both environments must handle dynamically loaded
	code (dynamic libraries in user space, kernel modules in the
	Linux kernel), which increases the complexity of per-thread
	variables.

	These complications make it significantly harder for user-space
	environments to provide access to other threads' per-thread
	variables.
	Nevertheless, such access is highly useful, and it is hoped
	that it will someday appear.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Per-Thread Statistical Counters} 의 라인~19
	에서의 \co{NULL} 체크는 브랜치 예측 실패를 가져오지 않나요?
	항상 0인 변수 집합을 두고 더이상 사용되지 않는 카운터로의 포인터를
	\co{NULL} 로 만드는 대신 그 변수로 향하게 하는게 어떤가요?
	\iffalse

	Doesn't the check for \co{NULL} on line~19 of
	Figure~\ref{fig:count:Per-Thread Statistical Counters}
	add extra branch mispredictions?
	Why not have a variable set permanently to zero, and point
	unused counter-pointers to that variable rather than setting
	them to \co{NULL}?
	\fi
\QuickA{}
	말 되는 이야기입니다.
	다만 성능이 어떻게 달라지는지는 독자의 몫으로 남겨두겠습니다.
	다만, 이 코드가 빠르게 하고자 하는 곳은 \co{read_count()} 가 아니라
	\co{inc_count()} 임을 항상 기억해 두시기 바랍니다.
	\iffalse

	This is a reasonable strategy.
	Checking for the performance difference is left as an exercise
	for the reader.
	However, please keep in mind that the fastpath is not
	\co{read_count()}, but rather \co{inc_count()}.
	\fi

\QuickQ{}
	도대체 왜 Figure~\ref{fig:count:Per-Thread Statistical Counters} 의
	\co{read_count()} 함수의 합을 계산하는 곳에서 무거운 \emph{lock} 을
	사용하는거죠?
	\iffalse

	Why on earth do we need something as heavyweight as a \emph{lock}
	guarding the summation in the function \co{read_count()} in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}?
	\fi
\QuickA{}
	쓰레드가 종료될 때, 그 쓰레드의 쓰레드별 변수는 사라짐을 기억하세요.
	따라서, 한 쓰레드의 쓰레드별 변수를 그 쓰레드가 종료된 후에 접근하려
	하면 세그먼테이션 폴트가 날 겁니다.
	해당 락은 합 계산과 쓰레드 종료 작업을 중재해서 그런 일이 발생하지 않게
	해줍니다.
	\iffalse

	Remember, when a thread exits, its per-thread variables disappear.
	Therefore, if we attempt to access a given thread's per-thread
	variables after that thread exits, we will get a segmentation
	fault.
	The lock coordinates summation and thread exit, preventing this
	scenario.
	\fi

	물론, 대신 reader-writer 락을 사용해 read-acquire 할수도
	있겠습니다만 Chapter~\ref{chp:Deferred Processing} 에서 이 중재작업을
	그보다도 가볍게 해줄 수 있는 메커니즘을 소개할 겁니다.

	다른 방법으로는 쓰레드별 변수 대신 배열을 사용하는 방법이 있겠는데요,
	Alexey Roytman 이 이야기한대로 \co{NULL} 테스트를 없앨 수 있겠죠.
	하지만, 배열에의 접근은 대부분의 경우 쓰레드별 변수보다 느리고,
	쓰레드의 갯수의 최대값에 대한 제한을 가져올 겁니다.
	또한, 테스트도 락도 우리가 빠르게 하고자 하는 부분인 \co{inc_count()}
	에서는 사용되지 않고 있음을 기억하세요.
	\iffalse

	Of course, we could instead read-acquire a reader-writer lock,
	but Chapter~\ref{chp:Deferred Processing} will introduce even
	lighter-weight mechanisms for implementing the required coordination.

	Another approach would be to use an array instead of a per-thread
	variable, which, as Alexey Roytman notes, would eliminate
	the tests against \co{NULL}.
	However, array accesses are often slower than accesses to
	per-thread variables, and use of an array would imply a
	fixed upper bound on the number of threads.
	Also, note that neither tests nor locks are needed on the
	\co{inc_count()} fastpath.
	\fi

\QuickQ{}
	대체 왜 Figure~\ref{fig:count:Per-Thread Statistical Counters} 의
	\co{count_register_thread()} 함수에서 락을 잡아야 하는거죠?
	여기서 사용하는건 다른 쓰레드가 건들지 않는, 제대로 정렬된 기계의 워드
	스토어 사이즈 데이터이니 어토믹할 거잖아요, 아닌가요?
	\iffalse

	Why on earth do we need to acquire the lock in
	\co{count_register_thread()} in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}?
	It is a single properly aligned machine-word store to a location
	that no other thread is modifying, so it should be atomic anyway,
	right?
	\fi
\QuickA{}
	이 락은 실제로 없앨 수도 있습니다만, 특히 이 함수가 쓰레드 시작
	시점에서만 실행되고, 따라서 성능에 중요한 영역이 아닌만큼 좀 더 안전에
	치중했습니다.
	만약 우리가 이 코드를 수천개의 CPU 를 가진 기계에서 테스트 한다면야 이
	락을 없애야 할수도 있습니다만 ``겨우'' 수백개 CPU 의 기계라면 굳이
	그렇게 할 필요 없겠죠.
	\iffalse

	This lock could in fact be omitted, but better safe than
	sorry, especially given that this function is executed only at
	thread startup, and is therefore not on any critical path.
	Now, if we were testing on machines with thousands of CPUs,
	we might need to omit the lock, but on machines with ``only''
	a hundred or so CPUs, there is no need to get fancy.
	\fi

\QuickQ{}
	좋아요, 하지만 리눅스 커널은 CPU 별 카운터의 값을 합칠 때 락을 잡지
	않아요.
	유저 스페이스 코드에선 왜 이게 필요한거죠???
	\iffalse

	Fine, but the Linux kernel doesn't have to acquire a lock
	when reading out the aggregate value of per-CPU counters.
	So why should user-space code need to do this???
	\fi
\QuickA{}
	기억해보세요, 리눅스 커널의 CPU 별 변수들은 항상, 심지어 해당 CPU 가
	꺼져 있다 해도 접근 가능해요 --- 심지어 해당 CPU 가 한번도 켜졌던 적
	없고 앞으로도 켜질 일이 없다 해도요.
	\iffalse

	Remember, the Linux kernel's per-CPU variables are always
	accessible, even if the corresponding CPU is offline --- even
	if the corresponding CPU never existed and never will exist.
	\fi

\begin{figure}[tbp]
{ \scriptsize
\begin{verbatim}
  1 long __thread counter = 0;
  2 long *counterp[NR_THREADS] = { NULL };
  3 int finalthreadcount = 0;
  4 DEFINE_SPINLOCK(final_mutex);
  5 
  6 void inc_count(void)
  7 {
  8   counter++;
  9 }
 10 
 11 long read_count(void)
 12 {
 13   int t;
 14   long sum = 0;
 15 
 16   for_each_thread(t)
 17     if (counterp[t] != NULL)
 18       sum += *counterp[t];
 19   return sum;
 20 }
 21 
 22 void count_init(void)
 23 {
 24 }
 25 
 26 void count_register_thread(void)
 27 {
 28   counterp[smp_thread_id()] = &counter;
 29 }
 30 
 31 void count_unregister_thread(int nthreadsexpected)
 32 {
 33   spin_lock(&final_mutex);
 34   finalthreadcount++;
 35   spin_unlock(&final_mutex);
 36   while (finalthreadcount < nthreadsexpected)
 37     poll(NULL, 0, 1);
 38 }
\end{verbatim}
}
\caption{Per-Thread Statistical Counters With Lockless Summation}
\label{fig:count:Per-Thread Statistical Counters With Lockless Summation}
\end{figure}

	다만 문제를 회피하는 한가지 방법은
	Figure~\ref{fig:count:Per-Thread Statistical Counters With Lockless Summation}
	(\co{count_tstat.c})
	에 나온 것처럼 각 쓰레드가 모든 쓰레드가 끝날 때까지 종료하지 않게 하는
	겁니다.
	이 코드의 분석은 독자의 몫으로 남겨두겠습니다만 이건
	\url{counttorture.h} 의 카운터 성능 평가 방법과는 맞지 않음을 알아
	두세요.(왜일까요?)
	Chapter~\ref{chp:Deferred Processing} 에서는 이 상황을 훨씬 우아한
	방법으로 해결하는 동기화 메커니즘을 소개합니다.
	\iffalse

	One workaround is to ensure that each thread continues to exist
	until all threads are finished, as shown in
	Figure~\ref{fig:count:Per-Thread Statistical Counters With Lockless Summation}
	(\co{count_tstat.c}).
	Analysis of this code is left as an exercise to the reader,
	however, please note that it does not fit well into the
	\url{counttorture.h} counter-evaluation scheme.
	(Why not?)
	Chapter~\ref{chp:Deferred Processing} will introduce 
	synchronization mechanisms that handle this situation in a much
	more graceful manner.
	\fi

\QuickQ{}
	패킷의 사이즈가 다양하다면 패킷의 갯수를 세는 것과 패킷의 전체 바이트
	수를 세는 것에 어떤 기본적 차이가 있나요?
	\iffalse

	What fundamental difference is there between counting packets
	and counting the total number of bytes in the packets, given
	that the packets vary in size?
	\fi
\QuickA{}
	패킷의 갯수를 셀 때, 카운터는 한번에 1씩 증가합니다.
	반면, 바이트를 셀 때에는, 카운터는 큰 수만큼 증가할 수도 있습니다.

	왜 이걸 신경써야 할까요?
	1씩 증가하는 경우에 리턴되는 값은 비록 그 값이 이루어진 시점이
	언제인지는 알 수 없더라도 분명히 어떤 시점의 값인 것은 분명하기
	때문입니다.
	반면, 바이트를 세는 경우에는 두개의 서로 다른 쓰레드는 오퍼레이션들의
	순서에 따라 비일관적인 값을 리턴할 수도 있습니다.
	\iffalse

	When counting packets, the counter is only incremented by the
	value one.
	On the other hand, when counting bytes, the counter might
	be incremented by largish numbers.

	Why does this matter?
	Because in the increment-by-one case, the value returned will
	be exact in the sense that the counter must necessarily have
	taken on that value at some point in time, even if it is impossible
	to say precisely when that point occurred.
	In contrast, when counting bytes, two different threads might
	return values that are inconsistent with any global ordering
	of operations.
	\fi

	쓰레드~0 이 자신의 카운터에 3을, 쓰레드~1 이 자신의 카운터에 5를
	더하고, 쓰레드~2 와 쓰레드~3이 카운터를 더하는 경우를 생각해 봅시다.
	만약 시스템이 ``취약한 순서''를 가지거나 컴파일러가 강력한 최적화를
	사용한다면, 쓰레드~2 는 합이 3이고 쓰레드~3 은 합이 5라고 볼 수
	있습니다.
	일관적인 시스템이라면 값이 변하는 순서는 0,3,8 또는 0,5,8 만 가능하므로
	이 예에서 쓰레드들이 발견한 결과는 일관적이지 못합니다.

	이걸 깜박했다 해도, 당신만 그런게 아닙니다.
	Michael Scott 은 Paul E.~McKenney 의 박사 학위 심사 과정에서 이 질문을
	했습니다.
	\iffalse

	To see this, suppose that thread~0 adds the value three to its
	counter, thread~1 adds the value five to its counter, and
	threads~2 and 3 sum the counters.
	If the system is ``weakly ordered'' or if the compiler
	uses aggressive optimizations, thread~2 might find the
	sum to be three and thread~3 might find the sum to be five.
	The only possible global orders of the sequence of values
	of the counter are 0,3,8 and 0,5,8, and neither order is
	consistent with the results obtained.

	If you missed this one, you are not alone.
	Michael Scott used this question to stump Paul E.~McKenney
	during Paul's Ph.D. defense.
	\fi

\QuickQ{}
	리더는 쓰레드들의 카운터를 모두 더해야 하므로, 쓰레드의 갯수가 늘어나면
	더 많은 시간을 쓰게 될겁니다.
	리더에게도 쓸만한 성능과 확장성을 주면서 쓰기 작업도 여전히 빠르고
	확장성 있게 하는 방법은 없을까요?
	\iffalse

	Given that the reader must sum all the threads' counters,
	this could take a long time given large numbers of threads.
	Is there any way that the increment operation can remain
	fast and scalable while allowing readers to also enjoy
	reasonable performance and scalability?
	\fi
\QuickA{}
	글로벌한 추정값을 두는 게 한 방법이 될겁니다.
	리더들은 각자의 쓰레드별 변수를 증가시키되 어떤 미리 지정된 한계에
	도달했을 때에는 그 값을 글로벌 변수에 어토믹하게 더하고, 쓰레드별
	변수를 0으로 초기화 시키는 겁니다.
	이 방법은 병균적 쓰기 오버헤드와 읽혀지는 값의 정확성 사이의 협상을
	가능하게 할겁니다.

	독자분들은 다른 방법들, 예를 들어 컴바이닝 트리와 같은 것들을
	생각해보고 시도해보는 것도 좋을 겁니다.
	\iffalse

	One approach would be to maintain a global approximation
	to the value.
	Readers would increment their per-thread variable, but when it
	reached some predefined limit, atomically add it to a global
	variable, then zero their per-thread variable.
	This would permit a tradeoff between average increment overhead
	and accuracy of the value read out.

	The reader is encouraged to think up and try out other approaches,
	for example, using a combining tree.
	\fi

\QuickQ{}
	어째서
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	는 Section~\ref{sec:count:Statistical Counters} 에서 나왔던
	\co{inc_count()} 와 \co{dec_count()} 인터페이스 대신에
	\co{add_count()} 와 \co{sub_count()} 를 제공하나요?
	\iffalse

	Why does
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	provide \co{add_count()} and \co{sub_count()} instead of the
	\co{inc_count()} and \co{dec_count()} interfaces show in
	Section~\ref{sec:count:Statistical Counters}?
	\fi
\QuickA{}
	서로 다른 크기의 구조체들이 할당 요청되기 때문입니다.
	물론, 특정 크기의 구조체에만 사용되는 한계치 카운터는 여전히
	\co{inc_count()} 와 \co{dec_count()} 를 사용할 수 있을 겁니다.
	\iffalse

	Because structures come in different sizes.
	Of course, a limit counter corresponding to a specific size
	of structure might still be able to use
	\co{inc_count()} and \co{dec_count()}.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	라인~3 의 저 이상한 조건문은 뭔가요?
	왜 다음과 같이 더 직관적힌 형태의 빠른 수행경로를 사용하지 않는거죠?
	\iffalse

	What is with the strange form of the condition on line~3 of
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}?
	Why not the following more intuitive form of the fastpath?
	\fi

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\small
	\begin{verbatim}
  3 if (counter + delta <= countermax){
  4   counter += delta;
  5   return 1;
  6 }
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}
\QuickA{}
	두단어로 설명하죠.
	``인티저 오버플로우.''

	앞의 코드를 10의 값을 갖는 \co{counter} 와 \co{ULONG_MAX} 값을 갖는
	\co{delta} 에 대해 수행해 보세요.
	그러고 나서
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read} 의
	코드로 한번 더 해보세요.

	이 예제의 뒷부분은 인티저 오버플로우에 대한 깊은 이해를 필요로 하므로,
	인티저 오버플로우 문제를 한번도 겪어본 적 없다면, 몇몇 예제를 가지고
	이해해 보려 노력해보세요.
	일부 경우에 있어서는 인티저 오버플로우가 병렬 알고리즘보다도 제대로
	처리하기가 어렵습니다!
	\iffalse

	Two words.
	``Integer overflow.''

	Try the above formulation with \co{counter} equal to 10 and
	\co{delta} equal to \co{ULONG_MAX}.
	Then try it again with the code shown in
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}.

	A good understanding of integer overflow will be required for
	the rest of this example, so if you have never dealt with
	integer overflow before, please try several examples to get
	the hang of it.
	Integer overflow can sometimes be more difficult to get right
	than parallel algorithms!
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	에서 왜 \co{globalize_count()} 는 나중에 \co{balance_count()} 가
	쓰레드별 변수를 다시 채우도록 쓰레드별 변수를 0으로 바꾸나요?
	왜 그냥 쓰레드별 변수를 0이 아닌채로 놔두질 않는거죠?
	\iffalse

	Why does \co{globalize_count()} zero the per-thread variables,
	only to later call \co{balance_count()} to refill them in
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}?
	Why not just leave the per-thread variables non-zero?
	\fi
\QuickA{}
	사실 이전의 버전의 이 코드에서는 그렇게 했습니다.
	하지만 더하기와 빼기는 매우 비용이 싼 동작이고, 모든 특수 케이스를
	처리하는건 상당히 복잡합니다.
	다시 말하지만, 직접 한번 해보세요, 다만 인티저 오버플로우를 조심하구요!
	\iffalse

	That is in fact what an earlier version of this code did.
	But addition and subtraction are extremely cheap, and handling
	all of the special cases that arise is quite complex.
	Again, feel free to try it yourself, but beware of integer
	overflow!
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	에서 \co{globalreserve} 는 \co{add_count()} 에서 값이 구해지는데, 왜
	\co{sub_count()} 에서 값을 구하지 않나요?
	\iffalse

	Given that \co{globalreserve} counted against us in \co{add_count()},
	why doesn't it count for us in \co{sub_count()} in
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}?
	\fi
\QuickA{}
	\co{globalreserve} 변수는 모든 쓰레드의 \co{countermax} 변수의 합을
	따라갑니다.
	이 쓰레드의 \co{counter} 변수들의 합은 0 부터 \co{globalreserve} 사이
	어딘가일 것입니다.
	따라서 우리는 모든 쓰레드의 \co{counter} 변수가 \co{add_count()} 에서
	꽉 차고 \co{sub_count()} 에서 비어버린다 가정하는, 보수적 방법을
	취합니다.

	하지만 나중에 다시 한번 이야기할테니, 이 질문을 기억해 두세요.
	\iffalse

	The \co{globalreserve} variable tracks the sum of all threads'
	\co{countermax} variables.
	The sum of these threads' \co{counter} variables might be anywhere
	from zero to \co{globalreserve}.
	We must therefore take a conservative approach, assuming that
	all threads' \co{counter} variables are full in \co{add_count()}
	and that they are all empty in \co{sub_count()}.

	But remember this question, as we will come back to it later.
	\fi

\QuickQ{}
	한 쓰레드가 Figure~\ref{fig:count:Simple Limit Counter Add, Subtract,
	and Read} 의 \co{add_count()} 를 호출하고, 다른 쓰레드가
	\co{sub_count()} 를 호출한다고 해봅시다.
	\co{sub_count()} 는 카운터의 값이 0이 아님에도 실패하지 않겠습니까?
	\iffalse

	Suppose that one thread invokes \co{add_count()} shown in
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read},
	and then another thread invokes \co{sub_count()}.
	Won't \co{sub_count()} return failure even though the value of
	the counter is non-zero?
	\fi
\QuickA{}
	실제로 그럴 것입니다!
	많은 경우에, 이것은 Section~\ref{sec:count:Simple Limit Counter
	Discussion} 에서 이야기되는 것처럼 문제가 될 것이고, 그런 경우에는
	Section~\ref{sec:count:Exact Limit Counters} 에서 다루는 알고리즘을
	사용하는게 좋을 겁니다.
	\iffalse

	Indeed it will!
	In many cases, this will be a problem, as discussed in
	Section~\ref{sec:count:Simple Limit Counter Discussion}, and
	in those cases the algorithms from
	Section~\ref{sec:count:Exact Limit Counters}
	will likely be preferable.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}
	에서는 왜 \co{add_count()} 와 \co{sub_count()} 를 모두 가지고 있는
	거죠?
	그냥 \co{add_count()} 에 음수를 넘기면 되지 않나요?
	\iffalse

	Why have both \co{add_count()} and \co{sub_count()} in
	Figure~\ref{fig:count:Simple Limit Counter Add, Subtract, and Read}?
	Why not simply pass a negative number to \co{add_count()}?
	\fi
\QuickA{}
	\co{add_count()} \co{unsigned} \co{long} 타입을 인자로 받기 때문에,
	음수를 넘기는건 조금 어려울 겁니다.
	그리고 설령 반물질 메모리를 가지고 있다 해도, 사용중인 구조체의 수를
	세는데에 음수를 넘긴다는건 좀 말이 이상하죠!
	\iffalse

	Given that \co{add_count()} takes an \co{unsigned} \co{long}
	as its argument, it is going to be a bit tough to pass it a
	negative number.
	And unless you have some anti-matter memory, there is little
	point in allowing negative numbers when counting the number
	of structures in use!
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Simple Limit Counter Utility Functions} 의
	라인~15 에서는 왜 \co{counter} 를 \co{countermax / 2}  로 만들죠?
	그냥 \co{countermax} 값을 가져오는게 더 간단하지 않나요?
	\iffalse

	Why set \co{counter} to \co{countermax / 2} in line~15 of
	Figure~\ref{fig:count:Simple Limit Counter Utility Functions}?
	Wouldn't it be simpler to just take \co{countermax} counts?
	\fi
\QuickA{}
	첫째로, \co{countermax} 카운트는 이미 예약되어 있긴 합니다만 (라인~14
	를 참고하세요), 이 코드는 해당 순간에 해당 쓰레드에 의해서 실제로는 그
	반만 사용하고 있다고 알리는 것입니다.
	이렇게 함으로써 해당 쓰레드는 \co{globalcount} 까지 찾아가지 않고도
	최소 \co{countermax / 2} 만큼까지는 증가 또는 감소 작업을 할 수 있게
	합니다.

	\co{globalcount} 의 수는 라인~18 에서의 조정 덕에 여전히 정확함을
	참고하세요.
	\iffalse

	First, it really is reserving \co{countermax} counts
	(see line~14), however,
	it adjusts so that only half of these are actually in use
	by the thread at the moment.
	This allows the thread to carry out at least \co{countermax / 2}
	increments or decrements before having to refer back to
	\co{globalcount} again.

	Note that the accounting in \co{globalcount} remains accurate,
	thanks to the adjustment in line~18.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Schematic of Globalization and Balancing} 에서
	보면, 가운데와 오른쪽 구성을 잇는 뒤쪽의 점선을 보면 알 수 있듯이,
	한계까지 남은 카운트의 4분의 1이 쓰레드~0 에게 주어졌음에도, 8분의 1
	만이 사용되었습니다.
	왜 그런건가요?
	\iffalse

	In Figure~\ref{fig:count:Schematic of Globalization and Balancing},
	even though a quarter of the remaining count up to the limit is
	assigned to thread~0, only an eighth of the remaining count is
	consumed, as indicated by the uppermost dotted line connecting
	the center and the rightmost configurations.
	Why is that?
	\fi
\QuickA{}
	쓰레드~0 의 \co{counter} 가 \co{countermax} 의 절반으로 책정되었기
	때문입니다.
	따라서, 쓰레드~0 에 할당된 4분의 1 중 절반 (8분의 1)은 \co{globalcount}
	에서 오고, 나머지 절반(역시 8분의 1)은 남은 카운트에서 오도록 남겨두는
	것이죠.

	이런 방법을 취하는데에는 두가지 목적이 있습니다:
	(1)~쓰레드~0가 증가만이 아니라 감소에서도 빠른 수행 경로를 사용할 수
	있도록 하는것, 그리고
	(2)~모든 쓰레드가 단조적으로 한계점을 향해 증가만 하고 있다면
	비정확성을 줄이기 위해서입니다.
	마지막 이야기를 이해하려면, 알고리즘에 한발 더 다가가 자세히
	살펴보세요.
	\iffalse

	The reason this happened is that thread~0's \co{counter} was
	set to half of its \co{countermax}.
	Thus, of the quarter assigned to thread~0, half of that quarter
	(one eighth) came from \co{globalcount}, leaving the other half
	(again, one eighth) to come from the remaining count.

	There are two purposes for taking this approach:
	(1)~To allow thread~0 to use the fastpath for decrements as
	well as increments, and
	(2)~To reduce the inaccuracies if all threads are monotonically
	incrementing up towards the limit.
	To see this last point, step through the algorithm and watch
	what it does.
	\fi

\QuickQ{}
	쓰레드의 \co{counter} 와 \co{countermax} 변수를 한번에 어토믹하게
	수정해야 하는 이유가 뭐죠?
	각 변수를 개별적으로 어토믹하게 수정해도 충분하지 않아요?
	\iffalse

	Why is it necessary to atomically manipulate the thread's
	\co{counter} and \co{countermax} variables as a unit?
	Wouldn't it be good enough to atomically manipulate them
	individually?
	\fi
\QuickA{}
	그렇게도 할 수 있겠지만, 엄청난 주의가 필요합니다.
	\co{counter} 를 \co{countermax} 를 먼저 0으로 하지 않고 없애는 것은
	\co{counter} 를 0이 된 직후 증가시키는 같은 쓰레드가 카운터를 0으로
	만든 효과를 없애버리게 만듭니다.

	반대로, \co{countermax} 를 0으로 만들고 \co{counter} 를 없애는 것 역시
	0이 아닌 \co{counter} 를 만들 수 있습니다.
	이걸 자세히 보기 위해 다음의 이벤트 시퀀스를 봅시다:
	\iffalse

	This might well be possible, but great care is required.
	Note that removing \co{counter} without first zeroing
	\co{countermax} could result in the corresponding thread
	increasing \co{counter} immediately after it was zeroed,
	completely negating the effect of zeroing the counter.

	The opposite ordering, namely zeroing \co{countermax} and then
	removing \co{counter}, can also result in a non-zero
	\co{counter}.
	To see this, consider the following sequence of events:
	\fi

	\begin{enumerate}
	\item	Thread~A 가 자신의 \co{countermax}를 가져오고, 0이 아님을
		확인합니다.
	\item	Thread~B 가 Thread~A 의 \co{countermax} 를 0으로 만듭니다.
	\item	Thread~B 가 Thread~A 의 \co{counter}를 제거합니다.
	\item	자신의 \co{countermax} 가 0이 아님을 확인했던 Thread~A 는
		\co{counter} 에 값을 더하고, 이로 인해 \co{counter} 는 0이 아닌
		값을 갖습니다.
	\end{enumerate}

	다시 말하지만, \co{countermax} 와 \co{counter} 를 별개의 변수로 두고서
	어토믹하게 조정하는 것도 가능하긴 할겁니다만, 많은 주의가 필요할 것임은
	분명합니다.
	또한 그렇게 하는 것은 빠른 수행경로를 느리게 만들 확률이 큽니다.

	이런 가능성을 더 알아보는건 독자 여러분의 숙제로 남겨두겠습니다.
	\iffalse

	\begin{enumerate}
	\item	Thread~A fetches its \co{countermax}, and finds that
		it is non-zero.
	\item	Thread~B zeroes Thread~A's \co{countermax}.
	\item	Thread~B removes Thread~A's \co{counter}.
	\item	Thread~A, having found that its \co{countermax}
		is non-zero, proceeds to add to its \co{counter},
		resulting in a non-zero value for \co{counter}.
	\end{enumerate}

	Again, it might well be possible to atomically manipulate
	\co{countermax} and \co{counter} as separate variables,
	but it is clear that great care is required.
	It is also quite likely that doing so will slow down the
	fastpath.

	Exploring these possibilities are left as exercises for
	the reader.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access Functions}
	의 라인~7 에서는 C 표준을 어기는거 아닌가요?
	\iffalse

	In what way does line~7 of
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access Functions}
	violate the C standard?
	\fi
\QuickA{}
	해당 코드는 바이트당 비트가 8개라 가정합니다.
	이 가정은 공유 메모리 멀티프로세서에 쉽게 장착될 수 있는 현재의 모든
	상품화된 마이크로프로세서에 성립합니다만, 물론 C 코드가 돌아갈 수 있는
	모든 컴퓨터 시스템에 성립하진 않습니다.
	(C 표준에 맞추려면 대신 어떻게 할 수 있을까요? 그리고 그 때의 단점은
	무엇일까요?)
	\iffalse

	It assumes eight bits per byte.
	This assumption does hold for all current commodity microprocessors
	that can be easily assembled into shared-memory multiprocessors,
	but certainly does not hold for all computer systems that have
	ever run C code.
	(What could you do instead in order to comply with the C
	standard?  What drawbacks would it have?)
	\fi

\QuickQ{}
	\co{ctrandmax} 변수는 하나 뿐인데,
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access Functions}
	의 라인~18에서는 왜 굳이 포인터로 받는거죠?
	\iffalse

	Given that there is only one \co{ctrandmax} variable,
	why bother passing in a pointer to it on line~18 of
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access Functions}?
	\fi
\QuickA{}
	\co{ctrandmax} 변수는 \emph{쓰레드당} 한개씩만 있습니다.
	뒤에서 우리는 다른 쓰레드의 \co{ctrandmax} 변수를
	\co{split_ctrandmax()} 에 넘기는 코드도 보게 될겁니다.
	\iffalse

	There is only one \co{ctrandmax} variable \emph{per thread}.
	Later, we will see code that needs to pass other threads'
	\co{ctrandmax} variables to \co{split_ctrandmax()}.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access
	Functions} 의 \co{merge_ctrandmax()} 는 왜 바로 \co{atomic_t} 에 값을
	저장하지 않고 \co{int} 값을 리턴하는 거죠?
	\iffalse

	Why does \co{merge_ctrandmax()} in
	Figure~\ref{fig:count:Atomic Limit Counter Variables and Access Functions}
	return an \co{int} rather than storing directly into an
	\co{atomic_t}?
	\fi
\QuickA{}
	나중에, \co{atomic_cmpxchg()} 함수에 넘기기 위해 \co{int} 리턴이 필요한
	부분을 보게 될 겁니다.
	\iffalse

	Later, we will see that we need the \co{int} return to pass
	to the \co{atomic_cmpxchg()} primitive.
	\fi

\QuickQ{}
	우웩!
	Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract} 라인~11 의
	저 더러운 \co{goto} 는 웬말이예요?
	\co{break} 몰라요???
	\iffalse

	Yecch!
	Why the ugly \co{goto} on line~11 of
	Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract}?
	Haven't you heard of the \co{break} statement???
	\fi
\QuickA{}
	해당 \co{goto} 를 \co{break} 로 대체하려면 라인~15 에서 리턴해야 할지
	말아야 할지를 결정하기 위한 플래그를 하나 더 만들어야 할텐데, 이건 빠른
	수행 경로에서 하고자 하는 일은 아닐 겁니다.
	정말로 \co{goto} 를 그렇게나 싫어한다면, 이 빠른 수행 경로를 별도의
	함수로 집어넣고 그 함수에서 성공인지 실패인지를 리턴하게 하고
	``실패''는 느린 수행경로의 수행 필요를 나타내도록 하는게 최선일 겁니다.
	이건 goto 싫어하는 독자분들의 연습문제로 남겨두겠습니다.
	\iffalse

	Replacing the \co{goto} with a \co{break} would require keeping
	a flag to determine whether or not line~15 should return, which
	is not the sort of thing you want on a fastpath.
	If you really hate the \co{goto} that much, your best bet would
	be to pull the fastpath into a separate function that returned
	success or failure, with ``failure'' indicating a need for the
	slowpath.
	This is left as an exercise for goto-hating readers.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract} 의
	라인~13-14 의 \co{atomic_cmpxchg()} 함수는 어떻게 실패할 수 있죠?
	우린 이전 값을 라인~9 에서 가져오고 나서 바꾼 적 없잖아요!
	\iffalse

	Why would the \co{atomic_cmpxchg()} primitive at lines~13-14 of
	Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract}
	ever fail?
	After all, we picked up its old value on line~9 and have not
	changed it!
	\fi
\QuickA{}
	나중에, Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1}
	의 \co{flush_local_count()} 함수에서 어떻게 이 쓰레드의 \co{ctrandmax}
	변수를 Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract} 의
	라인~8-14 의 빠른 수행 경로 실행과 동시에 수정할 수 있는지 알아볼
	겁니다.
	\iffalse

	Later, we will see how the \co{flush_local_count()} function in
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1}
	might update this thread's \co{ctrandmax} variable concurrently
	with the execution of the fastpath on lines~8-14 of
	Figure~\ref{fig:count:Atomic Limit Counter Add and Subtract}.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1} 의
	라인~14 에서 \co{flush_local_count()} 가 \co{ctrandmax} 변수를 0 으로
	만든 후 그냥 다시 값을 넣을 수 없는 이유는 뭐죠?
	\iffalse

	What stops a thread from simply refilling its
	\co{ctrandmax} variable immediately after
	\co{flush_local_count()} on line~14 of
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1}
	empties it?
	\fi
\QuickA{}
	이 다른 쓰레드는 \co{flush_local_count()} 를 호출한 쪽에서
	\co{gblcnt_mutex} 를 해제하기 전까지는 자신의 \co{ctrandmax} 를 재설정
	할 수 없습니다.
	\co{gblcnt_mutex} 가 해제되는 시점에선, \co{flush_local_count()} 호출자
	쪽에서는 카운트 값의 사용을 이미 끝냈을 거고, 따라서 재설정에 문제는
	없습니다 --- \co{globalcount} 가 재설정을 허용할 만큼 충분히 크다는
	가정 하에서요.
	\iffalse
	This other thread cannot refill its \co{ctrandmax}
	until the caller of \co{flush_local_count()} releases the
	\co{gblcnt_mutex}.
	By that time, the caller of \co{flush_local_count()} will have
	finished making use of the counts, so there will be no problem
	with this other thread refilling --- assuming that the value
	of \co{globalcount} is large enough to permit a refill.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1} 의
	라인~27 에서 \co{flush_local_count()} 가 \co{ctrandmax} 변수를 비우는
	동안 \co{add_count()} 나 \co{sub_count()} 의 빠른 수행경로가
	\co{ctrandmax} 를 함께 사용하면서 동시에 수행되지 못하는 이유는 뭐죠?
	\iffalse

	What prevents concurrent execution of the fastpath of either
	\co{add_count()} or \co{sub_count()} from interfering with
	the \co{ctrandmax} variable while
	\co{flush_local_count()} is accessing it on line~27 of
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 1}
	empties it?
	\fi
\QuickA{}
	그런 이유는 없습니다.
	다음의 세가지 경우를 생각해보죠:
	\begin{enumerate}
	\item	만약 \co{flush_local_count()} 의 \co{atomic_xchg()} 가 이야기된
		빠른 수행경로 두개의 \co{split_ctrandmax()} 이전에 수행된다면,
		빠른 수행경로에서는 0 이 된 \co{counter} 와 \co{countermax} 를
		보게 될거고, 따라서 (물론 \co{delta} 가 0 이 아니라면) 그냥
		느린 수행경로로 넘어갈 겁니다.
	\item	만약 \co{flush_local_count()} 의 \co{atomic_xchg()} 가 두 빠른
		수행경로의 \co{split_ctrandmax()} 뒤에, 그러나 빠른 수행경로의
		\co{atomic_cmpxchg()} 보단 앞에 수행된다면,
		\co{atomic_cmpxchg()} 를 실패할거고, 빠른 수행경로를 재시작해서
		앞의 case~1 의 상황으로 돌아갈 겁니다.
	\item	만약 \co{flush_local_count()} 의 \co{atomic_xchg()} 가 두 빠른
		수행경로의 \co{atomic_cmpxchg()} 뒤에 수행된다면, 빠른
		수행경로는 \co{flush_local_count()} 가 해당 쓰레드의
		\co{ctrandmax} 변수를 0 으로 만들기 이전에 이미 성공적으로
		완료될 겁니다.
	\end{enumerate}
	어느쪽이든, 경주는 올바르게 마무리 됩니다.
	\iffalse

	Nothing.
	Consider the following three cases:
	\begin{enumerate}
	\item	If \co{flush_local_count()}'s \co{atomic_xchg()} executes
		before the \co{split_ctrandmax()} of either fastpath,
		then the fastpath will see a zero \co{counter} and
		\co{countermax}, and will thus transfer to the slowpath
		(unless of course \co{delta} is zero).
	\item	If \co{flush_local_count()}'s \co{atomic_xchg()} executes
		after the \co{split_ctrandmax()} of either fastpath,
		but before that fastpath's \co{atomic_cmpxchg()},
		then the \co{atomic_cmpxchg()} will fail, causing the
		fastpath to restart, which reduces to case~1 above.
	\item	If \co{flush_local_count()}'s \co{atomic_xchg()} executes
		after the \co{atomic_cmpxchg()} of either fastpath,
		then the fastpath will (most likely) complete successfully
		before \co{flush_local_count()} zeroes the thread's
		\co{ctrandmax} variable.
	\end{enumerate}
	Either way, the race is resolved correctly.
	\fi

\QuickQ{}
	\co{atomic_set()} 은 주어진 \co{atomic_t} 에 단순히 스토어를 할 뿐인데,
	어떻게
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 2} 의
	라인~21 에서의 \co{balance_count()} 는 \co{flush_local_count()} 의 해당
	변수에 동시에 가해지는 업데이트에도 불구하고 올바르게 동작할 수 있는
	거죠?
	\iffalse

	Given that the \co{atomic_set()} primitive does a simple
	store to the specified \co{atomic_t}, how can line~21 of
	\co{balance_count()} in
	Figure~\ref{fig:count:Atomic Limit Counter Utility Functions 2}
	work correctly in face of concurrent \co{flush_local_count()}
	updates to this variable?
	\fi
\QuickA{}
	\co{balance_count()} 와 \co{flush_local_count()} 의 호출자 모두
	\co{gblcnt_mutex} 를 잡고 있으므로, 한번에 한쪽만 수행될 수 있습니다.
	\iffalse

	The caller of both \co{balance_count()} and
	\co{flush_local_count()} hold \co{gblcnt_mutex}, so
	only one may be executing at a given time.
	\fi

\QuickQ{}
	하지만 시그널 핸들러는 수행 중에 다른 CPU 로 옮겨져서 수행될 수도
	있잖아요.
	이런 가능성은 쓰레드와 해당 쓰레드를 인터럽트 하는 시그널 핸들러 사이의
	안정적인 통신을 위해 어토믹 인스트럭션과 메모리 배리어를 필요로 하게
	만들지 않을까요?
	\iffalse

	But signal handlers can be migrated to some other
	CPU while running.
	Doesn't this possibility require that atomic instructions
	and memory barriers are required to reliably communicate
	between a thread and a signal handler that interrupts that
	thread?
	\fi
\QuickA{}
	아니요.
	시그널 핸들러가 다른 CPU 로 옮겨가면, 인터럽트된 쓰레드 역시 그리로
	옮겨집니다.
	\iffalse

	No.
	If the signal handler is migrated to another CPU, then the
	interrupted thread is also migrated along with it.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft State Machine} 에서 REQ \co{theft}
	상태는 왜 빨간색으로 칠해졌나요?
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft State Machine}, why is
	the REQ \co{theft} state colored red?
	\fi
\QuickA{}
	빠른 수행 경로만이 \co{theft} 상태를 바꿀 수 있음과 해당 쓰레드가 이
	상태에 너무 오래 머무르면, 느린 수행 경로를 수행하고 있는 쓰레드는
	POSIX 시그널을 다시 보낼 것임을 알리기 위해서입니다.
	\iffalse

	To indicate that only the fastpath is permitted to change the
	\co{theft} state, and that if the thread remains in this
	state for too long, the thread running the slowpath will
	resend the POSIX signal.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft State Machine} 에서, 두개의 분리된
	REQ 와 ACK \co{theft} 상태를 갖는 이유가 뭐죠?
	왜 그 두 상태를 하나의 REQACK 상태로 만들어서 스테이트 머신을 간단하게
	만들지 않는 거예요?
	만약 그렇게 하면 그 상태에 먼저 도달하는 시그널 핸들러나 빠른 수행
	경로가 상태를 READY 로 바꿀 수 있을 텐데요.
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft State Machine}, what is
	the point of having separate REQ and ACK \co{theft} states?
	Why not simplify the state machine by collapsing
	them into a single REQACK state?
	Then whichever of the signal handler or the fastpath gets there
	first could set the state to READY.
	\fi
\QuickA{}
	REQ 와 ACK 상태를 합치는게 나쁜 이유를 들어보자면:
	\begin{enumerate}
	\item	해당 느린 수행 경로는 REQ 와 ACK 상태를 사용해 언제 시그널이
		다시 보내져야 할지 결정합니다.
		만약 해당 상태들이 합쳐진다면, 해당 느린 수행 경로는 반복적으로
		시그널을 보내는 수밖에 없고, 빠른 수행경로를 불필요하게 느리게
		만드는 효과를 만들 겁니다.
	\item	다음과 같은 레이스가 일어날 수 있습니다:
		\begin{enumerate}
		\item	느린 수행 경로가 주어진 쓰레드의 상태를 REQACK 으로
			만듭니다.
		\item	해당 쓰레드는 방금 빠른 수행 경로를 끝낸 참이었고,
			REQACK 상태임을 확인합니다.
		\item	해당 쓰레드는 시그널을 받고, 여기서도 REQACK 상태임을
			확인합니다만, 빠른 수행 경로는 아무 효과를 발휘하지
			못한 채이므로, 상태를 READY 로 바꿉니다.
		\item	느린 수행 경로는 READY 상태를 확인하고, 카운트를
			가져가고 상태를 IDLE 로 돌려놓고 완료됩니다.
		\item	빠른 수행 경로는 상태를 READY 로 바꾸고, 이
			쓰레드에서의 다음 빠른 수행 경로 수행을 막아버립니다.
		\end{enumerate}
		여기서의 기본적 문제는 합쳐진 REQACK 상태는 시그널 핸들러와
		빠른 수행 경로 둘 다 볼 수 있다는 겁니다.
		네개의 상태로 관리되는 명확한 분리 상태는 순서화된 상태 전환을
		분명히 보장합니다.
	\end{enumerate}
	\iffalse

	Reasons why collapsing the REQ and ACK states would be a very
	bad idea include:
	\begin{enumerate}
	\item	The slowpath uses the REQ and ACK states to determine
		whether the signal should be retransmitted.
		If the states were collapsed, the slowpath would have
		no choice but to send redundant signals, which would
		have the unhelpful effect of needlessly slowing down
		the fastpath.
	\item	The following race would result:
		\begin{enumerate}
		\item	The slowpath sets a given thread's state to REQACK.
		\item	That thread has just finished its fastpath, and
			notes the REQACK state.
		\item	The thread receives the signal, which also notes
			the REQACK state, and, because there is no fastpath
			in effect, sets the state to READY.
		\item	The slowpath notes the READY state, steals the
			count, and sets the state to IDLE, and completes.
		\item	The fastpath sets the state to READY, disabling
			further fastpath execution for this thread.
		\end{enumerate}
		The basic problem here is that the combined REQACK state
		can be referenced by both the signal handler and the
		fastpath.
		The clear separation maintained by the four-state
		setup ensures orderly state transitions.
	\end{enumerate}
	\fi
	그렇다곤 하지만, 세개의 상태만으로도 제대로 동작하도록 할 수 있을 수도
	있습니다.
	만약 성공하면 네개 상태 버전과 잘 비교해 보세요.
	세개 상태 버전이 정말 더 낫나요, 그리고 왜죠 또는 왜 아니죠?
	\iffalse

	That said, you might well be able to make a three-state setup
	work correctly.
	If you do succeed, compare carefully to the four-state setup.
	Is the three-state solution really preferable, and why or why not?
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration
	Functions} 의 \co{flush_local_count_sig()} 함수에서는 왜 \co{theft}
	쓰레드별 변수의 사용을 \co{ACCESS_ONCE()} 로 감싼거죠?
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions}
	function \co{flush_local_count_sig()}, why are there
	\co{ACCESS_ONCE()} wrappers around the uses of the
	\co{theft} per-thread variable?
	\fi
\QuickA{}
	첫번째 \co{ACCESS_ONCE} 사용은 (라인~11) 은 필요 없는 것이라 주장할
	수도 있습니다.
	다음의 두 군데 사용은 (라인~14 와 16) 중요합니다.
	이것들이 없어지면, 컴파일러는 라인~14-17 을 다음과 같이 바꿀 수도
	있습니다:
	\iffalse

	The first one (on line~11) can be argued to be unnecessary.
	The last two (lines~14 and 16) are important.
	If these are removed, the compiler would be within its rights
	to rewrite lines~14-17 as follows:
	\fi
	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\small
	\begin{verbatim}
 14   theft = THEFT_READY;
 15   if (counting) {
 16     theft = THEFT_ACK;
 17   }
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}
	느린 수행 경로는 잠깐 들어오는 값인 \co{THEFT_READY} 를 보고서 연관된
	쓰레드가 준비되기도 전에 값을 훔쳐가기 시작할테니 위험합니다.
	\iffalse

	This would be fatal, as the slowpath might see the transient
	value of \co{THEFT_READY}, and start stealing before the
	corresponding thread was ready.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration
	Functions} 에서, 왜 다른 쓰레드의 \co{countermax} 변수를 바로 접근해도
	안전한 거죠?
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions},
	why is it safe for line~28 to directly access the other thread's
	\co{countermax} variable?
	\fi
\QuickA{}
	그 다른 쓰레드는 자신의 \co{countermax} 변수의 값을 \co{gblcnt_mutex}
	락을 쥐지 않은 한 수정할 수 없기 때문입니다.
	하지만 이 함수 호출 코드는 락을 잡고 함수를 호출하기 때문에, 그 다른
	쓰레드는 해당 락을 잡을 수가 없고, 따라서 그 다른 쓰레드는
	\co{countermax} 변수를 수정할 수 없습니다.
	따라서 바로 접근해도 안전합니다 --- 하지만 바꾸진 않습니다.
	\iffalse

	Because the other thread is not permitted to change the value
	of its \co{countermax} variable unless it holds the
	\co{gblcnt_mutex} lock.
	But the caller has acquired this lock, so it is not possible
	for the other thread to hold it, and therefore the other thread
	is not permitted to change its \co{countermax} variable.
	We can therefore safely access it --- but not change it.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration
	Functions} 에서, 왜 라인~33 은 현재 쓰레드가 자기 자신에게 시그널을
	보내는지 체크하지 않나요?
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions},
	why doesn't line~33 check for the current thread sending itself
	a signal?
	\fi
\QuickA{}
	또한번 체크할 필요가 없습니다.
	\co{flush_local_count()} 는 이미 \co{globalize_count()} 를 호출했으니,
	라인~28 에서의 체크가 성공해서 뒤의 \co{pthread_kill()} 은 스킵될
	겁니다.
	\iffalse

	There is no need for an additional check.
	The caller of \co{flush_local_count()} has already invoked
	\co{globalize_count()}, so the check on line~28 will have
	succeeded, skipping the later \co{pthread_kill()}.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions}
	의 코드는 gcc 와 POSIX 에서 동작합니다.
	ISO C 표준에서 동작하게 하려면 뭐가 필요할까요?
	\iffalse

	The code in
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions},
	works with gcc and POSIX.
	What would be required to make it also conform to the ISO C standard?
	\fi
\QuickA{}
	\co{theft} 변수는 안전하게 시그널 핸들러와 시그널에 인터럽트되는 코드
	사이에서 안전하게 공유될 수 있도록 \co{sig_atomic_t} 타입이어야만
	합니다.
	\iffalse

	The \co{theft} variable must be of type \co{sig_atomic_t}
	to guarantee that it can be safely shared between the signal
	handler and the code interrupted by the signal.
	\fi

\QuickQ{}
	Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration
	Functions} 의 라인~41 에서는 왜 시그널을 다시 보내죠?
	\iffalse

	In Figure~\ref{fig:count:Signal-Theft Limit Counter Value-Migration Functions}, why does line~41 resend the signal?
	\fi
\QuickA{}
	지난 수십년간 많은 운영 체제는 갑자기 시그널을 잃어버리는 특성을 가졌기
	때문입니다.
	이게 기능인지 버그인지는 논쟁거리이지만, 그건 무의미합니다.
	사용자가 보기에 분명한 증상은 커널 버그가 아니라 사용자
	어플리케이션의 문제입니다.

	\emph{당신의} 어플리케이션의 문제입니다!
	\iffalse

	Because many operating systems over several decades have
	had the property of losing the occasional signal.
	Whether this is a feature or a bug is debatable, but
	irrelevant.
	The obvious symptom from the user's viewpoint will not be
	a kernel bug, but rather a user application hanging.

	\emph{Your} user application hanging!
	\fi

\QuickQ{}
	POSIX 시그널만 느린게 아니라, 시그널을 각 쓰레드에 보내는 행위 자체가
	확장성이 없어요.
	만약 10,000 개의 쓰레드가 있고 읽는 쪽도 빨라야 한다면 어떻게
	하시겠어요?
	\iffalse

	Not only are POSIX signals slow, sending one to each thread
	simply does not scale.
	What would you do if you had (say) 10,000 threads and needed
	the read side to be fast?
	\fi
\QuickA{}
	한가지 방법은
	Section~\ref{sec:count:Eventually Consistent Implementation} 에서
	보였던, 한개의 카운터 변수에 추정치를 합하는 방법입니다.
	또다른 방법으로는 각각 업데이트를 하는 쓰레드의 일부와 상호작용하면서
	읽기 작업을 함께 하는 복수의 쓰레드를 사용하는 방법도 있겠습니다.
	\iffalse

	One approach is to use the techniques shown in
	Section~\ref{sec:count:Eventually Consistent Implementation},
	summarizing an approximation to the overall counter value in
	a single variable.
	Another approach would be to use multiple threads to carry
	out the reads, with each such thread interacting with a
	specific subset of the updating threads.
	\fi

\QuickQ{}
	아래쪽 한계는 명확하게 지키지만 위쪽 한계는 좀 정확하지 않아도 되는
	한계 카운터를 원한다면 어떻게 하면 될까요?
	\iffalse

	What if you want an exact limit counter to be exact only for
	its lower limit, but to allow the upper limit to be inexact?
	\fi
\QuickA{}
	한가지 간단한 해결책은 위쪽 한계를 원하는 만큼 더 높게 잡아주는 것입니다.
	그렇게 더 높게 리미트를 잡아주는 것의 한계는 카운터가 표현할 수 있는
	최대의 값이 될 것입니다.
	\iffalse

	One simple solution is to overstate the upper limit by the
	desired amount.
	The limiting case of such overstatement results in the
	upper limit being set to the largest value that the counter is
	capable of representing.
	\fi

\QuickQ{}
	바이어스된 카운터를 사용할 때 그 외에 뭘 하면 좋을까요?
	\iffalse

	What else had you better have done when using a biased counter?
	\fi
\QuickA{}
	카운터가 액세스의 수가 최대값에 가까울 때에도 효과적으로 동작할 수
	있도록 위쪽 리미트를 바이어스, 예상되는 최대 액세스 수, 그리고 충분한
	``출렁거림'' 을 수용하기 충분하도록 크게 잡는게 좋을 겁니다.
	\iffalse

	You had better have set the upper limit to be large enough
	accommodate the bias, the expected maximum number of accesses,
	and enough ``slop'' to allow the counter to work efficiently
	even when the number of accesses is at its maximum.
	\fi

\QuickQ{}
	이거 참 웃기네요!
	카운터를 \emph{업데이트} 하기 위해 리더-라이터 락의 \emph{읽기} 권한
	획득을 한다니요?
	뭐하는거예요???
	\iffalse

	This is ridiculous!
	We are \emph{read}-acquiring a reader-writer lock to
	\emph{update} the counter?
	What are you playing at???
	\fi
\QuickA{}
	이상해 보일 수 있겠죠, 하지만 진짜예요!
	``리더-라이터 락'' 이라는 이름은 사실 완벽하게 의미를 설명하지 못한다는
	점을 상기하면 이해가 될 거예요, 그렇죠?
	\iffalse

	Strange, perhaps, but true!
	Almost enough to make you think that the name
	``reader-writer lock'' was poorly chosen, isn't it?
	\fi

\QuickQ{}
	실제 시스템에 적용하려면 해결해야할 문제들이 또 뭐가 있을 수 있을까요?
	\iffalse

	What other issues would need to be accounted for in a real system?
	\fi
\QuickA{}
	엄청나게 많죠!

	일단 몇가지 생각을 시작할 것들은:

	\begin{enumerate}
	\item	디바이스는 여러개가 있을 수 있으니, 전역 변수는 적절치 못하고,
		\co{do_io()}  에 인자가 없는 것도 마찬가지죠.
	\item	폴링하는 루프는 실제 시스템에서는 문제가 있을 수 있습니다.
		많은 경우, 마지막으로 I/O 를 완료 하는 쪽에서 디바이스 제거
		쓰레드를 깨우는 편이 낫습니다.
	\item	I/O 는 실패할 수 있으므로, \co{do_io()} 는 리턴 값을 가져야 할
		겁니다.
	\item	디바이스가 고장나면, 마지막 I/O 는 성공하지 못할 것입니다.
		이런 경우, 에러 복구를 위한 어떤 타임아웃 같은 것이 필요할
		것입니다.
	\item	\co{add_count()} 와 \co{sub_count()} 모두 실패할 수 있는데
		리턴값을 체크하지 않았습니다.
	\item	리더-라이터 락은 확장성이 그다지 좋지 않습니다.
		리더-라이터 락의 읽기 권한 획득의 높은 비용을 회피하는 방법
		한가지가 Chapter~\ref{chp:Locking},\ref{chp:Deferred
		Processing} 에 소개되어 있습니다.
	\item	폴링 루프는 매우 낮은 에너지 효율성을 초래할 것입니다.
		이벤트 기반 설계가 나을 겁니다.
	\end{enumerate}
	\iffalse

	A huge number!

	Here are a few to start with:

	\begin{enumerate}
	\item	There could be any number of devices, so that the
		global variables are inappropriate, as are the
		lack of arguments to functions like \co{do_io()}.
	\item	Polling loops can be problematic in real systems.
		In many cases, it is far better to have the last
		completing I/O wake up the device-removal thread.
	\item	The I/O might fail, and so \co{do_io()} will likely
		need a return value.
	\item	If the device fails, the last I/O might never complete.
		In such cases, there might need to be some sort of
		timeout to allow error recovery.
	\item	Both \co{add_count()} and \co{sub_count()} can
		fail, but their return values are not checked.
	\item	Reader-writer locks do not scale well.
		One way of avoiding the high read-acquisition costs
		of reader-writer locks is presented in
		Chapters~\ref{chp:Locking}
		and~\ref{chp:Deferred Processing}.
	\item	The polling loops result in poor energy efficiency.
		An event-driven design is preferable.
	\end{enumerate}
	\fi

\QuickQ{}
	Table~\ref{tab:count:Statistical Counter Performance on Power-6} 의
	\url{count_stat.c} 열에 보면 읽기 성능이 쓰레드 수에 따라
	선형적으로 확장되는데요.
	쓰레드 수가 늘어나면 더 많은 쓰레드별 카운터의 합이 이루어져야 하는데
	어떻게 그게 가능하죠?
	\iffalse

	On the \url{count_stat.c} row of
	Table~\ref{tab:count:Statistical Counter Performance on Power-6},
	we see that the read-side scales linearly with the number of
	threads.
	How is that possible given that the more threads there are,
	the more per-thread counters must be summed up?
	\fi
\QuickA{}
	읽는 쪽의 코드는 쓰레드의 수와 상관 없이 고정된 크기의 배열 전체를
	읽어야 하기 때문에 성능에 차이가 없습니다.
	반면, 뒤의 두개 알고리즘의 경우 쓰레드가 늘어나면 더 많은 일을 하게
	됩니다.
	더불어, 뒤의 두개 알고리즘은 쓰레드 ID 와 연관된 \co{__thread} 변수
	사이의 매핑을 유지하는 추가적인 계층을 갖습니다.
	\iffalse

	The read-side code must scan the entire fixed-size array, regardless
	of the number of threads, so there is no difference in performance.
	In contrast, in the last two algorithms, readers must do more
	work when there are more threads.
	In addition, the last two algorithms interpose an additional
	level of indirection because they map from integer thread ID
	to the corresponding \co{__thread} variable.
	\fi

\QuickQ{}
	Table~\ref{tab:count:Statistical Counter Performance on Power-6} 의
	마지막 열을 보더라도 통계적 카운터 구현의 읽기쪽 성능은 매우 나쁘군요.
	왜 이렇게 성능 나쁜 알고리즘을 신경쓰는거죠?
	\iffalse

	Even on the last row of
	Table~\ref{tab:count:Statistical Counter Performance on Power-6},
	the read-side performance of these statistical counter
	implementations is pretty horrible.
	So why bother with them?
	\fi
\QuickA{}
	``해야할 일에 걸맞는 도구를 사용하세요.''

	Figure~\ref{fig:count:Atomic Increment Scalability on Nehalem} 에서 볼
	수 있듯이, 하나의 변수에 어토믹 증가 오퍼레이션을 사용하는 방법은
	상당한 양의 병렬적 업데이트가 있는 작업에 사용되어선 안됩니다.
	반면, Table~\ref{tab:count:Statistical Counter Performance on Power-6}
	에 보인 알고리즘들은 업데이트가 많은 상황에서 일을 훌륭하게 처리할
	것입니다.
	물론, 읽기가 대부분인 상황이라면, 다른걸 사용해야 합니다. 예를 들자면,
	Section~\ref{sec:count:Eventually Consistent Implementation} 에 사용된
	것과 비슷하게 한번의 로드 오퍼레이션으로 읽어낼 수 있는, 어토믹하게
	증가되는 변수를 사용하는 결과적 일관성 설계와 같은 거요.
	\iffalse

	``Use the right tool for the job.''

	As can be seen from
	Figure~\ref{fig:count:Atomic Increment Scalability on Nehalem},
	single-variable atomic increment need not apply for any job
	involving heavy use of parallel updates.
	In contrast, the algorithms shown in
	Table~\ref{tab:count:Statistical Counter Performance on Power-6}
	do an excellent job of handling update-heavy situations.
	Of course, if you have a read-mostly situation, you should
	use something else, for example, an eventually consistent design
	featuring a single atomically incremented
	variable that can be read out using a single load,
	similar to the approach used in
	Section~\ref{sec:count:Eventually Consistent Implementation}.
	\fi

\QuickQ{}
	Table~\ref{tab:count:Limit Counter Performance on Power-6} 에 보여진
	성능 데이터를 놓고 보자면, 우리는 항상 어토믹 오퍼레이션보다는 시그널을
	사용해야겠군요, 그렇죠?
	\iffalse

	Given the performance data shown in
	Table~\ref{tab:count:Limit Counter Performance on Power-6},
	we should always prefer signals over atomic operations, right?
	\fi
\QuickA{}
	그건 워크로드에 따라 달라집니다.
	64-코어 시스템이라면, 단지 한개의 시그널 (약 40-나노세컨드 성능 향상)
	을 만들기 위해 100 개가 넘는 어토믹하지 않은 오퍼레이션들의 실행 (약
	5-\emph{마이크로세컨드} 성능 저하) 이 필요합니다.
	더욱 읽기 위주인 워크로드는 여전히 존재하지만, 현재 처리해야하는 특정
	워크로드에 신경쓸 필요가 있습니다.

	또한, 역사적으로 메모리 배리어는 일반 인스트럭션들에 비해 비용이
	비쌌지만, 당신이 운용하게 될 특정 하드웨어에서도 그러한지 확인해 봐야
	합니다.
	컴퓨터 하드웨어의 특성은 시간에 따라 변하고, 알고리즘도 그에 맞춰
	변해야만 합니다.
	\iffalse

	That depends on the workload.
	Note that on a 64-core system, you need more than
	one hundred non-atomic operations (with roughly
	a 40-nanosecond performance gain) to make up for even one
	signal (with almost a 5-\emph{microsecond} performance loss).
	Although there are no shortage of workloads with far greater
	read intensity, you will need to consider your particular
	workload.

	In addition, although memory barriers have historically been
	expensive compared to ordinary instructions, you should
	check this on the specific hardware you will be running.
	The properties of computer hardware do change over time,
	and algorithms must change accordingly.
	\fi

\QuickQ{}
	Table~\ref{tab:count:Limit Counter Performance on Power-6} 에 보여진
	읽는 쓰레드간의 락 컨텐션을 해결하기 위해 고급 테크닉들이 사용될 수
	있을까요?
	\iffalse

	Can advanced techniques be applied to address the lock
	contention for readers seen in
	Table~\ref{tab:count:Limit Counter Performance on Power-6}?
	\fi
\QuickA{}
	한가지 해결책은 scalable non-zero
	indicators(SNZI)~\cite{FaithEllen:2007:SNZI} 처럼 업데이트 쪽 성능을
	약간 포기하는 겁니다.
	SNZI 외에도 이 해결책을 구현하는 여러 방법이 있겠지만, 그건 독자의
	몫으로 남겨두겠습니다.
	자주 획득이 요청되는 글로벌 락을 낮은 레벨의 계층의 로컬 락의 획득들로
	대체하는 계층적 방법들도 이 문제를 잘 해결할 겁니다.
	\iffalse

	One approach is to give up some update-side performance, as is
	done with scalable non-zero indicators
	(SNZI)~\cite{FaithEllen:2007:SNZI}.
	There are a number of other ways one might go about this, and these
	are left as exercises for the reader.
	Any number of approaches that apply hierarchy, which replace
	frequent global-lock acquisitions with local lock acquisitions
	corresponding to lower levels of the hierarchy, should work quite well.
	\fi

\QuickQ{}
	\co{++} 오퍼레이터는 1,000 자리 숫자에도 잘 동작해요!
	연산자 오버로딩이라고 못들어봤어요???
	\iffalse

	The \co{++} operator works just fine for 1,000-digit numbers!
	Haven't you heard of operator overloading???
	\fi
\QuickA{}
	C++ 언어에서라면 그런 수를 구현하는 클래스에 액세스가 가능하다는 가정
	하에 1,000 자리 숫자에도 \co{++} 을 사용할 수 있겠죠.
	하지만 최소 2010년 까지는, C 언어는 오퍼레이터 오버로딩을 허용하지
	않습니다.
	\iffalse

	In the C++ language, you might well be able to use \co{++}
	on a 1,000-digit number, assuming that you had access to a
	class implementing such numbers.
	But as of 2010, the C language does not permit operator overloading.
	\fi

\QuickQ{}
	하지만 우리가 모든 것을 분할할 거라면, 왜 공유 메모리 멀티쓰레딩을
	신경쓰죠?
	그냥 문제를 완벽하게 분할해버리고 각 분할된 조각들을 여러 프로세스들로,
	각자의 어드레스 스페이스에서 처리하도록 돌리지 않는건가요?
	\iffalse

	But if we are going to have to partition everything, why bother
	with shared-memory multithreading?
	Why not just partition the problem completely and run as
	multiple processes, each in its own address space?
	\fi
\QuickA{}
	사실, 별도의 어드레스 스페이스를 갖는 여러 프로세스들은 병렬성을 보일
	수 있는 훌륭한 방법으로, 포크-조인 방법론의 지지자들과 Erlang 언어가 그
	사실을 잘 입증합니다.
	하지만, 공유 메모리 병렬성만의 장점 역시 일부 있습니다:
	\begin{enumerate}
	\item	어플리케이션의 성능에 치명적인 부분만이 분할되어야 하고, 그런
		성능에 치명적인 부분은 일반적으로 어플리케이션의 작은
		부분입니다.
	\item	캐시 미스는 개별적 레지스터간 인스트럭션들에 비하면 매우
		느리지만, TCP/IP 네트워킹과 같은 것들보다는 빠른 프로세스간
		통신 (inter-process-communication) 기능들에 비해서도 상당히
		빠릅니다.
	\item	공유 메모리 멀티프로세서들은 이미 시장에 나와 있고 상당히
		저렵하므로, 1990년대와는 정반대로, 공유 메모리 병렬성을
		사용하는데 비용 문제는 거의 없습니다.
	\end{enumerate}
	항상 말하듯이, 처리해야 하는 일에 걸맞는 도구를 사용하세요!
	\iffalse

	Indeed, multiple processes with separate address spaces can be
	an excellent way to exploit parallelism, as the proponents of
	the fork-join methodology and the Erlang language would be very
	quick to tell you.
	However, there are also some advantages to shared-memory parallelism:
	\begin{enumerate}
	\item	Only the most performance-critical portions of the
		application must be partitioned, and such portions
		are usually a small fraction of the application.
	\item	Although cache misses are quite slow compared to
		individual register-to-register instructions,
		they are typically considerably faster than
		inter-process-communication primitives, which in
		turn are considerably faster than things like
		TCP/IP networking.
	\item	Shared-memory multiprocessors are readily available
		and quite inexpensive, so, in stark contrast to the
		1990s, there is little cost penalty for use of
		shared-memory parallelism.
	\end{enumerate}
	As always, use the right tool for the job!
	\fi

\QuickQAC{cha:Partitioning and Synchronization Design}{Partitioning and Synchronization Design}
\QuickQ{}
	Is there a better solution to the Dining
	Philosophers Problem?
\QuickA{}

\begin{figure}[tb]
\begin{center}
\includegraphics[scale=.7]{SMPdesign/DiningPhilosopher5PEM}
\end{center}
\caption{Dining Philosophers Problem, Fully Partitioned}
\ContributedBy{Figure}{fig:SMPdesign:Dining Philosophers Problem, Fully Partitioned}{Kornilios Kourtis}
\end{figure}

	One such improved solution is shown in
	Figure~\ref{fig:SMPdesign:Dining Philosophers Problem, Fully Partitioned},
	where the philosophers are simply provided with an additional
	five forks.
	All five philosophers may now eat simultaneously, and there
	is never any need for philosophers to wait on one another.
	In addition, this approach offers greatly improved disease control.

	This solution might seem like cheating to some, but such
	``cheating'' is key to finding good solutions to many
	concurrency problems.

\QuickQ{}
	And in just what sense can this ``horizontal parallelism'' be
	said to be ``horizontal''?
\QuickA{}
	Inman was working with protocol stacks, which are normally
	depicted vertically, with the application on top and the
	hardware interconnect on the bottom.
	Data flows up and down this stack.
	``Horizontal parallelism'' processes packets from different network
	connections in parallel, while ``vertical parallelism''
	handles different protocol-processing steps for a given
	packet in parallel.

	``Vertical parallelism'' is also called ``pipelining''.

\QuickQ{}
	In this compound double-ended queue implementation, what should
	be done if the queue has become non-empty while releasing
	and reacquiring the lock?
\QuickA{}
	In this case, simply dequeue an item from the non-empty
	queue, release both locks, and return.

\QuickQ{}
	Is the hashed double-ended queue a good solution?
	Why or why not?
\QuickA{}
	The best way to answer this is to run \url{lockhdeq.c} on
	a number of different multiprocessor systems, and you are
	encouraged to do so in the strongest possible terms.
	One reason for concern is that each operation on this
	implementation must acquire not one but two locks.
	% Getting about 500 nanoseconds per element when used as
	% a queue on a 4.2GHz Power system.  This is roughly the same as
	% the version covered by a single lock.  Sequential (unlocked
	% variant is more than an order of magnitude faster!

	The first well-designed performance study will be cited.\footnote{
		The studies by Dalessandro
		et al.~\cite{LukeDalessandro:2011:ASPLOS:HybridNOrecSTM:deque}
		and Dice et al.~\cite{DavidDice:2010:SCA:HTM:deque} are
		good starting points.}
	Do not forget to compare to a sequential implementation!

\QuickQ{}
	Move \emph{all} the elements to the queue that became empty?
	In what possible universe is this brain-dead solution in any
	way optimal???
\QuickA{}
	It is optimal in the case where data flow switches direction only
	rarely.
	It would of course be an extremely poor choice if the double-ended
	queue was being emptied from both ends concurrently.
	This of course raises another question, namely, in what possible
	universe emptying from both ends concurrently would be a reasonable
	thing to do.
	Work-stealing queues are one possible answer to this question.

\QuickQ{}
	Why can't the compound parallel double-ended queue
	implementation be symmetric?
\QuickA{}
	The need to avoid deadlock by imposing a lock hierarchy
	forces the asymmetry, just as it does in the fork-numbering
	solution to the Dining Philosophers Problem
	(see Section~\ref{sec:SMPdesign:Dining Philosophers Problem}).

\QuickQ{}
	Why is it necessary to retry the right-dequeue operation
	on line~28 of
	Figure~\ref{fig:SMPdesign:Compound Parallel Double-Ended Queue Implementation}?
\QuickA{}
	This retry is necessary because some other thread might have
	enqueued an element between the time that this thread dropped
	\co{d->rlock} on line~25 and the time that it reacquired this
	same lock on line~27.

\QuickQ{}
	Surely the left-hand lock must \emph{sometimes} be available!!!
	So why is it necessary that line~25 of
	Figure~\ref{fig:SMPdesign:Compound Parallel Double-Ended Queue Implementation}
	unconditionally release the right-hand lock?
\QuickA{}
	It would be possible to use \co{spin_trylock()} to attempt
	to acquire the left-hand lock when it was available.
	However, the failure case would still need to drop the
	right-hand lock and then re-acquire the two locks in order.
	Making this transformation (and determining whether or not
	it is worthwhile) is left as an exercise for the reader.

\QuickQ{}
	Why are there not one but two solutions to the double-ended queue
	problem?
\QuickA{}
	There are actually at least three.
	The third, by Dominik Dingel, makes interesting use of
	reader-writer locking, and may be found in \co{lockrwdeq.c}.

\QuickQ{}
	The tandem double-ended queue runs about twice as fast as
	the hashed double-ended queue, even when I increase the
	size of the hash table to an insanely large number.
	Why is that?
\QuickA{}
	The hashed double-ended queue's locking design only permits
	one thread at a time at each end, and further requires
	two lock acquisitions for each operation.
	The tandem double-ended queue also permits one thread at a time
	at each end, and in the common case requires only one lock
	acquisition per operation.
	Therefore, the tandem double-ended queue should be expected to
	outperform the hashed double-ended queue.

	Can you created a double-ended queue that allows multiple
	concurrent operations at each end?
	If so, how?  If not, why not?

\QuickQ{}
	Is there a significantly better way of handling concurrency
	for double-ended queues?
\QuickA{}
	One approach is to transform the problem to be solved
	so that multiple double-ended queues can be used in parallel,
	allowing the simpler single-lock double-ended queue to be used,
	and perhaps also replace each double-ended queue with a pair of
	conventional single-ended queues.
	Without such ``horizontal scaling'', the speedup is limited
	to 2.0.
	In contrast, horizontal-scaling designs can achieve very large
	speedups, and are especially attractive if there are multiple threads
	working either end of the queue, because in this
	multiple-thread case the dequeue
	simply cannot provide strong ordering guarantees.
	After all, the fact that a given thread removed an item first
	in no way implies that it will process that item
	first~\cite{AndreasHaas2012FIFOisnt}.
	And if there are no guarantees, we may as well obtain the
	performance benefits that come with refusing to provide these
	guarantees.
	% about twice as fast as hashed version on 4.2GHz Power.

	Regardless of whether or not the problem can be transformed
	to use multiple queues, it is worth asking whether work can
	be batched so that each enqueue and dequeue operation corresponds
	to larger units of work.
	This batching approach decreases contention on the queue data
	structures, which increases both performance and scalability,
	as will be seen in
	Section~\ref{sec:SMPdesign:Synchronization Granularity}.
	After all, if you must incur high synchronization overheads,
	be sure you are getting your money's worth.

	Other researchers are working on other ways to take advantage
	of limited ordering guarantees in
	queues~\cite{ChristophMKirsch2012FIFOisntTR}.

\QuickQ{}
	Don't all these problems with critical sections mean that
	we should just always use
	non-blocking synchronization~\cite{MauriceHerlihy90a},
	which don't have critical sections?
\QuickA{}
	Although non-blocking synchronization can be very useful
	in some situations, it is no panacea.
	Also, non-blocking synchronization really does have
	critical sections, as noted by Josh Triplett.
	For example, in a non-blocking algorithm based on
	compare-and-swap operations, the code starting at the
	initial load and continuing to the compare-and-swap
	is in many ways analogous to a lock-based critical section.

\QuickQ{}
	What are some ways of preventing a structure from being freed while
	its lock is being acquired?
\QuickA{}
	Here are a few possible solutions to this \emph{existence guarantee}
	problem:

	\begin{enumerate}
	\item	Provide a statically allocated lock that is held while
		the per-structure lock is being acquired, which is an
		example of hierarchical locking (see
		Section~\ref{sec:SMPdesign:Hierarchical Locking}).
		Of course, using a single global lock for this purpose
		can result in unacceptably high levels of lock contention,
		dramatically reducing performance and scalability.
	\item	Provide an array of statically allocated locks, hashing
		the structure's address to select the lock to be acquired,
		as described in Chapter~\ref{chp:Locking}.
		Given a hash function of sufficiently high quality, this
		avoids the scalability limitations of the single global
		lock, but in read-mostly situations, the lock-acquisition
		overhead can result in unacceptably degraded performance.
	\item	Use a garbage collector, in software environments providing
		them, so that a structure cannot be deallocated while being
		referenced.
		This works very well, removing the existence-guarantee
		burden (and much else besides) from the developer's
		shoulders, but imposes the overhead of garbage collection
		on the program.
		Although garbage-collection technology has advanced
		considerably in the past few decades, its overhead
		may be unacceptably high for some applications.
		In addition, some applications require that the developer
		exercise more control over the layout and placement of
		data structures than is permitted by most garbage collected
		environments.
	\item	As a special case of a garbage collector, use a global
		reference counter, or a global array of reference counters.
	\item	Use \emph{hazard pointers}~\cite{MagedMichael04a}, which
		can be thought of as an inside-out reference count.
		Hazard-pointer-based algorithms maintain a per-thread list of
		pointers, so that the appearance of a given pointer on
		any of these lists acts as a reference to the corresponding
		structure.
		Hazard pointers are an interesting research direction, but
		have not yet seen much use in production (written in 2008).
	\item	Use transactional memory
		(TM)~\cite{Herlihy93a,DBLomet1977SIGSOFT,Shavit95},
		so that each reference and
		modification to the data structure in question is
		performed atomically.
		Although TM has engendered much excitement in recent years,
		and seems likely to be of some use in production software,
		developers should exercise some
		caution~\cite{Blundell2005DebunkTM,Blundell2006TMdeadlock,McKenney2007PLOSTM},
		particularly in performance-critical code.
		In particular, existence guarantees require that the
		transaction cover the full path from a global reference
		to the data elements being updated.
	\item	Use RCU, which can be thought of as an extremely lightweight
		approximation to a garbage collector.
		Updaters are not permitted to free RCU-protected
		data structures that RCU readers might still be referencing.
		RCU is most heavily used for read-mostly data structures,
		and is discussed at length in
		Chapter~\ref{chp:Deferred Processing}.
	\end{enumerate}

	For more on providing existence guarantees, see
	Chapters~\ref{chp:Locking} and \ref{chp:Deferred Processing}.

\QuickQ{}
	How can a single-threaded 64-by-64 matrix multiple possibly
	have an efficiency of less than 1.0?
	Shouldn't all of the traces in
	Figure~\ref{fig:SMPdesign:Matrix Multiply Efficiency}
	have efficiency of exactly 1.0 when running on only one thread?
\QuickA{}
	The \texttt{matmul.c} program creates the specified number of
	worker threads, so even the single-worker-thread case incurs
	thread-creation overhead.
	Making the changes required to optimize away thread-creation
	overhead in the single-worker-thread case is left as an
	exercise to the reader.

\QuickQ{}
	How are data-parallel techniques going to help with matrix
	multiply?
	It is \emph{already} data parallel!!!
\QuickA{}
	I am glad that you are paying attention!
	This example serves to show that although data parallelism can
	be a very good thing, it is not some magic wand that automatically
	wards off any and all sources of inefficiency.
	Linear scaling at full performance, even to ``only'' 64 threads,
	requires care at all phases of design and implementation.

	In particular, you need to pay careful attention to the
	size of the partitions.
	For example, if you split a 64-by-64 matrix multiply across
	64 threads, each thread gets only 64 floating-point multiplies.
	The cost of a floating-point multiply is miniscule compared to
	the overhead of thread creation.

	Moral: If you have a parallel program with variable input,
	always include a check for the input size being too small to
	be worth parallelizing.
	And when it is not helpful to parallelize, it is not helpful
	to incur the overhead required to spawn a thread, now is it?

\QuickQ{}
	In what situation would hierarchical locking work well?
\QuickA{}
	If the comparison on line~31 of
	Figure~\ref{fig:SMPdesign:Hierarchical-Locking Hash Table Search}
	were replaced by a much heavier-weight operation,
	then releasing {\tt bp->bucket\_lock} \emph{might} reduce lock
	contention enough to outweigh the overhead of the extra
	acquisition and release of {\tt cur->node\_lock}.

\QuickQ{}
	In Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
	there is a pattern of performance rising with increasing run
	length in groups of three samples, for example, for run lengths
	10, 11, and 12.
	Why?
\QuickA{}
	This is due to the per-CPU target value being three.
	A run length of 12 must acquire the global-pool lock twice,
	while a run length of 13 must acquire the global-pool lock
	three times.

\QuickQ{}
	Allocation failures were observed in the two-thread
	tests at run lengths of 19 and greater.
	Given the global-pool size of 40 and the per-thread target
	pool size $s$ of three, number of threads $n$ equal to two,
	and assuming that the per-thread pools are initially
	empty with none of the memory in use, what is the smallest allocation
	run length $m$ at which failures can occur?
	(Recall that each thread repeatedly allocates $m$ block of memory,
	and then frees the $m$ blocks of memory.)
	Alternatively, given $n$ threads each with pool size $s$, and
	where each thread repeatedly first allocates $m$ blocks of memory
	and then frees those $m$ blocks, how large must the global pool
	size be?
	\emph{Note:} Obtaining the correct answer will require you to
	examine the \co{smpalloc.c} source code, and very likely
	single-step it as well.
	You have been warned!
\QuickA{}
	This solution is adapted from one put forward by Alexey Roytman.
	It is based on the following definitions:

	\begin{description}
	\item[$g$]	Number of blocks globally available.
	\item[$i$]	Number of blocks left in the initializing thread's
			per-thread pool.  (This is one reason you needed
			to look at the code!)
	\item[$m$]	Allocation/free run length.
	\item[$n$]	Number of threads, excluding the initialization thread.
	\item[$p$]	Per-thread maximum block consumption, including
			both the blocks actually allocated and the blocks
			remaining in the per-thread pool.
	\end{description}

	The values $g$, $m$, and $n$ are given.  The value for $p$ is
	$m$ rounded up to the next multiple of $s$, as follows:

	\begin{equation}
		p = s \left \lfloor \frac{m + s - 1}{s} \right \rfloor
	\label{sec:SMPdesign:p}
	\end{equation}

	The value for $i$ is as follows:

	\begin{equation}
		i = \left \{
			\begin{array}{l}
				g \pmod{2 s} = 0: 2 s \\
				g \pmod{2 s} \ne 0: g \pmod{2 s}
			\end{array}
		    \right .
	\label{sec:SMPdesign:i}
	\end{equation}

	\begin{figure}[tb]
	\begin{center}
	\resizebox{3in}{!}{\includegraphics{SMPdesign/smpalloclim}}
	\end{center}
	\caption{Allocator Cache Run-Length Analysis}
	\label{fig:SMPdesign:Allocator Cache Run-Length Analysis}
	\end{figure}

	The relationships between these quantities is shown in
	Figure~\ref{fig:SMPdesign:Allocator Cache Run-Length Analysis}.
	The global pool is shown on the top of this figure, and
	the ``extra'' initializer thread's per-thread pool and
	per-thread allocations are the left-most pair of boxes.
	The initializer thread has no blocks allocated, but has
	$i$ blocks stranded in its per-thread pool.
	The rightmost two pairs of boxes are the per-thread pools and
	per-thread allocations of threads holding the maximum possible
	number of blocks, while the second-from-left pair of boxes
	represents the thread currently trying to allocate.

	The total number of blocks is $g$, and adding up the per-thread
	allocations and per-thread pools, we see that the global pool
	contains $g-i-p(n-1)$ blocks.
	If the allocating thread is to be successful, it needs at least
	$m$ blocks in the global pool, in other words:

	\begin{equation}
		g - i - p(n - 1) \ge m
	\label{sec:SMPdesign:g-vs-m}
	\end{equation}

	The question has $g=40$, $s=3$, and $n=2$.
	Equation~\ref{sec:SMPdesign:i} gives $i=4$, and
	Equation~\ref{sec:SMPdesign:p} gives $p=18$ for $m=18$
	and $p=21$ for $m=19$.
	Plugging these into Equation~\ref{sec:SMPdesign:g-vs-m}
	shows that $m=18$ will not overflow, but that $m=19$ might
	well do so.

	The presence of $i$ could be considered to be a bug.
	After all, why allocate memory only to have it stranded in
	the initialization thread's cache?
	One way of fixing this would be to provide a \co{memblock_flush()}
	function that flushed the current thread's pool into the
	global pool.
	The initialization thread could then invoke this function
	after freeing all of the blocks.

\QuickQAC{chp:Locking}{Locking}
\QuickQ{}
	Just how can serving as a whipping boy be considered to be
	in any way honorable???
\QuickA{}
	The reason locking serves as a research-paper whipping boy is
	because it is heavily used in practice.
	In contrast, if no one used or cared about locking, most research
	papers would not bother even mentioning it.

\QuickQ{}
	But the definition of deadlock only said that each thread
	was holding at least one lock and waiting on another lock
	that was held by some thread.
	How do you know that there is a cycle?
\QuickA{}
	Suppose that there is no cycle in the graph.
	We would then have a directed acyclic graph (DAG), which would
	have at least one leaf node.

	If this leaf node was a lock, then we would have a thread
	that was waiting on a lock that wasn't held by any thread,
	which violates the definition.
	(And in this case the thread would immediately acquire the
	lock.)

	On the other hand, if this leaf node was a thread, then
	we would have a thread that was not waiting on any lock,
	again violating the definition.
	(And in this case, the thread would either be running or
	be blocked on something that is not a lock.)

	Therefore, given this definition of deadlock, there must
	be a cycle in the corresponding graph.

\QuickQ{}
	Are there any exceptions to this rule, so that there really could be
	a deadlock cycle containing locks from both the library and
	the caller, even given that the library code never invokes
	any of the caller's functions?
\QuickA{}
	Indeed there are!
	Here are a few of them:
	\begin{enumerate}
	\item	If one of the library function's arguments is a pointer
		to a lock that this library function acquires, and if
		the library function holds one if its locks while
		acquiring the caller's lock, then we could have a
		deadlock cycle involving both caller and library locks.
	\item	If one of the library functions returns a pointer to
		a lock that is acquired by the caller, and if the
		caller acquires one if its locks while holding the
		library's lock, we could again have a deadlock
		cycle involving both caller and library locks.
	\item	If one of the library functions acquires a lock and
		then returns while still holding it, and if the caller
		acquires one of its locks, we have yet another way
		to create a deadlock cycle involving both caller
		and library locks.
	\item	If the caller has a signal handler that acquires
		locks, then the deadlock cycle can involve both
		caller and library locks.
		In this case, however, the library's locks are
		innocent bystanders in the deadlock cycle.
		That said, please note that acquiring a lock from
		within a signal handler is a no-no in most
		environments---it is not just a bad idea, it
		is unsupported.
	\end{enumerate}

\QuickQ{}
	But if \co{qsort()} releases all its locks before invoking
	the comparison function, how can it protect against races
	with other \co{qsort()} threads?
\QuickA{}
	By privatizing the data elements being compared
	(as discussed in Chapter~\ref{chp:Data Ownership})
	or through use of deferral mechanisms such as
	reference counting (as discussed in
	Chapter~\ref{chp:Deferred Processing}).

\QuickQ{}
	Name one common exception where it is perfectly reasonable
	to pass a pointer to a lock into a function.
\QuickA{}
	Locking primitives, of course!

\QuickQ{}
	Doesn't the fact that \co{pthread_cond_wait()} first releases the
	mutex and then re-acquires it eliminate the possibility of deadlock?
\QuickA{}
	Absolutely not!

	Consider the a program that acquires \co{mutex_a}, and then
	\co{mutex_b}, in that order, and then passes \co{mutex_a}
	to \co{pthread_cond_wait}.
	Now, \co{pthread_cond_wait} will release \co{mutex_a}, but
	will re-acquire it before returning.
	If some other thread acquires \co{mutex_a} in the meantime
	and then blocks on \co{mutex_b}, the program will deadlock.

\QuickQ{}
	Can the transformation from
	Figure~\ref{fig:locking:Protocol Layering and Deadlock} to
	Figure~\ref{fig:locking:Avoiding Deadlock Via Conditional Locking}
	be applied universally?
\QuickA{}
	Absolutely not!

	This transformation assumes that the
	\co{layer_2_processing()} function is idempotent, given that
	it might be executed multiple times on the same packet when
	the \co{layer_1()} routing decision changes.
	Therefore, in real life, this transformation can become
	arbitrarily complex.

\QuickQ{}
	But the complexity in
	Figure~\ref{fig:locking:Avoiding Deadlock Via Conditional Locking}
	is well worthwhile given that it avoids deadlock, right?
\QuickA{}
	Maybe.

	If the routing decision in \co{layer_1()} changes often enough,
	the code will always retry, never making forward progress.
	This is termed ``livelock'' if no thread makes any forward progress or
	``starvation''
	if some threads make forward progress but other do not
	(see Section~\ref{sec:locking:Livelock and Starvation}).

\QuickQ{}
	When using the ``acquire needed locks first'' approach described in
	Section~\ref{sec:locking:Acquire Needed Locks First},
	how can livelock be avoided?
\QuickA{}
	Provide an additional global lock.
	If a given thread has repeatedly tried and failed to acquire the needed
	locks, then have that thread unconditionally acquire the new
	global lock, and then unconditionally acquire any needed locks.
	(Suggested by Doug Lea.)

\QuickQ{}
	Why is it illegal to acquire a Lock~A that is acquired outside
	of a signal handler without blocking signals while holding
	a Lock~B that is acquired within a signal handler?
\QuickA{}
	Because this would lead to deadlock.
	Given that Lock~A is held outside of a signal
	handler without blocking signals, a signal might be handled while
	holding this lock.
	The corresponding signal handler might then acquire
	Lock~B, so that Lock~B is acquired while holding Lock~A.
	Therefore, if we also acquire Lock~A while holding Lock~B
	as called out in the question, we will have a deadlock cycle.

	Therefore, it is illegal to acquire a lock that is acquired outside
	of a signal handler without blocking signals while holding
	a another lock that is acquired within a signal handler.

\QuickQ{}
	How can you legally block signals within a signal handler?
\QuickA{}
	One of the simplest and fastest ways to do so is to use
	the \co{sa_mask} field of the \co{struct sigaction} that
	you pass to \co{sigaction()} when setting up the signal.

\QuickQ{}
	If acquiring locks in signal handlers is such a bad idea, why
	even discuss ways of making it safe?
\QuickA{}
	Because these same rules apply to the interrupt handlers used in
	operating-system kernels and in some embedded applications.

	In many application environments, acquiring locks in signal
	handlers is frowned upon~\cite{OpenGroup1997pthreads}.
	However, that does not stop clever developers from (usually
	unwisely) fashioning home-brew locks out of atomic operations.
	And atomic operations are in many cases perfectly legal in
	signal handlers.

\QuickQ{}
	Given an object-oriented application that passes control freely
	among a group of objects such that there is no straightforward
	locking hierarchy,\footnote{
		Also known as ``object-oriented spaghetti code.''}
	layered or otherwise, how can this
	application be parallelized?
\QuickA{}
	There are a number of approaches:
	\begin{enumerate}
	\item	In the case of parametric search via simulation,
		where a large number of simulations will be run
		in order to converge on (for example) a good design
		for a mechanical or electrical device, leave the
		simulation single-threaded, but run many instances
		of the simulation in parallel.
		This retains the object-oriented design, and
		gains parallelism at a higher level, and likely
		also avoids synchronization overhead.
	\item	Partition the objects into groups such that there
		is no need to operate on objects in
		more than one group at a given time.
		Then associate a lock with each group.
		This is an example of a single-lock-at-a-time
		design, which discussed in
		Section~\ref{sec:locking:Single-Lock-at-a-Time Designs}.
	\item	Partition the objects into groups such that threads
		can all operate on objects in the groups in some
		groupwise ordering.
		Then associate a lock with each group, and impose a
		locking hierarchy over the groups.
	\item	Impose an arbitrarily selected hierarchy on the locks,
		and then use conditional locking if it is necessary
		to acquire a lock out of order, as was discussed in
		Section~\ref{sec:locking:Conditional Locking}.
	\item	Before carrying out a given group of operations, predict
		which locks will be acquired, and attempt to acquire them
		before actually carrying out any updates.
		If the prediction turns out to be incorrect, drop
		all the locks and retry with an updated prediction
		that includes the benefit of experience.
		This approach was discussed in
		Section~\ref{sec:locking:Acquire Needed Locks First}.
	\item	Use transactional memory.
		This approach has a number of advantages and disadvantages
		which will be discussed in
		Section~\ref{sec:future:Transactional Memory}.
	\item	Refactor the application to be more concurrency-friendly.
		This would likely also have the side effect of making
		the application run faster even when single-threaded, but might
		also make it more difficult to modify the application.
	\item	Use techniques from later chapters in addition to locking.
	\end{enumerate}

\QuickQ{}
	How can the livelock shown in
	Figure~\ref{fig:locking:Abusing Conditional Locking}
	be avoided?
\QuickA{}
	Figure~\ref{fig:locking:Avoiding Deadlock Via Conditional Locking}
	provides some good hints.
	In many cases, livelocks are a hint that you should revisit your
	locking design.
	Or visit it in the first place if your locking design
	``just grew''.

	That said, one good-and-sufficient approach due to Doug Lea
	is to use conditional locking as described in
	Section~\ref{sec:locking:Conditional Locking}, but combine this
	with acquiring all needed locks first, before modifying shared
	data, as described in
	Section~\ref{sec:locking:Acquire Needed Locks First}.
	If a given critical section retries too many times,
	unconditionally acquire
	a global lock, then unconditionally acquire all the needed locks.
	This avoids both deadlock and livelock, and scales reasonably
	assuming that the global lock need not be acquired too often.

\QuickQ{}
	What problems can you spot in the code in
	Figure~\ref{fig:locking:Conditional Locking and Exponential Backoff}?
\QuickA{}
	Here are a couple:
	\begin{enumerate}
	\item	A one-second wait is way too long for most uses.
		Wait intervals should begin with roughly the time
		required to execute the critical section, which will
		normally be in the microsecond or millisecond range.
	\item	The code does not check for overflow.
		On the other hand, this bug is nullified
		by the previous bug: 32 bits worth of seconds is
		more than 50 years.
	\end{enumerate}

\QuickQ{}
	Wouldn't it be better just to use a good parallel design
	so that lock contention was low enough to avoid unfairness?
\QuickA{}
	It would be better in some sense, but there are situations
	where it can be appropriate to use
	designs that sometimes result in high lock contentions.

	For example, imagine a system that is subject to a rare error
	condition.
	It might well be best to have a simple error-handling design
	that has poor performance and scalability for the duration of
	the rare error condition, as opposed to a complex and
	difficult-to-debug design that is helpful only when one of
	those rare error conditions is in effect.

	That said, it is usually worth putting some effort into
	attempting to produce a design that both simple as well as
	efficient during error conditions, for example by partitioning
	the problem.

\QuickQ{}
	How might the lock holder be interfered with?
\QuickA{}
	If the data protected by the lock is in the same cache line
	as the lock itself, then attempts by other CPUs to acquire
	the lock will result in expensive cache misses on the part
	of the CPU holding the lock.
	This is a special case of false sharing, which can also occur
	if a pair of variables protected by different locks happen
	to share a cache line.
	In contrast, if the lock is in a different cache line than
	the data that it protects, the CPU holding the lock will
	usually suffer a cache miss only on first access to a given
	variable.

	Of course, the downside of placing the lock and data into separate
	cache lines is that the code will incur two cache misses rather
	than only one in the uncontended case.

\QuickQ{}
	Does it ever make sense to have an exclusive lock acquisition
	immediately followed by a release of that same lock, that is,
	an empty critical section?
\QuickA{}
	This usage is rare, but is occasionally used.
	The point is that the semantics of exclusive locks have two
	components: (1)~the familiar data-protection semantic and
	(2)~a messaging semantic, where releasing a given lock notifies
	a waiting acquisition of that same lock.
	An empty critical section uses the messaging component without
	the data-protection component.

	The rest of this answer provides some example uses of empty
	critical sections, however, these examples should be considered
	``gray magic.''\footnote{
		Thanks to Alexey Roytman for this decription.}
	As such, empty critical sections are almost never used in practice.
	Nevertheless, pressing on into this gray area...

	One historical use of empty critical sections appeared in the
	networking stack of the 2.4 Linux kernel.
	This usage pattern can be thought of as a way of approximating
	the effects of read-copy update (RCU), which is discussed in
	Section~\ref{sec:defer:Read-Copy Update (RCU)}.

	The empty-lock-critical-section idiom can also be used to
	reduce lock contention in some situations.
	For example, consider a multithreaded user-space application where
	each thread processes unit of work maintained in a per-thread
	list, where thread are prohibited from touching each others'
	lists.
	There could also be updates that require that all previously
	scheduled units of work have completed before the update can
	progress.
	One way to handle this is to schedule a unit of work on each
	thread, so that when all of these units of work complete, the
	update may proceed.

	In some applications, threads can come and go.
	For example, each thread might correspond to one user of the
	application, and thus be removed when that user logs out or
	otherwise disconnects.
	In many applications, threads cannot depart atomically: They must
	instead explicitly unravel themselves from various portions of
	the application using a specific sequence of actions.
	One specific action will be refusing to accept further requests
	from other threads, and another specific action will be disposing
	of any remaining units of work on its list, for example, by
	placing these units of work in a global work-item-disposal list
	to be taken by one of the remaining threads.
	(Why not just drain the thread's work-item list by executing
	each item?
	Because a given work item might generate more work items, so
	that the list could not be drained in a timely fashion.)

	If the application is to perform and scale well, a good locking
	design is required.
	One common solution is to have a global lock (call it \co{G})
	protecting the entire
	process of departing (and perhaps other things as well),
	with finer-grained locks protecting the
	individual unraveling operations.

	Now, a departing thread must clearly refuse to accept further
	requests before disposing of the work on its list, because
	otherwise additional work might arrive after the disposal action,
	which would render that disposal action ineffective.
	So simplified pseudocode for a departing thread might be as follows:

	\begin{enumerate}
	\item	Acquire lock \co{G}.
	\item	Acquire the lock guarding communications.
	\item	Refuse further communications from other threads.
	\item	Release the lock guarding communications.
	\item	Acquire the lock guarding the global work-item-disposal list.
	\item	Move all pending work items to the global
		work-item-disposal list.
	\item	Release the lock guarding the global work-item-disposal list.
	\item	Release lock \co{G}.
	\end{enumerate}

	Of course, a thread that needs to wait for all pre-existing work
	items will need to take departing threads into account.
	To see this, suppose that this thread starts waiting for all
	pre-existing work items just after a departing thread has refused
	further communications from other threads.
	How can this thread wait for the departing thread's work items
	to complete, keeping in mind that threads are not allowed to
	access each others' lists of work items?

	One straightforward approach is for this thread to acquire \co{G}
	and then the lock guarding the global work-item-disposal list, then
	move the work items to its own list.
	The thread then release both locks,
	places a work item on the end of it own list,
	and then wait for all of the work items that it placed on each thread's
	list (including its own) to complete.

	This approach does work well in many cases, but if special
	processing is required for each work item as it is pulled in
	from the global work-item-disposal list, the result could be
	excessive contention on \co{G}.
	One way to avoid that contention is to acquire \co{G} and then
	immediately release it.
	Then the process of waiting for all prior work items look
	something like the following:

	\begin{enumerate}
	\item	Set a global counter to one and initialize a condition
		variable to zero.
	\item	Send a message to all threads to cause them to atomically
		increment the global counter, and then to enqueue a
		work item.
		The work item will atomically decrement the global
		counter, and if the result is zero, it will set a
		condition variable to one.
	\item	Acquire \co{G}, which will wait on any currently departing
		thread to finish departing.
		Because only one thread may depart at a time, all the
		remaining threads will have already received the message
		sent in the preceding step.
	\item	Release \co{G}.
	\item	Acquire the lock guarding the global work-item-disposal list.
	\item	Move all work items from the global work-item-disposal list
		to this thread's list, processing them as needed along the way.
	\item	Release the lock guarding the global work-item-disposal list.
	\item	Enqueue an additional work item onto this thread's list.
		(As before, this work item will atomically decrement
		the global counter, and if the result is zero, it will
		set a condition variable to one.)
	\item	Wait for the condition variable to take on the value one.
	\end{enumerate}

	Once this procedure completes, all pre-existing work items are
	guaranteed to have completed.
	The empty critical sections are using locking for messaging as
	well as for protection of data.

\QuickQ{}
	Is there any other way for the VAX/VMS DLM to emulate
	a reader-writer lock?
\QuickA{}
	There are in fact several.
	One way would be to use the null, protected-read, and exclusive
	modes.
	Another way would be to use the null, protected-read, and
	concurrent-write modes.
	A third way would be to use the null, concurrent-read, and
	exclusive modes.

\QuickQ{}
	The code in
	Figure~\ref{fig:locking:Conditional Locking to Reduce Contention}
	is ridiculously complicated!
	Why not conditionally acquire a single global lock?
\QuickA{}
	Conditionally acquiring a single global lock does work very well,
	but only for relatively small numbers of CPUs.
	To see why it is problematic in systems with many hundreds of
	CPUs, look at
	Figure~\ref{fig:count:Atomic Increment Scalability on Nehalem}
	and extrapolate the delay from eight to 1,000 CPUs.

\QuickQ{}
	Wait a minute!
	If we ``win'' the tournament on line~16 of
	Figure~\ref{fig:locking:Conditional Locking to Reduce Contention},
	we get to do all the work of \co{do_force_quiescent_state()}.
	Exactly how is that a win, really?
\QuickA{}
	How indeed?
	This just shows that in concurrency, just as in life, one
	should take care to learn exactly what winning entails before
	playing the game.

\QuickQ{}
	Why not rely on the C language's default initialization of
	zero instead of using the explicit initializer shown on
	line~2 of
	Figure~\ref{fig:locking:Sample Lock Based on Atomic Exchange}?
\QuickA{}
	Because this default initialization does not apply to locks
	allocated as auto variables within the scope of a function.

\QuickQ{}
	Why bother with the inner loop on lines~7-8 of
	Figure~\ref{fig:locking:Sample Lock Based on Atomic Exchange}?
	Why not simply repeatedly do the atomic exchange operation
	on line~6?
\QuickA{}
	Suppose that the lock is held and that several threads
	are attempting to acquire the lock.
	In this situation, if these threads all loop on the atomic
	exchange operation, they will ping-pong the cache line
	containing the lock among themselves, imposing load
	on the interconnect.
	In contrast, if these threads are spinning in the inner
	loop on lines~7-8, they will each spin within their own
	caches, putting negligible load on the interconnect.

\QuickQ{}
	Why not simply store zero into the lock word on line 14 of
	Figure~\ref{fig:locking:Sample Lock Based on Atomic Exchange}?
\QuickA{}
	This can be a legitimate implementation, but only if
	this store is preceded by a memory barrier and makes use
	of \co{ACCESS_ONCE()}.
	The memory barrier is not required when the \co{xchg()}
	operation is used because this operation implies a
	full memory barrier due to the fact that it returns
	a value.

\QuickQ{}
	How can you tell if one counter is greater than another,
	while accounting for counter wrap?
\QuickA{}
	In the C language, the following macro correctly handles this:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
#define ULONG_CMP_LT(a, b) \
        (ULONG_MAX / 2 < (a) - (b))
\end{verbatim}
\end{minipage}
\vspace{5pt}

	Although it is tempting to simply subtract two signed integers,
	this should be avoided because signed overflow is undefined
	in the C language.
	For example, if the compiler knows that one of the values is
	positive and the other negative, it is within its rights to
	simply assume that the positive number is greater than the
	negative number, even though subtracting the negative number
	from the positive number might well result in overflow and
	thus a negative number.

	How could the compiler know the signs of the two numbers?
	It might be able to deduce it based on prior assignments
	and comparisons.
	In this case, if the per-CPU counters were signed, the compiler
	could deduce that they were always increasing in value, and
	then might assume that they would never go negative.
	This assumption could well lead the compiler to generate
	unfortunate code~\cite{PaulEMcKenney2012SignedOverflow,JohnRegehr2010UndefinedBehavior}.

\QuickQ{}
	Which is better, the counter approach or the flag approach?
\QuickA{}
	The flag approach will normally suffer fewer cache misses,
	but a better answer is to try both and see which works best
	for your particular workload.

\QuickQ{}
	How can relying on implicit existence guarantees result in
	a bug?
\QuickA{}
	Here are some bugs resulting from improper use of implicit
	existence guarantees:
	\begin{enumerate}
	\item	A program writes the address of a global variable to
		a file, then a later instance of that same program
		reads that address and attempts to dereference it.
		This can fail due to address-space randomization,
		to say nothing of recompilation of the program.
	\item	A module can record the address of one of its variables
		in a pointer located in some other module, then attempt
		to dereference that pointer after the module has
		been unloaded.
	\item	A function can record the address of one of its on-stack
		variables into a global pointer, which some other
		function might attempt to dereference after that function
		has returned.
	\end{enumerate}
	I am sure that you can come up with additional possibilities.

\QuickQ{}
	What if the element we need to delete is not the first element
	of the list on line~8 of
	Figure~\ref{fig:locking:Per-Element Locking Without Existence Guarantees}?
\QuickA{}
	This is a very simple hash table with no chaining, so the only
	element in a given bucket is the first element.
	The reader is invited to adapt this example to a hash table with
	full chaining.

\QuickQ{}
	What race condition can occur in
	Figure~\ref{fig:locking:Per-Element Locking Without Existence Guarantees}?
\QuickA{}
	Consider the following sequence of events:
	\begin{enumerate}
	\item	Thread~0 invokes \co{delete(0)}, and reaches line~10 of
		the figure, acquiring the lock.
	\item	Thread~1 concurrently invokes \co{delete(0)}, reaching
		line~10, but spins on the lock because Thread~0 holds it.
	\item	Thread~0 executes lines~11-14, removing the element from
		the hashtable, releasing the lock, and then freeing the
		element.
	\item	Thread~0 continues execution, and allocates memory, getting
		the exact block of memory that it just freed.
	\item	Thread~0 then initializes this block of memory as some
		other type of structure.
	\item	Thread~1's \co{spin_lock()} operation fails due to the
		fact that what it believes to be \co{p->lock} is no longer
		a spinlock.
	\end{enumerate}
	Because there is no existence guarantee, the identity of the
	data element can change while a thread is attempting to acquire
	that element's lock on line~10!

\QuickQAC{chp:Data Ownership}{Data Ownership}
\QuickQ{}
	What form of data ownership is extremely difficult
	to avoid when creating shared-memory parallel programs
	(for example, using pthreads) in C or C++?
\QuickA{}
	Use of auto variables in functions.
	By default, these are private to the thread executing the
	current function.

\QuickQ{}
	What synchronization remains in the example shown in
	Section~\ref{sec:owned:Multiple Processes}?
\QuickA{}
	The creation of the threads via the \co{sh} \co{&} operator
	and the joining of thread via the \co{sh} \co{wait}
	command.

	Of course, if the processes explicitly share memory, for
	example, using the \co{shmget()} or \co{mmap()} system
	calls, explicit synchronization might well be needed when
	acccessing or updating the shared memory.
	The processes might also synchronize using any of the following
	interprocess communications mechanisms:
	\begin{enumerate}
	\item	System V semaphores.
	\item	System V message queues.
	\item	UNIX-domain sockets.
	\item	Networking protocols, including TCP/IP, UDP, and a whole
		host of others.
	\item	File locking.
	\item	Use of the \co{open()} system call with the
		\co{O_CREAT} and \co{O_EXCL} flags.
	\item	Use of the \co{rename()} system call.
	\end{enumerate}
	A complete list of possible synchronization mechanisms is left
	as an exercise to the reader, who is warned that it will be
	an extremely long list.
	A surprising number of unassuming system calls can be pressed
	into service as synchronization mechanisms.

\QuickQ{}
	Is there any shared data in the example shown in
	Section~\ref{sec:owned:Multiple Processes}?
\QuickA{}
	That is a philosophical question.

	Those wishing the answer ``no'' might argue that processes by
	definition do not share memory.

	Those wishing to answer ``yes'' might list a large number of
	synchronization mechanisms that do not require shared memory,
	note that the kernel will have some shared state, and perhaps
	even argue that the assignment of process IDs (PIDs) constitute
	shared data.

	Such arguments are excellent intellectual exercise, and are
	also a wonderful way of feeling intelligent, scoring points
	against hapless classmates or colleagues,
	and (especially!) avoiding getting anything useful done.

\QuickQ{}
	Does it ever make sense to have partial data ownership where
	each thread reads only its own instance of a per-thread variable,
	but writes to other threads' instances?
\QuickA{}
	Amazingly enough, yes.
	One example is a simple message-passing system where threads post
	messages to other threads' mailboxes, and where each thread
	is responsible for removing any message it sent once that message
	has been acted on.
	Implementation of such an algorithm is left as an exercise for the
	reader, as is the task of identifying other algorithms with
	similar ownership patterns.

\QuickQ{}
	What mechanisms other than POSIX signals may be used for function
	shipping?
\QuickA{}
	There is a very large number of such mechanisms, including:
	\begin{enumerate}
	\item	System V message queues.
	\item	Shared-memory dequeue (see
		Section~\ref{sec:SMPdesign:Double-Ended Queue}).
	\item	Shared-memory mailboxes.
	\item	UNIX-domain sockets.
	\item	TCP/IP or UDP, possibly augmented by any number of
		higher-level protocols, including RPC, HTTP,
		XML, SOAP, and so on.
	\end{enumerate}
	Compilation of a complete list is left as an exercise to
	sufficiently single-minded readers, who are warned that the
	list will be extremely long.

\QuickQ{}
	But none of the data in the \co{eventual()} function shown on
	lines~15-32 of
	Figure~\ref{fig:count:Array-Based Per-Thread Eventually Consistent Counters}
	is actually owned by the \co{eventual()} thread!
	In just what way is this data ownership???
\QuickA{}
	The key phrase is ``owns the rights to the data''.
	In this case, the rights in question are the rights to access
	the per-thread \co{counter} variable defined on line~1
	of the figure.
	This situation is similar to that described in
	Section~\ref{sec:owned:Partial Data Ownership and pthreads}.

	However, there really is data that is owned by the \co{eventual()}
	thread, namely the \co{t} and \co{sum} variables defined on
	lines~17 and~18 of the figure.

	For other examples of designated threads, look at the kernel
	threads in the Linux kernel, for example, those created by
	\co{kthread_create()} and \co{kthread_run()}.

\QuickQ{}
	Is it possible to obtain greater accuracy while still
	maintaining full privacy of the per-thread data?
\QuickA{}
	Yes.
	One approach is for \co{read_count()} to add the value
	of its own per-thread variable.
	This maintains full ownership and performance, but only
	a slight improvement in accuracy, particulary on systems
	with very large numbers of threads.

	Another approach is for \co{read_count()} to use function
	shipping, for example, in the form of per-thread signals.
	This greatly improves accuracy, but at a significant performance
	cost for \co{read_count()}.

	However, both of these methods have the advantage of eliminating
	cache-line bouncing for the common case of updating counters.

\QuickQAC{chp:Deferred Processing}{Deferred Processing}
\QuickQ{}
	Why not implement reference-acquisition using
	a simple compare-and-swap operation that only
	acquires a reference if the reference counter is
	non-zero?
\QuickA{}
	Although this can resolve the race between the release of
	the last reference and acquisition of a new reference,
	it does absolutely nothing to prevent the data structure
	from being freed and reallocated, possibly as some completely
	different type of structure.
	It is quite likely that the ``simple compare-and-swap
	operation'' would give undefined results if applied to the
	differently typed structure.

	In short, use of atomic operations such as compare-and-swap
	absolutely requires either type-safety or existence guarantees.

\QuickQ{}
	Why isn't it necessary to guard against cases where one CPU
	acquires a reference just after another CPU releases the last
	reference?
\QuickA{}
	Because a CPU must already hold a reference in order
	to legally acquire another reference.
	Therefore, if one CPU releases the last reference,
	there cannot possibly be any CPU that is permitted
	to acquire a new reference.
	This same fact allows the non-atomic check in line~22
	of Figure~\ref{fig:defer:Linux Kernel kref API}.

\QuickQ{}
	Suppose that just after the \co{atomic_sub_and_test()}
	on line~22 of
	Figure~\ref{fig:defer:Linux Kernel kref API} is invoked,
	that some other CPU invokes \co{kref_get()}.
	Doesn't this result in that other CPU now having an illegal
	reference to a released object?
\QuickA{}
	This cannot happen if these functions are used correctly.
	It is illegal to invoke \co{kref_get()} unless you already
	hold a reference, in which case the \co{kref_sub()} could
	not possibly have decremented the counter to zero.

\QuickQ{}
	Suppose that \co{kref_sub()} returns zero, indicating that
	the \co{release()} function was not invoked.
	Under what conditions can the caller rely on the continued
	existence of the enclosing object?
\QuickA{}
	The caller cannot rely on the continued existence of the
	object unless it knows that at least one reference will
	continue to exist.
	Normally, the caller will have no way of knowing this, and
	must therefore carefullly avoid referencing the object after
	the call to \co{kref_sub()}.

\QuickQ{}
	Why not just pass \co{kfree()} as the release function?
\QuickA{}
	Because the \co{kref} structure normally is embedded in
	a larger structure, and it is necessary to free the entire
	structure, not just the \co{kref} field.
	This is normally accomplished by defining a wrapper function
	that does a \co{container_of()} and then a \co{kfree()}.

\QuickQ{}
	Why can't the check for a zero reference count be
	made in a simple ``if'' statement with an atomic
	increment in its ``then'' clause?
\QuickA{}
	Suppose that the ``if'' condition completed, finding
	the reference counter value equal to one.
	Suppose that a release operation executes, decrementing
	the reference counter to zero and therefore starting
	cleanup operations.
	But now the ``then'' clause can increment the counter
	back to a value of one, allowing the object to be
	used after it has been cleaned up.

\QuickQ{}
	Why does \co{hp_store()} in
	Figure~\ref{fig:defer:Hazard-Pointer Storage and Erasure}
	take a double indirection to the data element?
	Why not \co{void *} instead of \co{void **}?
\QuickA{}
	Because \co{hp_record()} must check for concurrent modifications.
	To do that job, it needs a pointer to a pointer to the element,
	so that it can check for a modification to the pointer to the
	element.

\QuickQ{}
	Why does \co{hp_store()}'s caller need to restart its
	traversal from the beginning in case of failure?
	Isn't that inefficient for large data structures?
\QuickA{}
	It might be inefficient in some sense, but the fact is that
	such restarting is absolutely required for correctness.
	To see this, consider a hazard-pointer-protected linked list
	containing elements~A, B, and~C that is subjecte to the
	following sequence of events:

	\begin{enumerate}
	\item	Thread~0 stores a hazard pointer to element~B
		(having presumably traversed to element~B from element~A).
	\item	Thread~1 removes element~B from the list, which sets
		the pointer from element~B to element~C to a special
		\co{HAZPTR_POISON} value in order to mark the deletion.
		Because Thread~0 has a hazard pointer to element~B,
		it cannot yet be freed.
	\item	Thread~1 removes element~C from the list.
		Because there are no hazard pointers referencing element~C,
		it is immediately freed.
	\item	Thread~0 attempts to acquire a hazard pointer to
		now-removed element~B's successor, but sees the
		\co{HAZPTR_POISON} value, and thus returns zero,
		forcing the caller to restart its traversal from the
		beginning of the list.
	\end{enumerate}

	Which is a very good thing, because otherwise Thread~0 would
	have attempted to access the now-freed element~C,
	which might have resulted in arbitrarily horrible
	memory corruption, especially if the memory for
	element~C had since been re-allocated for some other
	purpose.

\QuickQ{}
	Given that papers on hazard pointers use the bottom bits
	of each pointer to mark deleted elements, what is up with
	\co{HAZPTR_POISON}?
\QuickA{}
	The published implementations of hazard pointers used
	non-blocking synchornization techniques for insertion
	and deletion.
	These techniques require that readers traversing the
	data structure ``help'' updaters complete their updates,
	which in turn means that readers need to look at the successor
	of a deleted element.

	In contrast, we will be using locking to synchronize updates,
	which does away with the need for readers to help updaters
	complete their updates, which in turn allows us to leave
	pointers' bottom bits alone.
	This approach allows read-side code to be simpler and faster.

\QuickQ{}
	But don't these restrictions on hazard pointers also apply
	to other forms of reference counting?
\QuickA{}
	These restrictions apply only to reference-counting mechanisms whose
	reference acquisition can fail.

\QuickQ{}
	An \co{atomic_read()} and an \co{atomic_set()} that are
	non-atomic?
	Is this some kind of bad joke???
\QuickA{}
	It might well seem that way, but in situations where no other
	CPU has access to the atomic variable in question, the overhead
	of an actual atomic instruction would be wasteful.
	Two examples where no other CPU has access are
	during initialization and cleanup.

\QuickQ{}
	But hazard pointers don't write to the data structure!
\QuickA{}
	Indeed, they do not.
	However, they do write to the hazard pointers themselves,
	and, more important, require that possible failures be
	handled for all \co{hp_store()} calls, each of which
	might fail.
	Therefore, although hazard pointers are extremely useful,
	it is still worth looking for improved mechanisms.

\QuickQ{}
	Why isn't this sequence-lock discussion in Chapter~\ref{chp:Locking},
	you know, the one on \emph{locking}?
\QuickA{}
	The sequence-lock mechanism is really a combination of two
	separate synchronization mechanisms, sequence counts and
	locking.
	In fact, the sequence-count mechanism is available separately
	in the Linux kernel via the
	\co{write_seqcount_begin()} and \co{write_seqcount_end()}
	primitives.

	However, the combined \co{write_seqlock()} and
	\co{write_sequnlock()} primitives are used much more heavily
	in the Linux kernel.
	More importantly, many more people will understand what you
	mean if you say ``sequence lock'' than if you say
	``sequence count''.

	So this section is entitled ``Sequence Locks'' so that people
	will understand what it is about just from the title, and
	it appears in the ``Deferred Processing'' because (1) of the
	emphasis on the ``sequence count'' aspect of ``sequence locks''
	and (2) because a ``sequence lock'' is much more than merely
	a lock.

\QuickQ{}
	Can you use sequence locks as the only synchronization
	mechanism protecting a linked list supporting concurrent
	addition, deletion, and search?
\QuickA{}
	One trivial way of accomplishing this is to surround all
	accesses, including the read-only accesses, with
	\co{write_seqlock()} and \co{write_sequnlock()}.
	Of course, this solution also prohibits all read-side
	parallelism, and furthermore could just as easily be implemented
	using simple locking.

	If you do come up with a solution that uses \co{read_seqbegin()}
	and \co{read_seqretry()} to protect read-side accesses, make
	sure that you correctly handle the following sequence of events:

	\begin{enumerate}
	\item	CPU~0 is traversing the linked list, and picks up a pointer
		to list element~A.
	\item	CPU~1 removes element~A from the list and frees it.
	\item	CPU~2 allocates an unrelated data structure, and gets
		the memory formerly occupied by element~A.
		In this unrelated data structure, the memory previously
		used for element~A's \co{->next} pointer is now occupied
		by a floating-point number.
	\item	CPU~0 picks up what used to be element~A's \co{->next}
		pointer, gets random bits, and therefore gets a
		segmentation fault.
	\end{enumerate}

	One way to protect against this sort of problem requires use
	of ``type-safe memory'', which will be discussed in
	Section~\ref{sec:defer:RCU is a Way of Providing Type-Safe Memory}.
	But in that case, you would be using some other synchronization
	mechanism in addition to sequence locks!

\QuickQ{}
	Why bother with the check on line~19 of
	\co{read_seqbegin()} in
	Figure~\ref{fig:defer:Sequence-Locking Implementation}?
	Given that a new writer could begin at any time, why not
	simply incorporate the check into line~31 of
	\co{read_seqretry()}?
\QuickA{}
	That would be a legitimate implementation.
	However, it would not save anything to move the check down
	to \co{read_seqretry()}: There would be roughly the same number
	of instructions.
	Furthermore, the reader's accesses from its doomed read-side
	critical section could inflict overhead on the writer in
	the form of cache misses.
	We can avoid these cache misses by placing the check in
	\co{read_seqbegin()} as shown on line~19 of
	Figure~\ref{fig:defer:Sequence-Locking Implementation}.

\QuickQ{}
	Why is the \co{smp_mb()} on line~29 of
	Figure~\ref{fig:defer:Sequence-Locking Implementation}
	needed?
\QuickA{}
	If it was omitted, both the compiler and the CPU would be
	within their rights to move the critical section preceding
	the call to \co{read_seqretry()} down below this function.
	This would prevent the sequence lock from protecting the
	critical section.
	The \co{smp_mb()} primitive prevents such reordering.

\QuickQ{}
	Can't weaker memory barriers be used in the code in
	Figure~\ref{fig:defer:Sequence-Locking Implementation}?
\QuickA{}
	In older versions of the Linux kernel, no.

	In very new versions of the Linux kernel, line~17 could use
	\co{smp_load_acquire()} instead of \co{ACCESS_ONCE()}, which
	in turn would allow the \co{smp_mb()} on line~18 to be dropped.
	Similarly, line~44 could use an \co{smp_store_release()}, for
	example, as follows: \\
	\co{smp_store_release(&slp->seq, ACCESS_ONCE(slp->seq) + 1);} \\
	This would allow the \co{smp_mb()} on line~43 to be dropped.

\QuickQ{}
	What prevents sequence-locking updaters from starving readers?
\QuickA{}
	Nothing.
	This is one of the weaknesses of sequence locking, and as a
	result, you should use sequence locking only in read-mostly
	situations.
	Unless of course read-side starvation is acceptable in your
	situation, in which case, go wild with the sequence-locking updates!

\QuickQ{}
	What if something else serializes writers, so that the lock
	is not needed?
\QuickA{}
	In this case, the \co{->lock} field could be omitted, as it
	is in \co{seqcount_t} in the Linux kernel.

\QuickQ{}
	Why isn't \co{seq} on line 2 of
	Figure~\ref{fig:defer:Sequence-Locking Implementation}
	\co{unsigned} rather than \co{unsigned long}?
	After all, if \co{unsigned} is good enough for the Linux
	kernel, shouldn't it be good enough for everyone?
\QuickA{}
	Not at all.
	The Linux kernel has a number of special attributes that allow
	it to ignore the following sequence of events:
	\begin{enumerate}
	\item	Thread 0 executes \co{read_seqbegin()}, picking up
		\co{->seq} in line~17, noting that the value is even,
		and thus returning to the caller.
	\item	Thread 0 starts executing its read-side critical section,
		but is then preempted for a long time.
	\item	Other threads repeatedly invoke \co{write_seqlock()} and
		\co{write_sequnlock()}, until the value of \co{->seq}
		overflows back to the value that Thread~0 fetched.
	\item	Thread 0 resumes execution, completing its read-side
		critical section with inconsistent data.
	\item	Thread 0 invokes \co{read_seqretry()}, which incorrectly
		concludes that Thread~0 has seen a consistent view of
		the data protected by the sequence lock.
	\end{enumerate}

	The Linux kernel uses sequence locking for things that are
	updated rarely, with time-of-day information being a case
	in point.
	This information is updated at most once per millisecond,
	so that seven weeks would be required to overflow the counter.
	If a kernel thread was preempted for seven weeks, the Linux
	kernel's soft-lockup code would be emitting warnings every two
	minutes for that entire time.

	In contrast, with a 64-bit counter, more than five centuries
	would be required to overflow, even given an update every
	\emph{nano}second.
	Therefore, this implementation uses a type for \co{->seq}
	that is 64 bits on 64-bit systems.

\QuickQ{}
	But doesn't Section~\ref{sec:defer:Sequence Locks}'s seqlock
	also permit readers and updaters to get work done concurrently?
\QuickA{}
	Yes and no.
	Although seqlock readers can run concurrently with
	seqlock writers, whenever this happens, the {\tt read\_seqretry()}
	primitive will force the reader to retry.
	This means that any work done by a seqlock reader running concurrently
	with a seqlock updater will be discarded and redone.
	So seqlock readers can \emph{run} concurrently with updaters,
	but they cannot actually get any work done in this case.

	In contrast, RCU readers can perform useful work even in presence
	of concurrent RCU updaters.

\QuickQ{}
	What prevents the {\tt list\_for\_each\_entry\_rcu()} from
	getting a segfault if it happens to execute at exactly the same
	time as the {\tt list\_add\_rcu()}?
\QuickA{}
	On all systems running Linux, loads from and stores
	to pointers are atomic, that is, if a store to a pointer occurs at
	the same time as a load from that same pointer, the load will return
	either the initial value or the value stored, never some bitwise
	mashup of the two.
	In addition, the {\tt list\_for\_each\_entry\_rcu()} always proceeds
	forward through the list, never looking back.
	Therefore, the {\tt list\_for\_each\_entry\_rcu()} will either see
	the element being added by {\tt list\_add\_rcu()} or it will not,
	but either way, it will see a valid well-formed list.

\QuickQ{}
	Why do we need to pass two pointers into
	{\tt hlist\_for\_each\_entry\_rcu()}
	when only one is needed for {\tt list\_for\_each\_entry\_rcu()}?
\QuickA{}
	Because in an hlist it is necessary to check for
	NULL rather than for encountering the head.
	(Try coding up a single-pointer {\tt hlist\_for\_each\_entry\_rcu()}
	If you come up with a nice solution, it would be a very good thing!)

\QuickQ{}
	How would you modify the deletion example to permit more than two
	versions of the list to be active?
\QuickA{}
	One way of accomplishing this is as shown in
	Figure~\ref{fig:defer:Concurrent RCU Deletion}.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 spin_lock(&mylock);
  2 p = search(head, key);
  3 if (p == NULL)
  4   spin_unlock(&mylock);
  5 else {
  6   list_del_rcu(&p->list);
  7   spin_unlock(&mylock);
  8   synchronize_rcu();
  9   kfree(p);
 10 }
\end{verbatim}
}
\caption{Concurrent RCU Deletion}
\label{fig:defer:Concurrent RCU Deletion}
\end{figure}

	Note that this means that multiple concurrent deletions might be
	waiting in \co{synchronize_rcu()}.

\QuickQ{}
	How many RCU versions of a given list can be
	active at any given time?
\QuickA{}
	That depends on the synchronization design.
	If a semaphore protecting the update is held across the grace period,
	then there can be at most two versions, the old and the new.

	However, suppose that only the search, the update, and the
	\co{list_replace_rcu()} were protected by a lock, so that
	the \co{synchronize_rcu()} was outside of that lock, similar
	to the code shown in
	Figure~\ref{fig:defer:Concurrent RCU Deletion}.
	Suppose further that a large number of threads undertook an
	RCU replacement at about the same time, and that readers
	are also constantly traversing the data structure.

	Then the following sequence of events could occur, starting from
	the end state of
	Figure~\ref{fig:defer:RCU Replacement in Linked List}:

	\begin{enumerate}
	\item	Thread~A traverses the list, obtaining a reference to
		the 5,2,3 element.
	\item	Thread~B replaces the 5,2,3 element with a new
		5,2,4 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~C traverses the list, obtaining a reference to
		the 5,2,4 element.
	\item	Thread~D replaces the 5,2,4 element with a new
		5,2,5 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~E traverses the list, obtaining a reference to
		the 5,2,5 element.
	\item	Thread~F replaces the 5,2,5 element with a new
		5,2,6 element, then waits for its \co{synchronize_rcu()}
		call to return.
	\item	Thread~G traverses the list, obtaining a reference to
		the 5,2,6 element.
	\item	And the previous two steps repeat quickly, so that all
		of them happen before any of the \co{synchronize_rcu()}
		calls return.
	\end{enumerate}

	Thus, there can be an arbitrary number of versions active,
	limited only by memory and by how many updates could be completed
	within a grace period.
	But please note that data structures that are updated so frequently
	probably are not good candidates for RCU.
	That said, RCU can handle high update rates when necessary.

\QuickQ{}
	How can RCU updaters possibly delay RCU readers, given that the
	{\tt rcu\_read\_lock()} and {\tt rcu\_read\_unlock()}
	primitives neither spin nor block?
\QuickA{}
	The modifications undertaken by a given RCU updater will cause the
	corresponding CPU to invalidate cache lines containing the data,
	forcing the CPUs running concurrent RCU readers to incur expensive
	cache misses.
	(Can you design an algorithm that changes a data structure
	\emph{without}
	inflicting expensive cache misses on concurrent readers?
	On subsequent readers?)

\QuickQ{}
	WTF?
	How the heck do you expect me to believe that RCU has a
	100-femtosecond overhead when the clock period at 3GHz is more than
	300 \emph{picoseconds}?
\QuickA{}
	First, consider that the inner loop used to
	take this measurement is as follows:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\scriptsize
\begin{verbatim}
  1 for (i = 0; i < CSCOUNT_SCALE; i++) {
  2   rcu_read_lock();
  3   rcu_read_unlock();
  4 }
\end{verbatim}
\end{minipage}
\vspace{5pt}

	Next, consider the effective definitions of \co{rcu_read_lock()}
	and \co{rcu_read_unlock()}:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\scriptsize
\begin{verbatim}
  1 #define rcu_read_lock()   do { } while (0)
  2 #define rcu_read_unlock() do { } while (0)
\end{verbatim}
\end{minipage}
\vspace{5pt}

	Consider also that the compiler does simple optimizations,
	allowing it to replace the loop with:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\scriptsize
\begin{verbatim}
i = CSCOUNT_SCALE;
\end{verbatim}
\end{minipage}
\vspace{5pt}

	So the ``measurement'' of 100 femtoseconds is simply the fixed
	overhead of the timing measurements divided by the number of
	passes through the inner loop containing the calls
	to \co{rcu_read_lock()} and \co{rcu_read_unlock()}.
	And therefore, this measurement really is in error, in fact,
	in error by an arbitrary number of orders of magnitude.
	As you can see by the definition of \co{rcu_read_lock()}
	and \co{rcu_read_unlock()} above, the actual overhead
	is precisely zero.

	It certainly is not every day that a timing measurement of
	100 femtoseconds turns out to be an overestimate!

\QuickQ{}
	Why does both the variability and overhead of rwlock decrease as the
	critical-section overhead increases?
\QuickA{}
	Because the contention on the underlying
	\co{rwlock_t} decreases as the critical-section overhead
	increases.
	However, the rwlock overhead will not quite drop to that on a single
	CPU because of cache-thrashing overhead.

\QuickQ{}
	Is there an exception to this deadlock immunity, and if so,
	what sequence of events could lead to deadlock?
\QuickA{}
	One way to cause a deadlock cycle involving
	RCU read-side primitives is via the following (illegal) sequence
	of statements:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
idx = srcu_read_lock(&srcucb);
synchronize_srcu(&srcucb);
srcu_read_unlock(&srcucb, idx);
\end{verbatim}
\end{minipage}
\vspace{5pt}

	The \co{synchronize_srcu()} cannot return until all
	pre-existing SRCU read-side critical sections complete, but
	is enclosed in an SRCU read-side critical section that cannot
	complete until the \co{synchronize_srcu()} returns.
	The result is a classic self-deadlock--you get the same
	effect when attempting to write-acquire a reader-writer lock
	while read-holding it.

	Note that this self-deadlock scenario does not apply to
	RCU Classic, because the context switch performed by the
	\co{synchronize_rcu()} would act as a quiescent state
	for this CPU, allowing a grace period to complete.
	However, this is if anything even worse, because data used
	by the RCU read-side critical section might be freed as a
	result of the grace period completing.

	In short, do not invoke synchronous RCU update-side primitives
	from within an RCU read-side critical section.

\QuickQ{}
	Immunity to both deadlock and priority inversion???
	Sounds too good to be true.
	Why should I believe that this is even possible?
\QuickA{}
	It really does work.
	After all, if it didn't work, the Linux kernel would not run.

\QuickQ{}
	But wait!
	This is exactly the same code that might be used when thinking
	of RCU as a replacement for reader-writer locking!
	What gives?
\QuickA{}
	This is an effect of the Law of Toy Examples:
	beyond a certain point, the code fragments look the same.
	The only difference is in how we think about the code.
	However, this difference can be extremely important.
	For but one example of the importance, consider that if we think
	of RCU as a restricted reference counting scheme, we would never
	be fooled into thinking that the updates would exclude the RCU
	read-side critical sections.

	It nevertheless is often useful to think of RCU as a replacement
	for reader-writer locking, for example, when you are replacing
	reader-writer locking with RCU.

\QuickQ{}
	Why the dip in refcnt overhead near 6 CPUs?
\QuickA{}
	Most likely NUMA effects.
	However, there is substantial variance in the values measured for the
	refcnt line, as can be seen by the error bars.
	In fact, standard deviations range in excess of 10\% of measured
	values in some cases.
	The dip in overhead therefore might well be a statistical aberration.

\QuickQ{}
	What if the element we need to delete is not the first element
	of the list on line~9 of
	Figure~\ref{fig:defer:Existence Guarantees Enable Per-Element Locking}?
\QuickA{}
	As with
	Figure~\ref{fig:locking:Per-Element Locking Without Existence Guarantees},
	this is a very simple hash table with no chaining, so the only
	element in a given bucket is the first element.
	The reader is again invited to adapt this example to a hash table with
	full chaining.

\QuickQ{}
	Why is it OK to exit the RCU read-side critical section on
	line~15 of
	Figure~\ref{fig:defer:Existence Guarantees Enable Per-Element Locking}
	before releasing the lock on line~17?
\QuickA{}
	First, please note that the second check on line~14 is
	necessary because some other
	CPU might have removed this element while we were waiting
	to acquire the lock.
	However, the fact that we were in an RCU read-side critical section
	while acquiring the lock guarantees that this element could not
	possibly have been re-allocated and re-inserted into this
	hash table.
	Furthermore, once we acquire the lock, the lock itself guarantees
	the element's existence, so we no longer need to be in an
	RCU read-side critical section.

	The question as to whether it is necessary to re-check the
	element's key is left as an exercise to the reader.
	% A re-check is necessary if the key can mutate or if it is
	% necessary to reject deleted entries (in cases where deletion
	% is recorded by mutating the key.

\QuickQ{}
	Why not exit the RCU read-side critical section on
	line~23 of
	Figure~\ref{fig:defer:Existence Guarantees Enable Per-Element Locking}
	before releasing the lock on line~22?
\QuickA{}
	Suppose we reverse the order of these two lines.
	Then this code is vulnerable to the following sequence of
	events:
	\begin{enumerate}
	\item	CPU~0 invokes \co{delete()}, and finds the element
		to be deleted, executing through line~15.
		It has not yet actually deleted the element, but
		is about to do so.
	\item	CPU~1 concurrently invokes \co{delete()}, attempting
		to delete this same element.
		However, CPU~0 still holds the lock, so CPU~1 waits
		for it at line~13.
	\item	CPU~0 executes lines~16 and 17, and blocks at
		line~18 waiting for CPU~1 to exit its RCU read-side
		critical section.
	\item	CPU~1 now acquires the lock, but the test on line~14
		fails because CPU~0 has already removed the element.
		CPU~1 now executes line~22 (which we switched with line~23
		for the purposes of this Quick Quiz)
		and exits its RCU read-side critical section.
	\item	CPU~0 can now return from \co{synchronize_rcu()},
		and thus executes line~19, sending the element to
		the freelist.
	\item	CPU~1 now attempts to release a lock for an element
		that has been freed, and, worse yet, possibly
		reallocated as some other type of data structure.
		This is a fatal memory-corruption error.
	\end{enumerate}

\QuickQ{}
	But what if there is an arbitrarily long series of RCU
	read-side critical sections in multiple threads, so that at
	any point in time there is at least one thread in the system
	executing in an RCU read-side critical section?
	Wouldn't that prevent any data from a \co{SLAB_DESTROY_BY_RCU}
	slab ever being returned to the system, possibly resulting
	in OOM events?
\QuickA{}
	There could certainly be an arbitrarily long period of time
	during which at least one thread is always in an RCU read-side
	critical section.
	However, the key words in the description in
	Section~\ref{sec:defer:RCU is a Way of Providing Type-Safe Memory}
	are ``in-use'' and ``pre-existing''.
	Keep in mind that a given RCU read-side critical section is
	conceptually only permitted to gain references to data elements
	that were in use at the beginning of that critical section.
	Furthermore, remember that a slab cannot be returned to the
	system until all of its data elements have been freed, in fact,
	the RCU grace period cannot start until after they have all been
	freed.

	Therefore, the slab cache need only wait for those RCU read-side
	critical sections that started before the freeing of the last element
	of the slab.
	This in turn means that any RCU grace period that begins after
	the freeing of the last element will do---the slab may be returned
	to the system after that grace period ends.

\QuickQ{}
	Suppose that the \co{nmi_profile()} function was preemptible.
	What would need to change to make this example work correctly?
\QuickA{}
	One approach would be to use
	\co{rcu_read_lock()} and \co{rcu_read_unlock()}
	in \co{nmi_profile()}, and to replace the
	\co{synchronize_sched()} with \co{synchronize_rcu()},
	perhaps as shown in
	Figure~\ref{fig:defer:Using RCU to Wait for Mythical Preemptible NMIs to Finish}.

\begin{figure}[tbp]
{ \tt \scriptsize
\begin{verbatim}
  1 struct profile_buffer {
  2   long size;
  3   atomic_t entry[0];
  4 };
  5 static struct profile_buffer *buf = NULL;
  6
  7 void nmi_profile(unsigned long pcvalue)
  8 {
  9   struct profile_buffer *p;
 10
 11   rcu_read_lock();
 12   p = rcu_dereference(buf);
 13   if (p == NULL) {
 14     rcu_read_unlock();
 15     return;
 16   }
 17   if (pcvalue >= p->size) {
 18     rcu_read_unlock();
 19     return;
 20   }
 21   atomic_inc(&p->entry[pcvalue]);
 22   rcu_read_unlock();
 23 }
 24
 25 void nmi_stop(void)
 26 {
 27   struct profile_buffer *p = buf;
 28
 29   if (p == NULL)
 30     return;
 31   rcu_assign_pointer(buf, NULL);
 32   synchronize_rcu();
 33   kfree(p);
 34 }
\end{verbatim}
}
\caption{Using RCU to Wait for Mythical Preemptible NMIs to Finish}
\label{fig:defer:Using RCU to Wait for Mythical Preemptible NMIs to Finish}
\end{figure}


\QuickQ{}
	Why do some of the cells in
	Table~\ref{tab:defer:RCU Wait-to-Finish APIs}
	have exclamation marks (``!'')?
\QuickA{}
	The API members with exclamation marks (\co{rcu_read_lock()},
	\co{rcu_read_unlock()}, and \co{call_rcu()}) were the
	only members of the Linux RCU API that Paul E. McKenney was aware
	of back in the mid-90s.
	During this timeframe, he was under the mistaken impression that
	he knew all that there is to know about RCU.

\QuickQ{}
	How do you prevent a huge number of RCU read-side critical
	sections from indefinitely blocking a \co{synchronize_rcu()}
	invocation?
\QuickA{}
	There is no need to do anything to prevent RCU read-side
	critical sections from indefinitely blocking a
	\co{synchronize_rcu()} invocation, because the
	\co{synchronize_rcu()} invocation need wait only for
	\emph{pre-existing} RCU read-side critical sections.
	So as long as each RCU read-side critical section is
	of finite duration, there should be no problem.

\QuickQ{}
	The \co{synchronize_rcu()} API waits for all pre-existing
	interrupt handlers to complete, right?
\QuickA{}
	Absolutely not!
	And especially not when using preemptible RCU!
	You instead want \co{synchronize_irq()}.
	Alternatively, you can place calls to \co{rcu_read_lock()}
	and \co{rcu_read_unlock()} in the specific interrupt handlers that
	you want \co{synchronize_rcu()} to wait for.

\QuickQ{}
	What happens if you mix and match?
	For example, suppose you use \co{rcu_read_lock()} and
	\co{rcu_read_unlock()} to delimit RCU read-side critical
	sections, but then use \co{call_rcu_bh()} to post an
	RCU callback?
\QuickA{}
	If there happened to be no RCU read-side critical
	sections delimited by \co{rcu_read_lock_bh()} and
	\co{rcu_read_unlock_bh()} at the time \co{call_rcu_bh()}
	was invoked, RCU would be within its rights to invoke the callback
	immediately, possibly freeing a data structure still being used by
	the RCU read-side critical section!
	This is not merely a theoretical possibility: a long-running RCU
	read-side critical section delimited by \co{rcu_read_lock()}
	and \co{rcu_read_unlock()} is vulnerable to this failure mode.

	However, the \co{rcu_dereference()} family of functions apply
	to all flavors of RCU.
	(There was an attempt to have per-flavor variants of
	\co{rcu_dereference()}, but it was just too messy.)

\QuickQ{}
	Hardware interrupt handlers can be thought of as being
	under the protection of an implicit \co{rcu_read_lock_bh()},
	right?
\QuickA{}
	Absolutely not!
	And especially not when using preemptible RCU!
	If you need to access ``rcu\_bh''-protected data structures
	in an interrupt handler, you need to provide explicit calls to
	\co{rcu_read_lock_bh()} and \co{rcu_read_unlock_bh()}.

\QuickQ{}
	What happens if you mix and match RCU Classic and RCU Sched?
\QuickA{}
	In a non-\co{PREEMPT} or a \co{PREEMPT} kernel, mixing these
	two works ``by accident'' because in those kernel builds, RCU Classic
	and RCU Sched map to the same implementation.
	However, this mixture is fatal in \co{PREEMPT_RT} builds using the -rt
	patchset, due to the fact that Realtime RCU's read-side critical
	sections can be preempted, which would permit
	\co{synchronize_sched()} to return before the
	RCU read-side critical section reached its \co{rcu_read_unlock()}
	call.
	This could in turn result in a data structure being freed before the
	read-side critical section was finished with it,
	which could in turn greatly increase the actuarial risk experienced
	by your kernel.

	In fact, the split between RCU Classic and RCU Sched was inspired
	by the need for preemptible RCU read-side critical sections.

\QuickQ{}
	In general, you cannot rely on \co{synchronize_sched()} to
	wait for all pre-existing interrupt handlers,
	right?
\QuickA{}
	That is correct!
	Because -rt Linux uses threaded interrupt handlers, there can
	be context switches in the middle of an interrupt handler.
	Because \co{synchronize_sched()} waits only until each
	CPU has passed through a context switch, it can return
	before a given interrupt handler completes.

	If you need to wait for a given interrupt handler to complete,
	you should instead use \co{synchronize_irq()} or place
	explicit RCU read-side critical sections in the interrupt
	handlers that you wish to wait on.

\QuickQ{}
	Why do both SRCU and QRCU lack asynchronous \co{call_srcu()}
	or \co{call_qrcu()} interfaces?
\QuickA{}
	Given an asynchronous interface, a single task
	could register an arbitrarily large number of SRCU or QRCU callbacks,
	thereby consuming an arbitrarily large quantity of memory.
	In contrast, given the current synchronous
	\co{synchronize_srcu()} and \co{synchronize_qrcu()}
	interfaces, a given task must finish waiting for a given grace period
	before it can start waiting for the next one.

\QuickQ{}
	Under what conditions can \co{synchronize_srcu()} be safely
	used within an SRCU read-side critical section?
\QuickA{}
	In principle, you can use
	\co{synchronize_srcu()} with a given \co{srcu_struct}
	within an SRCU read-side critical section that uses some other
	\co{srcu_struct}.
	In practice, however, doing this is almost certainly a bad idea.
	In particular, the code shown in
	Figure~\ref{fig:defer:Multistage SRCU Deadlocks}
	could still result in deadlock.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 idx = srcu_read_lock(&ssa);
  2 synchronize_srcu(&ssb);
  3 srcu_read_unlock(&ssa, idx);
  4
  5 /* . . . */
  6
  7 idx = srcu_read_lock(&ssb);
  8 synchronize_srcu(&ssa);
  9 srcu_read_unlock(&ssb, idx);
\end{verbatim}
}
\caption{Multistage SRCU Deadlocks}
\label{fig:defer:Multistage SRCU Deadlocks}
\end{figure}


\QuickQ{}
	Why doesn't \co{list_del_rcu()} poison both the \co{next}
	and \co{prev} pointers?
\QuickA{}
	Poisoning the \co{next} pointer would interfere
	with concurrent RCU readers, who must use this pointer.
	However, RCU readers are forbidden from using the \co{prev}
	pointer, so it may safely be poisoned.

\QuickQ{}
	Normally, any pointer subject to \co{rcu_dereference()} \emph{must}
	always be updated using \co{rcu_assign_pointer()}.
	What is an exception to this rule?
\QuickA{}
	One such exception is when a multi-element linked
	data structure is initialized as a unit while inaccessible to other
	CPUs, and then a single \co{rcu_assign_pointer()} is used
	to plant a global pointer to this data structure.
	The initialization-time pointer assignments need not use
	\co{rcu_assign_pointer()}, though any such assignments that
	happen after the structure is globally visible \emph{must} use
	\co{rcu_assign_pointer()}.

	However, unless this initialization code is on an impressively hot
	code-path, it is probably wise to use \co{rcu_assign_pointer()}
	anyway, even though it is in theory unnecessary.
	It is all too easy for a ``minor'' change to invalidate your cherished
	assumptions about the initialization happening privately.

\QuickQ{}
	Are there any downsides to the fact that these traversal and update
	primitives can be used with any of the RCU API family members?
\QuickA{}
	It can sometimes be difficult for automated
	code checkers such as ``sparse'' (or indeed for human beings) to
	work out which type of RCU read-side critical section a given
	RCU traversal primitive corresponds to.
	For example, consider the code shown in
	Figure~\ref{fig:defer:Diverse RCU Read-Side Nesting}.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 rcu_read_lock();
  2 preempt_disable();
  3 p = rcu_dereference(global_pointer);
  4
  5 /* . . . */
  6
  7 preempt_enable();
  8 rcu_read_unlock();
\end{verbatim}
}
\caption{Diverse RCU Read-Side Nesting}
\label{fig:defer:Diverse RCU Read-Side Nesting}
\end{figure}

	Is the \co{rcu_dereference()} primitive in an RCU Classic
	or an RCU Sched critical section?
	What would you have to do to figure this out?

\QuickQ{}
	Why wouldn't any deadlock in the RCU implementation in
	Figure~\ref{fig:defer:Lock-Based RCU Implementation}
	also be a deadlock in any other RCU implementation?
\QuickA{}

\begin{figure}[tbp]
{ \scriptsize
\begin{verbatim}
  1 void foo(void)
  2 {
  3   spin_lock(&my_lock);
  4   rcu_read_lock();
  5   do_something();
  6   rcu_read_unlock();
  7   do_something_else();
  8   spin_unlock(&my_lock);
  9 }
 10
 11 void bar(void)
 12 {
 13   rcu_read_lock();
 14   spin_lock(&my_lock);
 15   do_some_other_thing();
 16   spin_unlock(&my_lock);
 17   do_whatever();
 18   rcu_read_unlock();
 19 }
\end{verbatim}
}
\caption{Deadlock in Lock-Based RCU Implementation}
\label{fig:defer:Deadlock in Lock-Based RCU Implementation}
\end{figure}

	Suppose the functions \co{foo()} and \co{bar()} in
	Figure~\ref{fig:defer:Deadlock in Lock-Based RCU Implementation}
	are invoked concurrently from different CPUs.
	Then \co{foo()} will acquire \co{my_lock()} on line~3,
	while \co{bar()} will acquire \co{rcu_gp_lock} on
	line~13.
	When \co{foo()} advances to line~4, it will attempt to
	acquire \co{rcu_gp_lock}, which is held by \co{bar()}.
	Then when \co{bar()} advances to line~14, it will attempt
	to acquire \co{my_lock}, which is held by \co{foo()}.

	Each function is then waiting for a lock that the other
	holds, a classic deadlock.

	Other RCU implementations neither spin nor block in
	\co{rcu_read_lock()}, hence avoiding deadlocks.

\QuickQ{}
	Why not simply use reader-writer locks in the RCU implementation
	in
	Figure~\ref{fig:defer:Lock-Based RCU Implementation}
	in order to allow RCU readers to proceed in parallel?
\QuickA{}
	One could in fact use reader-writer locks in this manner.
	However, textbook reader-writer locks suffer from memory
	contention, so that the RCU read-side critical sections would
	need to be quite long to actually permit parallel
	execution~\cite{McKenney03a}.

	On the other hand, use of a reader-writer lock that is
	read-acquired in \co{rcu_read_lock()} would avoid the
	deadlock condition noted above.

\QuickQ{}
	Wouldn't it be cleaner to acquire all the locks, and then
	release them all in the loop from lines~15-18 of
	Figure~\ref{fig:defer:Per-Thread Lock-Based RCU Implementation}?
	After all, with this change, there would be a point in time
	when there were no readers, simplifying things greatly.
\QuickA{}
	Making this change would re-introduce the deadlock, so
	no, it would not be cleaner.

\QuickQ{}
	Is the implementation shown in
	Figure~\ref{fig:defer:Per-Thread Lock-Based RCU Implementation}
	free from deadlocks?
	Why or why not?
\QuickA{}
	One deadlock is where a lock is
	held across \co{synchronize_rcu()}, and that same lock is
	acquired within an RCU read-side critical section.
	However, this situation could deadlock any correctly designed
	RCU implementation.
	After all, the \co{synchronize_rcu()} primitive must wait for all
	pre-existing RCU read-side critical sections to complete,
	but if one of those critical sections is spinning on a lock
	held by the thread executing the \co{synchronize_rcu()},
	we have a deadlock inherent in the definition of RCU.

	Another deadlock happens when attempting to nest RCU read-side
	critical sections.
	This deadlock is peculiar to this implementation, and might
	be avoided by using recursive locks, or by using reader-writer
	locks that are read-acquired by \co{rcu_read_lock()} and
	write-acquired by \co{synchronize_rcu()}.

	However, if we exclude the above two cases,
	this implementation of RCU does not introduce any deadlock
	situations.
	This is because only time some other thread's lock is acquired is when
	executing \co{synchronize_rcu()}, and in that case, the lock
	is immediately released, prohibiting a deadlock cycle that
	does not involve a lock held across the \co{synchronize_rcu()}
	which is the first case above.

\QuickQ{}
	Isn't one advantage of the RCU algorithm shown in
	Figure~\ref{fig:defer:Per-Thread Lock-Based RCU Implementation}
	that it uses only primitives that are widely available,
	for example, in POSIX pthreads?
\QuickA{}
	This is indeed an advantage, but do not forget that
	\co{rcu_dereference()} and \co{rcu_assign_pointer()}
	are still required, which means \co{volatile} manipulation
	for \co{rcu_dereference()} and memory barriers for
	\co{rcu_assign_pointer()}.
	Of course, many Alpha CPUs require memory barriers for both
	primitives.

\QuickQ{}
	But what if you hold a lock across a call to
	\co{synchronize_rcu()}, and then acquire that same lock within
	an RCU read-side critical section?
\QuickA{}
	Indeed, this would deadlock any legal RCU implementation.
	But is \co{rcu_read_lock()} \emph{really} participating in
	the deadlock cycle?
	If you believe that it is, then please
	ask yourself this same question when looking at the
	RCU implementation in
	Section~\ref{defer:RCU Based on Quiescent States}.

\QuickQ{}
	How can the grace period possibly elapse in 40 nanoseconds when
	\co{synchronize_rcu()} contains a 10-millisecond delay?
\QuickA{}
	The update-side test was run in absence of readers, so the
	\co{poll()} system call was never invoked.
	In addition, the actual code has this \co{poll()}
	system call commented out, the better to evaluate the
	true overhead of the update-side code.
	Any production uses of this code would be better served by
	using the \co{poll()} system call, but then again,
	production uses would be even better served by other implementations
	shown later in this section.

\QuickQ{}
	Why not simply make \co{rcu_read_lock()} wait when a concurrent
	\co{synchronize_rcu()} has been waiting too long in
	the RCU implementation in
	Figure~\ref{fig:defer:RCU Implementation Using Single Global Reference Counter}?
	Wouldn't that prevent \co{synchronize_rcu()} from starving?
\QuickA{}
	Although this would in fact eliminate the starvation, it would
	also mean that \co{rcu_read_lock()} would spin or block waiting
	for the writer, which is in turn waiting on readers.
	If one of these readers is attempting to acquire a lock that
	the spinning/blocking \co{rcu_read_lock()} holds, we again
	have deadlock.

	In short, the cure is worse than the disease.
	See Section~\ref{defer:Starvation-Free Counter-Based RCU}
	for a proper cure.

\QuickQ{}
	Why the memory barrier on line~5 of \co{synchronize_rcu()} in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
	given that there is a spin-lock acquisition immediately after?
\QuickA{}
	The spin-lock acquisition only guarantees that the spin-lock's
	critical section will not ``bleed out'' to precede the
	acquisition.
	It in no way guarantees that code preceding the spin-lock
	acquisition won't be reordered into the critical section.
	Such reordering could cause a removal from an RCU-protected
	list to be reordered to follow the complementing of
	\co{rcu_idx}, which could allow a newly starting RCU
	read-side critical section to see the recently removed
	data element.

	Exercise for the reader: use a tool such as Promela/spin
	to determine which (if any) of the memory barriers in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
	are really needed.
	See Section~\ref{chp:formal:Formal Verification}
	for information on using these tools.
	The first correct and complete response will be credited.

\QuickQ{}
	Why is the counter flipped twice in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}?
	Shouldn't a single flip-and-wait cycle be sufficient?
\QuickA{}
	Both flips are absolutely required.
	To see this, consider the following sequence of events:
	\begin{enumerate}
	\item	Line~8 of \co{rcu_read_lock()} in
		Figure~\ref{fig:defer:RCU Read-Side Using Global Reference-Count Pair}
		picks up \co{rcu_idx}, finding its value to be zero.
	\item	Line~8 of \co{synchronize_rcu()} in
		Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
		complements the value of \co{rcu_idx}, setting its
		value to one.
	\item	Lines~10-13 of \co{synchronize_rcu()} find that the
		value of \co{rcu_refcnt[0]} is zero, and thus
		returns.
		(Recall that the question is asking what happens if
		lines~14-20 are omitted.)
	\item	Lines~9 and 10 of \co{rcu_read_lock()} store the
		value zero to this thread's instance of \co{rcu_read_idx}
		and increments \co{rcu_refcnt[0]}, respectively.
		Execution then proceeds into the RCU read-side critical
		section.
		\label{defer:rcu_rcgp:RCU Read Side Start}
	\item	Another instance of \co{synchronize_rcu()} again complements
		\co{rcu_idx}, this time setting its value to zero.
		Because \co{rcu_refcnt[1]} is zero, \co{synchronize_rcu()}
		returns immediately.
		(Recall that \co{rcu_read_lock()} incremented
		\co{rcu_refcnt[0]}, not \co{rcu_refcnt[1]}!)
		\label{defer:rcu_rcgp:RCU Grace Period Start}
	\item	The grace period that started in
		step~\ref{defer:rcu_rcgp:RCU Grace Period Start}
		has been allowed to end, despite
		the fact that the RCU read-side critical section
		that started beforehand in
		step~\ref{defer:rcu_rcgp:RCU Read Side Start}
		has not completed.
		This violates RCU semantics, and could allow the update
		to free a data element that the RCU read-side critical
		section was still referencing.
	\end{enumerate}

	Exercise for the reader: What happens if \co{rcu_read_lock()}
	is preempted for a very long time (hours!) just after
	line~8?
	Does this implementation operate correctly in that case?
	Why or why not?
	The first correct and complete response will be credited.

\QuickQ{}
	Given that atomic increment and decrement are so expensive,
	why not just use non-atomic increment on line~10 and a
	non-atomic decrement on line~25 of
	Figure~\ref{fig:defer:RCU Read-Side Using Global Reference-Count Pair}?
\QuickA{}
	Using non-atomic operations would cause increments and decrements
	to be lost, in turn causing the implementation to fail.
	See Section~\ref{defer:Scalable Counter-Based RCU}
	for a safe way to use non-atomic operations in
	\co{rcu_read_lock()} and \co{rcu_read_unlock()}.

\QuickQ{}
	Come off it!
	We can see the \co{atomic_read()} primitive in
	\co{rcu_read_lock()}!!!
	So why are you trying to pretend that \co{rcu_read_lock()}
	contains no atomic operations???
\QuickA{}
	The \co{atomic_read()} primitives does not actually execute
	atomic machine instructions, but rather does a normal load
	from an \co{atomic_t}.
	Its sole purpose is to keep the compiler's type-checking happy.
	If the Linux kernel ran on 8-bit CPUs, it would also need to
	prevent ``store tearing'', which could happen due to the need
	to store a 16-bit pointer with two eight-bit accesses on some
	8-bit systems.
	But thankfully, it seems that no one runs Linux on 8-bit systems.

\QuickQ{}
	Great, if we have $N$ threads, we can have $2N$ ten-millisecond
	waits (one set per \co{flip_counter_and_wait()} invocation,
	and even that assumes that we wait only once for each thread.
	Don't we need the grace period to complete \emph{much} more quickly?
\QuickA{}
	Keep in mind that we only wait for a given thread if that thread
	is still in a pre-existing RCU read-side critical section,
	and that waiting for one hold-out thread gives all the other
	threads a chance to complete any pre-existing RCU read-side
	critical sections that they might still be executing.
	So the only way that we would wait for $2N$ intervals
	would be if the last thread still remained in a pre-existing
	RCU read-side critical section despite all the waiting for
	all the prior threads.
	In short, this implementation will not wait unnecessarily.

	However, if you are stress-testing code that uses RCU, you
	might want to comment out the \co{poll()} statement in
	order to better catch bugs that incorrectly retain a reference
	to an RCU-protected data element outside of an RCU
	read-side critical section.

\QuickQ{}
	All of these toy RCU implementations have either atomic operations
	in \co{rcu_read_lock()} and \co{rcu_read_unlock()},
	or \co{synchronize_rcu()}
	overhead that increases linearly with the number of threads.
	Under what circumstances could an RCU implementation enjoy
	light-weight implementations for all three of these primitives,
	all having deterministic ($O\left(1\right)$) overheads and latencies?
\QuickA{}
	Special-purpose uniprocessor implementations of RCU can attain
	this ideal~\cite{PaulEMcKenney2009BloatwatchRCU}.

\QuickQ{}
	If any even value is sufficient to tell \co{synchronize_rcu()}
	to ignore a given task, why don't lines~10 and~11 of
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}
	simply assign zero to \co{rcu_reader_gp}?
\QuickA{}
	Assigning zero (or any other even-numbered constant)
	would in fact work, but assigning the value of
	\co{rcu_gp_ctr} can provide a valuable debugging aid,
	as it gives the developer an idea of when the corresponding
	thread last exited an RCU read-side critical section.

\QuickQ{}
	Why are the memory barriers on lines~19 and~31 of
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}
	needed?
	Aren't the memory barriers inherent in the locking
	primitives on lines~20 and~30 sufficient?
\QuickA{}
	These memory barriers are required because the locking
	primitives are only guaranteed to confine the critical
	section.
	The locking primitives are under absolutely no obligation
	to keep other code from bleeding in to the critical section.
	The pair of memory barriers are therefore requires to prevent
	this sort of code motion, whether performed by the compiler
	or by the CPU.

\QuickQ{}
	Couldn't the update-side batching optimization described in
	Section~\ref{defer:Scalable Counter-Based RCU With Shared Grace Periods}
	be applied to the implementation shown in
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}?
\QuickA{}
	Indeed it could, with a few modifications.
	This work is left as an exercise for the reader.

\QuickQ{}
	Is the possibility of readers being preempted in
	lines~3-4 of Figure~\ref{fig:defer:Free-Running Counter Using RCU}
	a real problem, in other words, is there a real sequence
	of events that could lead to failure?
	If not, why not?
	If so, what is the sequence of events, and how can the
	failure be addressed?
\QuickA{}
	It is a real problem, there is a sequence of events leading to
	failure, and there are a number of possible ways of
	addressing it.
	For more details, see the Quick Quizzes near the end of
	Section~\ref{defer:Nestable RCU Based on Free-Running Counter}.
	The reason for locating the discussion there is to (1) give you
	more time to think about it, and (2) because the nesting support
	added in that section greatly reduces the time required to
	overflow the counter.

\QuickQ{}
	Why not simply maintain a separate per-thread nesting-level
	variable, as was done in previous section, rather than having
	all this complicated bit manipulation?
\QuickA{}
	The apparent simplicity of the separate per-thread variable
	is a red herring.
	This approach incurs much greater complexity in the guise
	of careful ordering of operations, especially if signal
	handlers are to be permitted to contain RCU read-side
	critical sections.
	But don't take my word for it, code it up and see what you
	end up with!

\QuickQ{}
	Given the algorithm shown in
	Figure~\ref{fig:defer:Nestable RCU Using a Free-Running Counter},
	how could you double the time required to overflow the global
	\co{rcu_gp_ctr}?
\QuickA{}
	One way would be to replace the magnitude comparison on
	lines~33 and 34 with an inequality check of the per-thread
	\co{rcu_reader_gp} variable against
	\co{rcu_gp_ctr+RCU_GP_CTR_BOTTOM_BIT}.

\QuickQ{}
	Again, given the algorithm shown in
	Figure~\ref{fig:defer:Nestable RCU Using a Free-Running Counter},
	is counter overflow fatal?
	Why or why not?
	If it is fatal, what can be done to fix it?
\QuickA{}
	It can indeed be fatal.
	To see this, consider the following sequence of events:
	\begin{enumerate}
	\item	Thread~0 enters \co{rcu_read_lock()}, determines
		that it is not nested, and therefore fetches the
		value of the global \co{rcu_gp_ctr}.
		Thread~0 is then preempted for an extremely long time
		(before storing to its per-thread \co{rcu_reader_gp}
		variable).
	\item	Other threads repeatedly invoke \co{synchronize_rcu()},
		so that the new value of the global \co{rcu_gp_ctr}
		is now \co{RCU_GP_CTR_BOTTOM_BIT}
		less than it was when thread~0 fetched it.
	\item	Thread~0 now starts running again, and stores into
		its per-thread \co{rcu_reader_gp} variable.
		The value it stores is
		\co{RCU_GP_CTR_BOTTOM_BIT+1}
		greater than that of the global \co{rcu_gp_ctr}.
	\item	Thread~0 acquires a reference to RCU-protected data
		element~A.
	\item	Thread 1 now removes the data element~A that thread~0
		just acquired a reference to.
	\item	Thread 1 invokes \co{synchronize_rcu()}, which
		increments the global \co{rcu_gp_ctr} by
		\co{RCU_GP_CTR_BOTTOM_BIT}.
		It then checks all of the per-thread \co{rcu_reader_gp}
		variables, but thread~0's value (incorrectly) indicates
		that it started after thread~1's call to
		\co{synchronize_rcu()}, so thread~1 does not wait
		for thread~0 to complete its RCU read-side critical
		section.
	\item	Thread 1 then frees up data element~A, which thread~0
		is still referencing.
	\end{enumerate}

	Note that scenario can also occur in the implementation presented in
	Section~\ref{defer:RCU Based on Free-Running Counter}.

	One strategy for fixing this problem is to use 64-bit
	counters so that the time required to overflow them would exceed
	the useful lifetime of the computer system.
	Note that non-antique members of the 32-bit x86 CPU family
	allow atomic manipulation of 64-bit counters via the
	\co{cmpxchg64b} instruction.

	Another strategy is to limit the rate at which grace periods are
	permitted to occur in order to achieve a similar effect.
	For example, \co{synchronize_rcu()} could record the last time
	that it was invoked, and any subsequent invocation would then
	check this time and block as needed to force the desired
	spacing.
	For example, if the low-order four bits of the counter were
	reserved for nesting, and if grace periods were permitted to
	occur at most ten times per second, then it would take more
	than 300 days for the counter to overflow.
	However, this approach is not helpful if there is any possibility
	that the system will be fully loaded with CPU-bound high-priority
	real-time threads for the full 300 days.
	(A remote possibility, perhaps, but best to consider it ahead
	of time.)

	A third approach is to administratively abolish real-time threads
	from the system in question.
	In this case, the preempted process will age up in priority,
	thus getting to run long before the counter had a chance to
	overflow.
	Of course, this approach is less than helpful for real-time
	applications.

	A final approach would be for \co{rcu_read_lock()} to recheck
	the value of the global \co{rcu_gp_ctr} after storing to its
	per-thread \co{rcu_reader_gp} counter, retrying if the new
	value of the global \co{rcu_gp_ctr} is inappropriate.
	This works, but introduces non-deterministic execution time
	into \co{rcu_read_lock()}.
	On the other hand, if your application is being preempted long
	enough for the counter to overflow, you have no hope of
	deterministic execution time in any case!

	% @@@ A fourth approach is rcu_nest32.[hc].

\QuickQ{}
	Doesn't the additional memory barrier shown on line~14 of
	Figure~\ref{fig:defer:Quiescent-State-Based RCU Read Side},
	greatly increase the overhead of \co{rcu_quiescent_state}?
\QuickA{}
	Indeed it does!
	An application using this implementation of RCU should therefore
	invoke \co{rcu_quiescent_state} sparingly, instead using
	\co{rcu_read_lock()} and \co{rcu_read_unlock()} most of the
	time.

	However, this memory barrier is absolutely required so that
	other threads will see the store on lines~12-13 before any
	subsequent RCU read-side critical sections executed by the
	caller.

\QuickQ{}
	Why are the two memory barriers on lines~19 and 22 of
	Figure~\ref{fig:defer:Quiescent-State-Based RCU Read Side}
	needed?
\QuickA{}
	The memory barrier on line~19 prevents any RCU read-side
	critical sections that might precede the
	call to \co{rcu_thread_offline()} won't be reordered by either
	the compiler or the CPU to follow the assignment on lines~20-21.
	The memory barrier on line~22 is, strictly speaking, unnecessary,
	as it is illegal to have any RCU read-side critical sections
	following the call to \co{rcu_thread_offline()}.

\QuickQ{}
	To be sure, the clock frequencies of Power
	systems in 2008 were quite high, but even a 5GHz clock
	frequency is insufficient to allow
	loops to be executed in 50~picoseconds!
	What is going on here?
\QuickA{}
	Since the measurement loop contains a pair of empty functions,
	the compiler optimizes it away.
	The measurement loop takes 1,000 passes between each call to
	\co{rcu_quiescent_state()}, so this measurement is roughly
	one thousandth of the overhead of a single call to
	\co{rcu_quiescent_state()}.

\QuickQ{}
	Why would the fact that the code is in a library make
	any difference for how easy it is to use the RCU
	implementation shown in
	Figures~\ref{fig:defer:Quiescent-State-Based RCU Read Side} and
	\ref{fig:defer:RCU Update Side Using Quiescent States}?
\QuickA{}
	A library function has absolutely no control over the caller,
	and thus cannot force the caller to invoke \co{rcu_quiescent_state()}
	periodically.
	On the other hand, a library function that made many references
	to a given RCU-protected data structure might be able to invoke
	\co{rcu_thread_online()} upon entry,
	\co{rcu_quiescent_state()} periodically, and
	\co{rcu_thread_offline()} upon exit.

\QuickQ{}
	But what if you hold a lock across a call to
	\co{synchronize_rcu()}, and then acquire that same lock within
	an RCU read-side critical section?
	This should be a deadlock, but how can a primitive that
	generates absolutely no code possibly participate in a
	deadlock cycle?
\QuickA{}
	Please note that the RCU read-side critical section is in
	effect extended beyond the enclosing
	\co{rcu_read_lock()} and \co{rcu_read_unlock()}, out to
	the previous and next call to \co{rcu_quiescent_state()}.
	This \co{rcu_quiescent_state} can be thought of as a
	\co{rcu_read_unlock()} immediately followed by an
	\co{rcu_read_lock()}.

	Even so, the actual deadlock itself will involve the lock
	acquisition in the RCU read-side critical section and
	the \co{synchronize_rcu()}, never the \co{rcu_quiescent_state()}.

\QuickQ{}
	Given that grace periods are prohibited within RCU read-side
	critical sections, how can an RCU data structure possibly be
	updated while in an RCU read-side critical section?
\QuickA{}
	This situation is one reason for the existence of asynchronous
	grace-period primitives such as \co{call_rcu()}.
	This primitive may be invoked within an RCU read-side critical
	section, and the specified RCU callback will in turn be invoked
	at a later time, after a grace period has elapsed.

	The ability to perform an RCU update while within an RCU read-side
	critical section can be extremely convenient, and is analogous
	to a (mythical) unconditional read-to-write upgrade for
	reader-writer locking.

\QuickQ{}
	The statistical-counter implementation shown in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}
	(\url{count_end.c})
	used a global lock to guard the summation in \co{read_count()},
	which resulted in poor performance and negative scalability.
	How could you use RCU to provide \co{read_count()} with
	excellent performance and good scalability.
	(Keep in mind that \co{read_count()}'s scalability will
	necessarily be limited by its need to scan all threads'
	counters.)
\QuickA{}
	Hint: place the global variable \co{finalcount} and the
	array \co{counterp[]} into a single RCU-protected struct.
	At initialization time, this structure would be allocated
	and set to all zero and \co{NULL}.

	The \co{inc_count()} function would be unchanged.

	The \co{read_count()} function would use \co{rcu_read_lock()}
	instead of acquiring \co{final_mutex}, and would need to
	use \co{rcu_dereference()} to acquire a reference to the
	current structure.

	The \co{count_register_thread()} function would set the
	array element corresponding to the newly created thread
	to reference that thread's per-thread \co{counter} variable.

	The \co{count_unregister_thread()} function would need to
	allocate a new structure, acquire \co{final_mutex},
	copy the old structure to the new one, add the outgoing
	thread's \co{counter} variable to the total, \co{NULL}
	the pointer to this same \co{counter} variable,
	use \co{rcu_assign_pointer()} to install the new structure
	in place of the old one, release \co{final_mutex},
	wait for a grace period, and finally free the old structure.

	Does this really work?
	Why or why not?

	See
	Section~\ref{sec:together:RCU and Per-Thread-Variable-Based Statistical Counters}
	on
	page~\pageref{sec:together:RCU and Per-Thread-Variable-Based Statistical Counters}
	for more details.

\QuickQ{}
	Section~\ref{sec:count:Applying Specialized Parallel Counters}
	showed a fanciful pair of code fragments that dealt with counting
	I/O accesses to removable devices.
	These code fragments suffered from high overhead on the fastpath
	(starting an I/O) due to the need to acquire a reader-writer
	lock.
	How would you use RCU to provide excellent performance and
	scalability?
	(Keep in mind that the performance of the common-case first
	code fragment that does I/O accesses is much more important
	than that of the device-removal code fragment.)
\QuickA{}
	Hint: replace the read-acquisitions of the reader-writer lock
	with RCU read-side critical sections, then adjust the
	device-removal code fragment to suit.

	See
	Section~\ref{sec:together:RCU and Counters for Removable I/O Devices}
	on
	Page~\pageref{sec:together:RCU and Counters for Removable I/O Devices}
	for one solution to this problem.

\QuickQ{}
	But can't both reference counting and hazard pointers can also acquire
	a reference to multiple data elements with constant overhead?
	A single reference count can cover multiple data elements, right?
\QuickA{}
	Almost.
	As we will see in the ``Unconditional Acquisition'' column,
	neither reference counting
	nor hazard pointers provide unconditional acquisition of references,
	so acquiring a reference can have non-constant overhead in the face
	of conflicting updates.

	In addition, using a single reference count to cover multiple
	data items can have severe consequences, for example, you cannot
	remove any of the data items until all references to all of them
	have been released.
	This can result in more complex data-element-cleanup code,
	and can also increase memory footprint to rival that of RCU.
	In other words, the increased memory footprint is a consequence
	not of RCU in particular, but of bulk reference-count acquisition
	in general.

\QuickQAC{chp:Data Structures}{Data Structures}
\QuickQ{}
	But there are many types of hash tables, of which the chained
	hash tables described here are but one type.
	Why the focus on chained hash tables?
\QuickA{}
	Chained hash tables are completely partitionable, and thus
	well-suited to concurrent use.
	There are other completely-partitionable hash tables, for
	example, split-ordered list~\cite{OriShalev2006SplitOrderListHash},
	but they are considerably more complex.
	We therefore start with chained hash tables.

\QuickQ{}
	But isn't the double comparison on lines~15-18 in
	Figure~\ref{fig:datastruct:Hash-Table Lookup} inefficient
	in the case where the key fits into an unsigned long?
\QuickA{}
	Indeed it is!
	However, hash tables quite frequently store information with
	keys such as character strings that do not necessarily fit
	into an unsigned long.
	Simplifying the hash-table implementation for the case where
	keys always fit into unsigned longs is left as an exercise
	for the reader.

\QuickQ{}
	Instead of simply increasing the number of hash buckets,
	wouldn't it be better to cache-align the existing hash buckets?
\QuickA{}
	The answer depends on a great many things.
	If the hash table has a large number of elements per bucket, it
	would clearly be better to increase the number of hash buckets.
	On the other hand, if the hash table is lightly loaded,
	the answer depends on the hardware, the effectiveness of the
	hash function, and the workload.
	Interested readers are encouraged to experiment.

\QuickQ{}
	Given the negative scalability of the Schr\"odinger's
	Zoo application across sockets, why not just run multiple
	copies of the application, with each copy having a subset
	of the animals and confined to run on a single socket?
\QuickA{}
	You can do just that!
	In fact, you can extend this idea to large clustered systems,
	running one copy of the application on each node of the cluster.
	This practice is called ``sharding'', and is heavily used in
	practice by large web-based
	retailers~\cite{DeCandia:2007:DAH:1323293.1294281}.

	However, if you are going to shard on a per-socket basis within
	a multisocket system, why not buy separate smaller and cheaper
	single-socket systems, and then run one shard of the database
	on each of those systems?

\QuickQ{}
	But if elements in a hash table can be deleted concurrently
	with lookups, doesn't that mean that a lookup could return
	a reference to a data element that was deleted immediately
	after it was looked up?
\QuickA{}
	Yes it can!
	This is why \co{hashtab_lookup()} must be invoked within an
	RCU read-side critical section, and it is why
	\co{hashtab_add()} and \co{hashtab_del()} must also use
	RCU-aware list-manipulation primitives.
	Finally, this is why the caller of \co{hashtab_del()} must
	wait for a grace period (e.g., by calling \co{synchronize_rcu()})
	before freeing the deleted element.

\QuickQ{}
	The dangers of extrapolating from eight CPUs to 60 CPUs was
	made quite clear in
	Section~\ref{sec:datastruct:Hash-Table Performance}.
	But why should extrapolating up from 60 CPUs be any safer?
\QuickA{}
	It isn't any safer, and a useful exercise would be to run these
	programs on larger systems.
	That said, other testing has shown that RCU read-side primitives
	offer consistent performance and scalability up to at least 1024 CPUs.

\QuickQ{}
	The code in
	Figure~\ref{fig:datastruct:Resizable Hash-Table Bucket Selection}
	computes the hash twice!
	Why this blatant inefficiency?
\QuickA{}
	The reason is that the old and new hash tables might have
	completely different hash functions, so that a hash computed
	for the old table might be completely irrelevant to the
	new table.

\QuickQ{}
	How does the code in
	Figure~\ref{fig:datastruct:Resizable Hash-Table Bucket Selection}
	protect against the resizing process progressing past the
	selected bucket?
\QuickA{}
	It does not provide any such protection.
	That is instead the job of the update-side concurrency-control
	functions described next.

\QuickQ{}
	The code in
	Figures~\ref{fig:datastruct:Resizable Hash-Table Bucket Selection}
	and~\ref{fig:datastruct:Resizable Hash-Table Update-Side Concurrency Control}
	computes the hash and executes the bucket-selection logic twice for
	updates!
	Why this blatant inefficiency?
\QuickA{}
	This approach allows the \co{hashtorture.h} testing infrastructure
	to be reused.
	That said, a production-quality resizable hash table would likely
	be optimized to avoid this double computation.
	Carrying out this optimization is left as an exercise for the reader.

\QuickQ{}
	Suppose that one thread is inserting an element into the
	new hash table during a resize operation.
	What prevents this insertion from being lost due to a subsequent
	resize operation completing before the insertion does?
\QuickA{}
	The second resize operation will not be able to move beyond
	the bucket into which the insertion is taking place due to
	the insertion holding the lock on one of the hash buckets in
	the new hash table (the second hash table of three in this
	example).
	Furthermore, the insertion operation takes place within an
	RCU read-side critical section.
	As we will see when we examine the \co{hashtab_resize()}
	function, this means that the first resize operation will
	use
	\co{synchronize_rcu()} to wait for the insertion's read-side
	critical section to complete.

\QuickQ{}
	In the \co{hashtab_lookup()} function in
	Figure~\ref{fig:datastruct:Resizable Hash-Table Access Functions},
	the code carefully finds the right bucket in the new hash table
	if the element to be looked up has already been distributed
	by a concurrent resize operation.
	This seems wasteful for RCU-protected lookups.
	Why not just stick with the old hash table in this case?
\QuickA{}
	Suppose that a resize operation begins and distributes half of
	the old table's buckets to the new table.
	Suppose further that a thread adds a new element that goes into
	one of the already-distributed buckets, and that this same thread
	now looks up this newly added element.
	If lookups unconditionally traversed only the old hash table,
	this thread would get a lookup failure for the element that it
	just added, which certainly sounds like a bug to me!

\QuickQ{}
	The \co{hashtab_del()} function in
	Figure~\ref{fig:datastruct:Resizable Hash-Table Access Functions}
	does not always remove the element from the old hash table.
	Doesn't this mean that readers might access this newly removed
	element after it has been freed?
\QuickA{}
	No.
	The \co{hashtab_del()} function omits removing the element
	from the old hash table only if the resize operation has
	already progressed beyond the bucket containing the just-deleted
	element.
	But this means that new \co{hashtab_lookup()} operations will
	use the new hash table when looking up that element.
	Therefore, only old \co{hashtab_lookup()} operations that started
	before the \co{hashtab_del()} might encounter the newly
	removed element.
	This means that \co{hashtab_del()} need only wait for an
	RCU grace period to avoid inconveniencing
	\co{hashtab_lookup()} operations.

\QuickQ{}
	In the \co{hashtab_resize()} function in
	Figure~\ref{fig:datastruct:Resizable Hash-Table Access Functions},
	what guarantees that the update to \co{->ht_new} on line~29
	will be seen as happening before the update to \co{->ht_resize_cur}
	on line~36 from the perspective of \co{hashtab_lookup()},
	\co{hashtab_add()}, and \co{hashtab_del()}?
\QuickA{}
	The \co{synchronize_rcu()} on line~30 of
	Figure~\ref{fig:datastruct:Resizable Hash-Table Access Functions}
	ensures that all pre-existing RCU readers have completed between
	the time that we install the new hash-table reference on
	line~29 and the time that we update \co{->ht_resize_cur} on
	line~36.
	This means that any reader that sees a non-negative value
	of \co{->ht_resize_cur} cannot have started before the
	assignment to \co{->ht_new}, and thus must be able to see
	the reference to the new hash table.

\QuickQ{}
	Couldn't the \co{hashtorture.h} code be modified to accommodate
	a version of \co{hashtab_lock_mod()} that subsumes the
	\co{ht_get_bucket()} functionality?
\QuickA{}
	It probably could, and doing so would benefit all of the
	per-bucket-locked hash tables presented in this chapter.
	Making this modification is left as an exercise for the
	reader.

\QuickQ{}
	How much do these specializations really save?
	Are they really worth it?
\QuickA{}
	The answer to the first question is left as an exercise to
	the reader.
	Try specializing the resizable hash table and see how much
	performance improvement results.
	The second question cannot be answered in general, but must
	instead be answered with respect to a specific use case.
	Some use cases are extremely sensitive to performance and
	scalability, while others are less so.

\QuickQAC{chp:Validation}{Validation}
\QuickQ{}
	When in computing is the willingness to follow a fragmentary
	plan critically important?
\QuickA{}
	There are any number of situations, but perhaps the most important
	situation is when no one has ever created anything resembling
	the program to be developed.
	In this case, the only way to create a credible plan is to
	implement the program, create the plan, and implement it a
	second time.
	But whoever implements the program for the first time has no
	choice but to follow a fragmentary plan because any detailed
	plan created in ignorance cannot survive first contact with
	the real world.

	And perhaps this is one reason why evolution has favored insanely
	optimistic human beings who are happy to follow fragmentary plans!

\QuickQ{}
	Suppose that you are writing a script that processes the
	output of the \co{time} command, which looks as follows:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\tt
	\scriptsize
	\begin{verbatim}
		real    0m0.132s
		user    0m0.040s
		sys     0m0.008s
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	The script is required to check its input for errors, and to
	give appropriate diagnostics if fed erroneous \co{time} output.
	What test inputs should you provide to this program to test it
	for use with \co{time} output generated by single-threaded programs?
\QuickA{}
	\begin{enumerate}
	\item	Do you have a test case in which all the time is
		consumed in user mode by a CPU-bound program?
	\item	Do you have a test case in which all the time is
		consumed in system mode by a CPU-bound program?
	\item	Do you have a test case in which all three times
		are zero?
	\item	Do you have a test case in which the ``user'' and ``sys''
		times sum to more than the ``real'' time?
		(This would of course be completely legitimate in
		a multithreaded program.)
	\item	Do you have a set of tests cases in which one of the
		times uses more than one second?
	\item	Do you have a set of tests cases in which one of the
		times uses more than ten second?
	\item	Do you have a set of test cases in which one of the
		times has non-zero minutes?  (For example, ``15m36.342s''.)
	\item	Do you have a set of test cases in which one of the
		times has a seconds value of greater than 60?
	\item	Do you have a set of test cases in which one of the
		times overflows 32 bits of milliseconds?  64 bits of
		milliseconds?
	\item	Do you have a set of test cases in which one of the
		times is negative?
	\item	Do you have a set of test cases in which one of the
		times has a positive minutes value but a negative
		seconds value?
	\item	Do you have a set of test cases in which one of the
		times omits the ``m'' or the ``s''?
	\item	Do you have a set of test cases in which one of the
		times is non-numeric?  (For example, ``Go Fish''.)
	\item	Do you have a set of test cases in which one of the
		lines is omitted?  (For example, where there is a
		``real'' value and a ``sys'' value, but no ``user''
		value.)
	\item	Do you have a set of test cases where one of the
		lines is duplicated?  Or duplicated, but with a
		different time value for the duplicate?
	\item	Do you have a set of test cases where a given line
		has more than one time value?  (For example,
		``real 0m0.132s 0m0.008s''.)
	\item	Do you have a set of test cases containing random
		characters?
	\item	In all test cases involving invalid input, did you
		generate all permutations?
	\item	For each test case, do you have an expected outcome
		for that test?
	\end{enumerate}

	If you did not generate test data for a substantial number of
	the above cases, you will need to cultivate a more destructive
	attitude in order to have a chance of generating high-quality
	tests.

	Of course, one way to economize on destructiveness is to
	generate the tests with the to-be-tested source code at hand,
	which is called white-box testing (as opposed to black-box testing).
	However, this is no panacea: You will find that it is all too
	easy to find your thinking limited by what the program can handle,
	thus failing to generate truly destructive inputs.

\QuickQ{}
	You are asking me to do all this validation BS before
	I even start coding???
	That sounds like a great way to never get started!!!
\QuickA{}
	If it is your project, for example, a hobby, do what you like.
	Any time you waste will be your own, and you have no one else
	to answer to for it.
	And there is a good chance that the time will not be completely
	wasted.
	For example, if you are embarking on a first-of-a-kind project,
	the requirements are in some sense unknowable anyway.
	In this case, the best approach might be to quickly prototype
	a number of rough solutions, try them out, and see what works
	best.

	On the other hand, if you are being paid to produce a system that
	is broadly similar to existing systems, you owe it to your users,
	your employer, and your future self to validate early and often.

\QuickQ{}
	How can you implement \co{WARN_ON_ONCE()}?
\QuickA{}
	If you don't mind having a \co{WARN_ON_ONCE()} that
	will sometimes warn twice or three times, simply maintain
	a static variable that is initialized to zero.
	If the condition triggers, check the static variable, and
	if it is non-zero, return.
	Otherwise, set it to one, print the message, and return.

	If you really need the message to never appear more than once,
	perhaps because it is huge, you can use an atomic exchange
	operation in place of ``set it to one'' above.
	Print the message only if the atomic exchange operation returns
	zero.

\QuickQ{}
	Why would anyone bother copying existing code in pen on paper???
	Doesn't that just increase the probability of transcription errors?
\QuickA{}
	If you are worried about transcription errors, please allow me
	to be the first to introduce you to a really cool tool named
	\co{diff}.
	In addition, carrying out the copying can be quite valuable:
	\begin{enumerate}
	\item	If you are copying a lot of code, you are probably failing
		to take advantage of an opportunity for abstraction.
		The act of copying code can provide great motivation
		for abstraction.
	\item	Copying the code gives you an opportunity to think about
		whether the code really works in its new setting.
		Is there some non-obvious constraint, such as the need
		to disable interrupts or to hold some lock?
	\item	Copying the code also gives you time to consider whether
		there is some better way to get the job done.
	\end{enumerate}
	So, yes, copy the code!

\QuickQ{}
	This procedure is ridiculously over-engineered!
	How can you expect to get a reasonable amount of software
	written doing it this way???
\QuickA{}
	Indeed, repeatedly copying code by hand is laborious and slow.
	However, when combined with heavy-duty stress testing and
	proofs of correctness, this approach is also extremely effective
	for complex parallel code where ultimate performance and
	reliability are required and where debugging is difficult.
	The Linux-kernel RCU implementation is a case in point.

	On the other hand, if you are writing a simple single-threaded
	shell script to manipulate some data, then you would be
	best-served by a different methodology.
	For example, you might enter each command one at a time
	into an interactive shell with a test data set to make
	sure that it did what you wanted, then copy-and-paste the
	successful commands into your script.
	Finally, test the script as a whole.

	If you have a friend or colleague who is willing to help out,
	pair programming can work very well, as can any number of
	formal design- and code-review processes.

	And if you are writing code as a hobby, then do whatever you like.

	In short, different types of software need different development
	methodologies.

\QuickQ{}
	Suppose that you had a very large number of systems at your
	disposal.
	For example, at current cloud prices, you can purchase a
	huge amount of CPU time at a reasonably low cost.
	Why not use this approach to get close enough to certainty
	for all practical purposes?
\QuickA{}
	This approach might well be a valuable addition to your
	validation arsenal.
	But it does have a few limitations:
	\begin{enumerate}
	\item	Some bugs have extremely low probabilities of occurrence,
		but nevertheless need to be fixed.
		For example, suppose that the Linux kernel's RCU
		implementation had a bug that is triggered only once
		per century of machine time on average.
		A century of CPU time is hugely expensive even on
		the cheapest cloud platforms, but we could expect
		this bug to result in more than 2,000 failures per day
		on the more than 100 million Linux instances in the
		world as of 2011.
	\item	The bug might well have zero probability of occurrence
		on your test setup, which means that you won't see it
		no matter how much machine time you burn testing it.
	\end{enumerate}
	Of course, if your code is small enough, formal validation
	may be helpful, as discussed in
	Section~\ref{chp:formal:Formal Verification}.
	But beware: formal validation of your code will not find
	errors in your assumptions, misunderstanding of the
	requirements, misunderstanding of the software or hardware
	primitives you use, or errors that you did not think to construct
	a proof for.

\QuickQ{}
	Say what???
	When I plug the earlier example of five tests each with a
	10\% failure rate into the formula, I get 59,050\% and that
	just doesn't make sense!!!
\QuickA{}
	You are right, that makes no sense at all.

	Remember that a probability is a number between zero and one,
	so that you need to divide a percentage by 100 to get a
	probability.
	So 10\% is a probability of 0.1, which gets a probability
	of 0.4095, which rounds to 41\%, which quite sensibly
	matches the earlier result.

\QuickQ{}
	In Equation~\ref{eq:debugging:Binomial Number of Tests Required},
	are the logarithms base-10, base-2, or base-$e$?
\QuickA{}
	It does not matter.
	You will get the same answer no matter what base of logarithms
	you use because the result is a pure ratio of logarithms.
	The only constraint is that you use the same base for both
	the numerator and the denominator.

\QuickQ{}
	Suppose that a bug causes a test failure three times per hour
	on average.
	How long must the test run error-free to provide 99.9\%
	confidence that the fix significantly reduced the probability
	of failure?
\QuickA{}
	We set $n$ to $3$ and $P$ to $99.9$ in
	Equation~\ref{eq:debugging:Error-Free Test Duration}, resulting in:

	\begin{equation}
		T = - \frac{1}{3} \log \frac{100 - 99.9}{100} = 2.3
	\end{equation}

	If the test runs without failure for 2.3 hours, we can be 99.9\%
	certain that the fix reduced the probability of failure.

\QuickQ{}
	Doing the summation of all the factorials and exponentials
	is a real pain.
	Isn't there an easier way?
\QuickA{}
	One approach is to use the open-source symbolic manipulation
	program named ``maxima''.
	Once you have installed this program, which is a part of many
	Debian-based Linux distributions, you can run it and give the
	\co{load(distrib);} command followed by any number of
	\co{bfloat(cdf_poisson(m,l));} commands, where the \co{m}
	is replaced by the desired value of $m$ and the \co{l}
	is replaced by the desired value of $\lambda$.

	In particular, the \co{bfloat(cdf_poisson(2,24));} command
	results in \co{1.181617112359357b-8}, which matches the value
	given by Equation~\ref{eq:debugging:Possion CDF}.

	Alternatively, you can use the rough-and-ready method described in
	Section~\ref{sec:debugging:Abusing Statistics for Discrete Testing}.

\QuickQ{}
	But wait!!!
	Given that there has to be \emph{some} number of failures
	(including the possibility of zero failures),
	shouldn't the summation shown in
	Equation~\ref{eq:debugging:Possion CDF}
	approach the value $1$ as $m$ goes to infinity?
\QuickA{}
	Indeed it should.
	And it does.

	To see this, note that $e^{-\lambda}$ does not depend on $i$,
	which means that it can be pulled out of the summation as follows:

	\begin{equation}
		e^{-\lambda} \sum_{i=0}^\infty \frac{\lambda^i}{i!}
	\end{equation}

	The remaining summation is exactly the Taylor series for
	$e^\lambda$, yielding:

	\begin{equation}
		e^{-\lambda} e^\lambda
	\end{equation}

	The two exponentials are reciprocals, and therefore cancel,
	resulting in exactly $1$, as required.

\QuickQ{}
	How is this approach supposed to help if the corruption affected some
	unrelated pointer, which then caused the corruption???
\QuickA{}
	Indeed, that can happen.
	Many CPUs have hardware-debugging facilities that can help you
	locate that unrelated pointer.
	Furthermore, if you have a core dump, you can search the core
	dump for pointers referencing the corrupted region of memory.
	You can also look at the data layout of the corruption, and
	check pointers whose type matches that layout.

	You can also step back and test the modules making up your
	program more intensively, which will likely confine the corruption
	to the module responsible for it.
	If this makes the corruption vanish, consider adding additional
	argument checking to the functions exported from each module.

	Nevertheless, this is a hard problem, which is why I used the
	words ``a bit of a dark art''.

\QuickQ{}
	But I did the bisection, and ended up with a huge commit.
	What do I do now?
\QuickA{}
	A huge commit?
	Shame on you!
	This is but one reason why you are supposed to keep the commits small.

	And that is your answer: Break up the commit into bite-sized
	pieces and bisect the pieces.
	In my experience, the act of breaking up the commit is often
	sufficient to make the bug painfully obvious.

\QuickQ{}
	Why don't existing conditional-locking primitives provide this
	spurious-failure functionality?
\QuickA{}
	There are locking algorithms that depend on conditional-locking
	primitives telling them the truth.
	For example, if conditional-lock failure signals that
	some other thread is already working on a given job,
	spurious failure might cause that job to never get done,
	possibly resulting in a hang.

\QuickQ{}
	That is ridiculous!!!
	After all, isn't getting the correct answer later than one would like
	better than getting an incorrect answer???
\QuickA{}
	This question fails to consider the option of choosing not to
	compute the answer at all, and in doing so, also fails to consider
	the costs of computing the answer.
	For example, consider short-term weather forecasting, for which
	accurate models exist, but which require large (and expensive)
	clustered supercomputers, at least if you want to actually run
	the model faster than the weather.

	And in this case, any performance bug that prevents the model from
	running faster than the actual weather prevents any forecasting.
	Given that the whole purpose of purchasing the large clustered
	supercomputer was to forecast weather, if you cannot run the
	model faster than the weather, you would be better off not running
	the model at all.

	More severe examples may be found in the area of safety-critical
	real-time computing.

\QuickQ{}
	But if you are going to put in all the hard work of parallelizing
	an application, why not do it right?
	Why settle for anything less than optimal performance and
	linear scalability?
\QuickA{}
	Although I do heartily salute your spirit and aspirations,
	you are forgetting that there may be high costs due to delays
	in the program's completion.
	For an extreme example, suppose that a 40\% performance shortfall
	from a single-threaded application is causing one person to die
	each day.
	Suppose further that in a day you could hack together a
	quick and dirty
	parallel program that ran 50\% faster on an eight-CPU system
	than the sequential version, but that an optimal parallel
	program would require four months of painstaking design, coding,
	debugging, and tuning.

	It is safe to say that more than 100 people would prefer the
	quick and dirty version.

\QuickQ{}
	But what about other sources of error, for example, due to
	interactions between caches and memory layout?
\QuickA{}
	Changes in memory layout can indeed result in unrealistic
	decreases in execution time.
	For example, suppose that a given microbenchmark almost
	always overflows the L0 cache's associativity, but with just the right
	memory layout, it all fits.
	If this is a real concern, consider running your microbenchmark
	using huge pages (or within the kernel or on bare metal) in
	order to completely control the memory layout.

\QuickQ{}
	Wouldn't the techniques suggested to isolate the code under
	test also affect that code's performance, particularly if
	it is running within a larger application?
\QuickA{}
	Indeed it might, although in most microbenchmarking efforts
	you would extract the code under test from the enclosing
	application.
	Nevertheless, if for some reason you must keep the code under
	test within the application, you will very likely need to use
	the techniques discussed in
	Section~\ref{sec:debugging:Detecting Interference}.

\QuickQ{}
	This approach is just plain weird!
	Why not use means and standard deviations, like we were taught
	in our statistics classes?
\QuickA{}
	Because mean and standard deviation were not designed to do this job.
	To see this, try applying mean and standard deviation to the
	following data set, given a 1\% relative error in measurement:

	\begin{quote}
		49,548.4 49,549.4 49,550.2 49,550.9 49,550.9 49,551.0
		49,551.5 49,552.1 49,899.0 49,899.3 49,899.7 49,899.8
		49,900.1 49,900.4 52,244.9 53,333.3 53,333.3 53,706.3
		53,706.3 54,084.5
	\end{quote}

	The problem is that mean and standard deviation do not rest on
	any sort of measurement-error assumption, and they will therefore
	see the difference between the values near 49,500 and those near
	49,900 as being statistically significant, when in fact they are
	well within the bounds of estimated measurement error.

	Of course, it is possible to create a script similar to
	that in
	Figure~\ref{fig:count:Statistical Elimination of Interference}
	that uses standard deviation rather than absolute difference
	to get a similar effect,
	and this is left as an exercise for the interested reader.
	Be careful to avoid divide-by-zero errors arising from strings
	of identical data values!

\QuickQ{}
	But what if all the y-values in the trusted group of data
	are exactly zero?
	Won't that cause the script to reject any non-zero value?
\QuickA{}
	Indeed it will!
	But if your performance measurements often produce a value of
	exactly zero, perhaps you need to take a closer look at your
	performance-measurement code.

	Note that many approaches based on mean and standard deviation
	will have similar problems with this sort of dataset.

\QuickQAC{chp:formal:Formal Verification}{Formal Verification}
\QuickQ{}
	Why is there an unreached statement in
	locker?  After all, isn't this a \emph{full} state-space
	search?
\QuickA{}
	The locker process is an infinite loop, so control
	never reaches the end of this process.
	However, since there are no monotonically increasing variables,
	Promela is able to model this infinite loop with a small
	number of states.

\QuickQ{}
	What are some Promela code-style issues with this example?
\QuickA{}
	There are several:
	\begin{enumerate}
	\item	The declaration of {\tt sum} should be moved to within
		the init block, since it is not used anywhere else.
	\item	The assertion code should be moved outside of the
		initialization loop.  The initialization loop can
		then be placed in an atomic block, greatly reducing
		the state space (by how much?).
	\item	The atomic block covering the assertion code should
		be extended to include the initialization of {\tt sum}
		and {\tt j}, and also to cover the assertion.
		This also reduces the state space (again, by how
		much?).
	\end{enumerate}

\QuickQ{}
	Is there a more straightforward way to code the do-od statement?
\QuickA{}
	Yes.
	Replace it with {\tt if-fi} and remove the two {\tt break} statements.

\QuickQ{}
	Why are there atomic blocks at lines 12-21
	and lines 44-56, when the operations within those atomic
	blocks have no atomic implementation on any current
	production microprocessor?
\QuickA{}
	Because those operations are for the benefit of the
	assertion only.  They are not part of the algorithm itself.
	There is therefore no harm in marking them atomic, and
	so marking them greatly reduces the state space that must
	be searched by the Promela model.

\QuickQ{}
	Is the re-summing of the counters on lines 24-27
	\emph{really} necessary?
\QuickA{}
	Yes.  To see this, delete these lines and run the model.

	Alternatively, consider the following sequence of steps:

	\begin{enumerate}
	\item	One process is within its RCU read-side critical
		section, so that the value of {\tt ctr[0]} is zero and
		the value of {\tt ctr[1]} is two.
	\item	An updater starts executing, and sees that the sum of
		the counters is two so that the fastpath cannot be
		executed.  It therefore acquires the lock.
	\item	A second updater starts executing, and fetches the value
		of {\tt ctr[0]}, which is zero.
	\item	The first updater adds one to {\tt ctr[0]}, flips
		the index (which now becomes zero), then subtracts
		one from {\tt ctr[1]} (which now becomes one).
	\item	The second updater fetches the value of {\tt ctr[1]},
		which is now one.
	\item	The second updater now incorrectly concludes that it
		is safe to proceed on the fastpath, despite the fact
		that the original reader has not yet completed.
	\end{enumerate}

\QuickQ{}
	Given that we have two independent proofs of correctness for
	the QRCU algorithm described herein, and given that the
	proof of incorrectness covers what is likely a different
	algorithm, why is there any room for doubt?
\QuickA{}
	There is always room for doubt.
	In this case, it is important to keep in mind that the two proofs
	of correctness preceded the formalization of real-world memory
	models, raising the possibility that these two proofs are based
	on incorrect memory-ordering assumptions.
	Furthermore, since both proofs were constructed by the same person,
	it is quite possible that they contain a common error.
	Again, there is always room for doubt.

\QuickQ{}
	Yeah, that's just great!
	Now, just what am I supposed to do if I don't happen to have a
	machine with 40GB of main memory???
\QuickA{}
	Relax, there are a number of lawful answers to
	this question:
	\begin{enumerate}
	\item	Further optimize the model, reducing its memory consumption.
	\item	Work out a pencil-and-paper proof, perhaps starting with the
		comments in the code in the Linux kernel.
	\item	Devise careful torture tests, which, though they cannot prove
		the code correct, can find hidden bugs.
	\item	There is some movement towards tools that do model
		checking on clusters of smaller machines.
		However, please note that we have not actually used such
		tools myself, courtesy of some large machines that Paul has
		occasional access to.
	\item	Wait for memory sizes of affordable systems to expand
		to fit your problem.
	\item	Use one of a number of cloud-computing services to rent
		a large system for a short time period.
	\end{enumerate}

\QuickQ{}
	Why not simply increment \co{rcu_update_flag}, and then only
	increment \co{dynticks_progress_counter} if the old value
	of \co{rcu_update_flag} was zero???
\QuickA{}
	This fails in presence of NMIs.
	To see this, suppose an NMI was received just after
	\co{rcu_irq_enter()} incremented \co{rcu_update_flag},
	but before it incremented \co{dynticks_progress_counter}.
	The instance of \co{rcu_irq_enter()} invoked by the NMI
	would see that the original value of \co{rcu_update_flag}
	was non-zero, and would therefore refrain from incrementing
	\co{dynticks_progress_counter}.
	This would leave the RCU grace-period machinery no clue that the
	NMI handler was executing on this CPU, so that any RCU read-side
	critical sections in the NMI handler would lose their RCU protection.

	The possibility of NMI handlers, which, by definition cannot
	be masked, does complicate this code.

\QuickQ{}
	But if line~7 finds that we are the outermost interrupt,
	wouldn't we \emph{always} need to increment
	\co{dynticks_progress_counter}?
\QuickA{}
	Not if we interrupted a running task!
	In that case, \co{dynticks_progress_counter} would
	have already been incremented by \co{rcu_exit_nohz()},
	and there would be no need to increment it again.

\QuickQ{}
	Can you spot any bugs in any of the code in this section?
\QuickA{}
	Read the next section to see if you were correct.

\QuickQ{}
	Why isn't the memory barrier in \co{rcu_exit_nohz()}
	and \co{rcu_enter_nohz()} modeled in Promela?
\QuickA{}
	Promela assumes sequential consistency, so
	it is not necessary to model memory barriers.
	In fact, one must instead explicitly model lack of memory barriers,
	for example, as shown in
	Figure~\ref{fig:analysis:QRCU Unordered Summation} on
	page~\pageref{fig:analysis:QRCU Unordered Summation}.

\QuickQ{}
	Isn't it a bit strange to model \co{rcu_exit_nohz()}
	followed by \co{rcu_enter_nohz()}?
	Wouldn't it be more natural to instead model entry before exit?
\QuickA{}
	It probably would be more natural, but we will need
	this particular order for the liveness checks that we will add later.

\QuickQ{}
	Wait a minute!
	In the Linux kernel, both \co{dynticks_progress_counter} and
	\co{rcu_dyntick_snapshot} are per-CPU variables.
	So why are they instead being modeled as single global variables?
\QuickA{}
	Because the grace-period code processes each
	CPU's \co{dynticks_progress_counter} and
	\co{rcu_dyntick_snapshot} variables separately,
	we can collapse the state onto a single CPU.
	If the grace-period code were instead to do something special
	given specific values on specific CPUs, then we would indeed need
	to model multiple CPUs.
	But fortunately, we can safely confine ourselves to two CPUs, the
	one running the grace-period processing and the one entering and
	leaving dynticks-idle mode.

\QuickQ{}
	Given there are a pair of back-to-back changes to
	\co{gp_state} on lines~25 and 26,
	how can we be sure that line~25's changes won't be lost?
\QuickA{}
	Recall that Promela and spin trace out
	every possible sequence of state changes.
	Therefore, timing is irrelevant: Promela/spin will be quite
	happy to jam the entire rest of the model between those two
	statements unless some state variable specifically prohibits
	doing so.

\QuickQ{}
	But what would you do if you needed the statements in a single
	\co{EXECUTE_MAINLINE()} group to execute non-atomically?
\QuickA{}
	The easiest thing to do would be to put
	each such statement in its own \co{EXECUTE_MAINLINE()}
	statement.

\QuickQ{}
	But what if the \co{dynticks_nohz()} process had
	``if'' or ``do'' statements with conditions,
	where the statement bodies of these constructs
	needed to execute non-atomically?
\QuickA{}
	One approach, as we will see in a later section,
	is to use explicit labels and ``goto'' statements.
	For example, the construct:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
	\begin{verbatim}
		if
		:: i == 0 -> a = -1;
		:: else -> a = -2;
		fi;
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	could be modeled as something like:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
	\begin{verbatim}
		EXECUTE_MAINLINE(stmt1,
				 if
				 :: i == 0 -> goto stmt1_then;
				 :: else -> goto stmt1_else;
				 fi)
		stmt1_then: skip;
		EXECUTE_MAINLINE(stmt1_then1, a = -1; goto stmt1_end)
		stmt1_else: skip;
		EXECUTE_MAINLINE(stmt1_then1, a = -2)
		stmt1_end: skip;
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	However, it is not clear that the macro is helping much in the case
	of the ``if'' statement, so these sorts of situations will
	be open-coded in the following sections.

\QuickQ{}
	Why are lines~45 and 46 (the \co{in_dyntick_irq = 0;}
	and the \co{i++;}) executed atomically?
\QuickA{}
	These lines of code pertain to controlling the
	model, not to the code being modeled, so there is no reason to
	model them non-atomically.
	The motivation for modeling them atomically is to reduce the size
	of the state space.

\QuickQ{}
	What property of interrupts is this \co{dynticks_irq()}
	process unable to model?
\QuickA{}
	One such property is nested interrupts,
	which are handled in the following section.

\QuickQ{}
	Does Paul \emph{always} write his code in this painfully incremental
	manner?
\QuickA{}
	Not always, but more and more frequently.
	In this case, Paul started with the smallest slice of code that
	included an interrupt handler, because he was not sure how best
	to model interrupts in Promela.
	Once he got that working, he added other features.
	(But if he was doing it again, he would start with a ``toy'' handler.
	For example, he might have the handler increment a variable twice and
	have the mainline code verify that the value was always even.)

	Why the incremental approach?
	Consider the following, attributed to Brian W. Kernighan:

	\begin{quote}
		Debugging is twice as hard as writing the code in the first
		place. Therefore, if you write the code as cleverly as possible,
		you are, by definition, not smart enough to debug it.
	\end{quote}

	This means that any attempt to optimize the production of code should
	place at least 66\% of its emphasis on optimizing the debugging process,
	even at the expense of increasing the time and effort spent coding.
	Incremental coding and testing is one way to optimize the debugging
	process, at the expense of some increase in coding effort.
	Paul uses this approach because he rarely has the luxury of
	devoting full days (let alone weeks) to coding and debugging.

\QuickQ{}
	But what happens if an NMI handler starts running before
	an irq handler completes, and if that NMI handler continues
	running until a second irq handler starts?
\QuickA{}
	This cannot happen within the confines of a single CPU.
	The first irq handler cannot complete until the NMI handler
	returns.
	Therefore, if each of the \co{dynticks} and \co{dynticks_nmi}
	variables have taken on an even value during a given time
	interval, the corresponding CPU really was in a quiescent
	state at some time during that interval.

\QuickQ{}
	This is still pretty complicated.
	Why not just have a \co{cpumask_t} that has a bit set for
	each CPU that is in dyntick-idle mode, clearing the bit
	when entering an irq or NMI handler, and setting it upon
	exit?
\QuickA{}
	Although this approach would be functionally correct, it
	would result in excessive irq entry/exit overhead on
	large machines.
	In contrast, the approach laid out in this section allows
	each CPU to touch only per-CPU data on irq and NMI entry/exit,
	resulting in much lower irq entry/exit overhead, especially
	on large machines.

\QuickQ{}
	But x86 has strong memory ordering!  Why would you need to
	formalize its memory model?
\QuickA{}
	Actually, academics consider the x86 memory model to be weak
	because it can allow prior stores to be reordered with
	subsequent loads.
	From an academic viewpoint, a strong memory model is one
	that allows absolutely no reordering, so that all threads
	agree on the order of all operations visible to them.

\QuickQ{}
	Why does line~8
	of Figure~\ref{fig:sec:formal:PPCMEM Litmus Test}
	initialize the registers?
	Why not instead initialize them on lines~4 and~5?
\QuickA{}
	Either way works.
	However, in general, it is better to use initialization than
	explicit instructions.
	The explicit instructions are used in this example to demonstrate
	their use.
	In addition, many of the litmus tests available on the tool's
	web site (\url{http://www.cl.cam.ac.uk/~pes20/ppcmem/}) were
	automatically generated, which generates explicit
	initialization instructions.

\QuickQ{}
	But whatever happened to line~17 of
	Figure~\ref{fig:sec:formal:PPCMEM Litmus Test},
	the one that is the \co{Fail:} label?
\QuickA{}
	The implementation of powerpc version of \co{atomic_add_return()}
	loops when the \co{stwcx} instruction fails, which it communicates
	by setting non-zero status in the condition-code register,
	which in turn is tested by the bne instruction. Because actually
	modeling the loop would result in state-space explosion, we
	instead branch to the Fail: label, terminating the model with
	the initial value of 2 in thread~1's \co{r3} register, which
	will not trigger the exists assertion.

	There is some debate about whether this trick is universally
	applicable, but I have not seen an example where it fails.

\QuickQ{}
	Does the ARM Linux kernel have a similar bug?
\QuickA{}
	ARM does not have this particular bug because that it places
	\co{smp_mb()} before and after the \co{atomic_add_return()}
	function's assembly-language implementation.
	PowerPC no longer has this bug; it has long since been fixed.
	Finding any other bugs that the Linux kernel might have is left
	as an exercise for the reader.

\QuickQ{}
	In light of the full verification of the L4 microkernel,
	isn't this limited view of formal verification just a little
	bit obsolete?
\QuickA{}
	Unfortunately, no.

	The full verification of the L4 microkernel was a tour de force,
	with a large number of Ph.D.~students hand-verifying code at a
	very slow per-student rate.
	This level of effort could not be applied to most software projects
	because the rate of change is just too great.
	Furthermore, although the L4 microkernel is a large software
	artifact from the viewpoint of formal verification, it is tiny
	compared to the a great number of projects, including LLVM,
	gcc, the Linux kernel, Hadoop, MongoDB, and a great many others.

	Although formal verification is finally starting to show some
	promise, it currently has no chance of completely displacing testing
	in the foreseeable future.
	And although I would dearly love to be proven wrong on this point,
	please note that such a proof will be in the form of a real tool
	that verifies real software, not in the form of a large body of
	rousing rhetoric.

\QuickQAC{chp:Putting It All Together}{Putting It All Together}
\QuickQ{}
	Why on earth did we need that global lock in the first place?
\QuickA{}
	A given thread's \co{__thread} variables vanish when that
	thread exits.
	It is therefore necessary to synchronize any operation that
	accesses other threads' \co{__thread} variables with
	thread exit.
	Without such synchronization, accesses to \co{__thread} variable
	of a just-exited thread will result in segmentation faults.

\QuickQ{}
	Just what is the accuracy of \co{read_count()}, anyway?
\QuickA{}
	Refer to
	Figure~\ref{fig:count:Per-Thread Statistical Counters} on
	Page~\pageref{fig:count:Per-Thread Statistical Counters}.
	Clearly, if there are no concurrent invocations of \co{inc_count()},
	\co{read_count()} will return an exact result.
	However, if there \emph{are} concurrent invocations of
	\co{inc_count()}, then the sum is in fact changing as
	\co{read_count()} performs its summation.
	That said, because thread creation and exit are excluded by
	\co{final_mutex}, the pointers in \co{counterp} remain constant.

	Let's imagine a mythical machine that is able to take an
	instantaneous snapshot of its memory.
	Suppose that this machine takes such a snapshot at the
	beginning of \co{read_count()}'s execution, and another
	snapshot at the end of \co{read_count()}'s execution.
	Then \co{read_count()} will access each thread's counter
	at some time between these two snapshots, and will therefore
	obtain a result that is bounded by those of the two snapshots,
	inclusive.
	The overall sum will therefore be bounded by the pair of sums that
	would have been obtained from each of the two snapshots (again,
	inclusive).

	The expected error is therefore half of the difference between
	the pair of sums that would have been obtained from each of the
	two snapshots, that is to say, half of the execution time of
	\co{read_count()} multiplied by the number of expected calls to
	\co{inc_count()} per unit time.

	Or, for those who prefer equations:
	\begin{equation}
	\epsilon = \frac{T_r R_i}{2}
	\end{equation}
	where $\epsilon$ is the expected error in \co{read_count()}'s
	return value,
	$T_r$ is the time that \co{read_count()} takes to execute,
	and $R_i$ is the rate of \co{inc_count()} calls per unit time.
	(And of course, $T_r$ and $R_i$ should use the same units of
	time: microseconds and calls per microsecond, seconds and calls
	per second, or whatever, as long as they are the same units.)

\QuickQ{}
	Hey!!!
	Line~45 of
	Figure~\ref{fig:together:RCU and Per-Thread Statistical Counters}
	modifies a value in a pre-existing \co{countarray} structure!
	Didn't you say that this structure, once made available to
	\co{read_count()}, remained constant???
\QuickA{}
	Indeed I did say that.
	And it would be possible to make \co{count_register_thread()}
	allocate a new structure, much as \co{count_unregister_thread()}
	currently does.

	But this is unnecessary.
	Recall the derivation of the error bounds of \co{read_count()}
	that was based on the snapshots of memory.
	Because new threads start with initial \co{counter} values of
	zero, the derivation holds even if we add a new thread partway
	through \co{read_count()}'s execution.
	So, interestingly enough, when adding a new thread, this
	implementation gets the effect of allocating a new structure,
	but without actually having to do the allocation.

\QuickQ{}
	Wow!
	Figure~\ref{fig:together:RCU and Per-Thread Statistical Counters}
	contains 69 lines of code, compared to only 42 in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}.
	Is this extra complexity really worth it?
\QuickA{}
	This of course needs to be decided on a case-by-case basis.
	If you need an implementation of \co{read_count()} that
	scales linearly, then the lock-based implementation shown in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}
	simply will not work for you.
	On the other hand, if calls to \co{count_read()} are sufficiently
	rare, then the lock-based version is simpler and might thus be
	better, although much of the size difference is due
	to the structure definition, memory allocation, and \co{NULL}
	return checking.

	Of course, a better question is ``why doesn't the language
	implement cross-thread access to \co{__thread} variables?''
	After all, such an implementation would make both the locking
	and the use of RCU unnecessary.
	This would in turn enable an implementation that
	was even simpler than the one shown in
	Figure~\ref{fig:count:Per-Thread Statistical Counters}, but
	with all the scalability and performance benefits of the
	implementation shown in
	Figure~\ref{fig:together:RCU and Per-Thread Statistical Counters}!

\QuickQ{}
	But cant't the approach shown in
	Figure~\ref{fig:together:Correlated Measurement Fields}
	result in extra cache misses, in turn resulting in additional
	read-side overhead?
\QuickA{}
	Indeed it can.

\begin{figure}[tbp]
{ \scriptsize
\begin{verbatim}
 1 struct measurement {
 2   double meas_1;
 3   double meas_2;
 4   double meas_3;
 5 };
 6 
 7 struct animal {
 8   char name[40];
 9   double age;
10   struct measurement *mp;
11   struct measurement meas;
12   char photo[0]; /* large bitmap. */
13 };
\end{verbatim}
}
\caption{Localized Correlated Measurement Fields}
\label{fig:together:Localized Correlated Measurement Fields}
\end{figure}

	One way to avoid this cache-miss overhead is shown in
	Figure~\ref{fig:together:Localized Correlated Measurement Fields}:
	Simply embed an instance of a \co{measurement} structure
	named \co{meas}
	into the \co{animal} structure, and point the \co{->mp}
	field at this \co{->meas} field.

	Measurement updates can then be carried out as follows:

	\begin{enumerate}
	\item	Allocate a new \co{measurement} structure and place
		the new measurements into it.
	\item	Use \co{rcu_assign_pointer()} to point \co{->mp} to
		this new structure.
	\item	Wait for a grace period to elapse, for example using
		either \co{synchronize_rcu()} or \co{call_rcu()}.
	\item	Copy the measurements from the new \co{measurement}
		structure into the embedded \co{->meas} field.
	\item	Use \co{rcu_assign_pointer()} to point \co{->mp}
		back to the old embedded \co{->meas} field.
	\item	After another grace period elapses, free up the
		new \co{measurement} field.
	\end{enumerate}

	This approach uses a heavier weight update procedure to eliminate
	the extra cache miss in the common case.
	The extra cache miss will be incurred only while an update is
	actually in progress.

\QuickQ{}
	But how does this scan work while a resizable hash table
	is being resized?
	In that case, neither the old nor the new hash table is
	guaranteed to contain all the elements in the hash table!
\QuickA{}
	True, resizable hash tables as described in
	Section~\ref{sec:datastruct:Non-Partitionable Data Structures}
	cannot be fully scanned while being resized.
	One simple way around this is to acquire the
	\co{hashtab} structure's \co{->ht_lock} while scanning,
	but this prevents more than one scan from proceeding
	concurrently.

	Another approach is for updates to mutate the old hash
	table as well as the new one while resizing is in
	progress.
	This would allow scans to find all elements in the old
	hash table.
	Implementing this is left as an exercise for the reader.

\QuickQAC{sec:advsync:Advanced Synchronization}{Advanced Synchronization}
\QuickQ{}
	How on earth could the assertion on line~21 of the code in
	Figure~\ref{fig:advsync:Parallel Hardware is Non-Causal} on
	page~\pageref{fig:advsync:Parallel Hardware is Non-Causal}
	\emph{possibly} fail?
\QuickA{}
	The key point is that the intuitive analysis missed is that
	there is nothing preventing the assignment to C from overtaking
	the assignment to A as both race to reach {\tt thread2()}.
	This is explained in the remainder of this section.

\QuickQ{}
	Great...  So how do I fix it?
\QuickA{}
	The easiest fix is to replace each of the \co{barrier()}s on
	line~12 and line~20 with an \co{smp_mb()}.

	Of course, some hardware is more forgiving than other hardware.
	For example, on x86 the assertion on line~21 of
	Figure~\ref{fig:advsync:Parallel Hardware is Non-Causal} on
	page~\pageref{fig:advsync:Parallel Hardware is Non-Causal}
	cannot trigger.
	On PowerPC, only the \co{barrier()} on line~20 need be
	replaced with \co{smp_mb()} to prevent the assertion from
	triggering.

\QuickQ{}
	What assumption is the code fragment
	in Figure~\ref{fig:advsync:Software Logic Analyzer}
	making that might not be valid on real hardware?
\QuickA{}
	The code assumes that as soon as a given CPU stops
	seeing its own value, it will immediately see the
	final agreed-upon value.
	On real hardware, some of the CPUs might well see several
	intermediate results before converging on the final value.

\QuickQ{}
	How could CPUs possibly have different views of the
	value of a single variable \emph{at the same time?}
\QuickA{}
	Many CPUs have write buffers that record the values of
	recent writes, which are applied once the corresponding
	cache line makes its way to the CPU.
	Therefore, it is quite possible for each CPU to see a
	different value for a given variable at a single point
	in time --- and for main memory to hold yet another value.
	One of the reasons that memory barriers were invented was
	to allow software to deal gracefully with situations like
	this one.

\QuickQ{}
	Why do CPUs~2 and 3 come to agreement so quickly, when it
	takes so long for CPUs~1 and 4 to come to the party?
\QuickA{}
	CPUs~2 and 3 are a pair of hardware threads on the same
	core, sharing the same cache hierarchy, and therefore have
	very low communications latencies.
	This is a NUMA, or, more accurately, a NUCA effect.

	This leads to the question of why CPUs~2 and 3 ever disagree
	at all.
	One possible reason is that they each might have a small amount
	of private cache in addition to a larger shared cache.
	Another possible reason is instruction reordering, given the
	short 10-nanosecond duration of the disagreement and the
	total lack of memory barriers in the code fragment.

\QuickQ{}
	But if the memory barriers do not unconditionally force
	ordering, how the heck can a device driver reliably execute
	sequences of loads and stores to MMIO registers?
\QuickA{}
	MMIO registers are special cases: because they appear
	in uncached regions of physical memory.
	Memory barriers \emph{do} unconditionally force ordering
	of loads and stores to uncached memory, as discussed in
	Section~\ref{sec:advsync:Device Operations}.

\QuickQ{}
	How do we know that modern hardware guarantees that at least
	one of the loads will see the value stored by the other thread
	in the ears-to-mouths scenario?
\QuickA{}
	The scenario is as follows, with A and B both initially zero:

	CPU~0: A=1; \co{smp_mb()}; r1=B;

	CPU~1: B=1; \co{smp_mb()}; r2=A;

	If neither of the loads see the corresponding store, when both
	CPUs finish, both \co{r1} and \co{r2} will be equal to zero.
	Let's suppose that \co{r1} is equal to zero.
	Then we know that CPU~0's load from B happened before CPU~1's
	store to B: After all, we would have had \co{r1} equal to one
	otherwise.
	But given that CPU~0's load from B happened before CPU~1's store
	to B, memory-barrier pairing guarantees that CPU~0's store to A
	happens before CPU~1's load from A, which in turn guarantees that
	\co{r2} will be equal to one, not zero.

	Therefore, at least one of \co{r1} and \co{r2} must be nonzero,
	which means that at least one of the loads saw the value from
	the corresponding store, as claimed.

\QuickQ{}
	How can the other ``Only one store'' entries in
	Table~\ref{tab:advsync:Memory-Barrier Combinations}
	be used?
\QuickA{}
	For combination~2, if CPU~1's load from B sees a value prior
	to CPU~2's store to B, then we know that CPU~2's load from A
	will return the same value as CPU~1's load from A, or some later
	value.

	For combination~4, if CPU~2's load from B sees the value from
	CPU~1's store to B, then we know that CPU~2's load from A
	will return the same value as CPU~1's load from A, or some later
	value.

	For combination~8, if CPU~2's load from A sees CPU~1's store
	to A, then we know that CPU~1's load from B will return the same
	value as CPU~2's load from A, or some later value.

\QuickQ{}
	How could the assertion {\tt b==2} on
	page~\pageref{codesample:advsync:What Can You Count On? 1}
	possibly fail?
\QuickA{}
	If the CPU is not required to see all of its loads and
	stores in order, then the {\tt b=1+a} might well see an
	old version of the variable ``a''.

	This is why it is so very important that each CPU or thread
	see all of its own loads and stores in program order.

\QuickQ{}
	How could the code on
	page~\pageref{codesample:advsync:What Can You Count On? 2}
	possibly leak memory?
\QuickA{}
	Only the first execution of the critical section should
	see {\tt p==NULL}.
	However, if there is no global ordering of critical sections for
	{\tt mylock}, then how can you say that a particular one was
	first?
	If several different executions of that critical section thought
	that they were first, they would all see {\tt p==NULL}, and
	they would all allocate memory.
	All but one of those allocations would be leaked.

	This is why it is so very important that all the critical sections
	for a given exclusive lock appear to execute in some well-defined
	order.

\QuickQ{}
	How could the code on
	page~\pageref{codesample:advsync:What Can You Count On? 2}
	possibly count backwards?
\QuickA{}
	Suppose that the counter started out with the value zero,
	and that three executions of the critical section had therefore
	brought its value to three.
	If the fourth execution of the critical section is not constrained
	to see the most recent store to this variable, it might well see
	the original value of zero, and therefore set the counter to
	one, which would be going backwards.

	This is why it is so very important that loads from a given variable
	in a given critical
	section see the last store from the last prior critical section to
	store to that variable.

\QuickQ{}
	What effect does the following sequence have on the
	order of stores to variables ``a'' and ``b''? \\
	{\tt ~~~~a = 1;} \\
	{\tt ~~~~b = 1;} \\
	{\tt ~~~~<write barrier>}
\QuickA{}
	Absolutely none.  This barrier {\em would} ensure that the
	assignments to ``a'' and ``b'' happened before any subsequent
	assignments, but it does nothing to enforce any order of
	assignments to ``a'' and ``b'' themselves.

\QuickQ{}
	What sequence of LOCK-UNLOCK operations \emph{would}
	act as a full memory barrier?
\QuickA{}
	A series of two back-to-back LOCK-UNLOCK operations, or, somewhat
	less conventionally, an UNLOCK operation followed by a LOCK
	operation.

\QuickQ{}
	What (if any) CPUs have memory-barrier instructions
	from which these semi-permeable locking primitives might
	be constructed?
\QuickA{}
	Itanium is one example.
	The identification of any others is left as an
	exercise for the reader.

\QuickQ{}
	Given that operations grouped in curly braces are executed
	concurrently, which of the rows of
	Table~\ref{tab:advsync:Lock-Based Critical Sections}
	are legitimate reorderings of the assignments to variables
	``A'' through ``F'' and the LOCK/UNLOCK operations?
	(The order in the code is A, B, LOCK, C, D, UNLOCK, E, F.)
	Why or why not?
\QuickA{}
	\begin{enumerate}
	\item	Legitimate, executed in order.
	\item	Legitimate, the lock acquisition was executed concurrently
		with the last assignment preceding the critical section.
	\item	Illegitimate, the assignment to ``F'' must follow the LOCK
		operation.
	\item	Illegitimate, the LOCK must complete before any operation in
		the critical section.  However, the UNLOCK may legitimately
		be executed concurrently with subsequent operations.
	\item	Legitimate, the assignment to ``A'' precedes the UNLOCK,
		as required, and all other operations are in order.
	\item	Illegitimate, the assignment to ``C'' must follow the LOCK.
	\item	Illegitimate, the assignment to ``D'' must precede the UNLOCK.
	\item	Legitimate, all assignments are ordered with respect to the
		LOCK and UNLOCK operations.
	\item	Illegitimate, the assignment to ``A'' must precede the UNLOCK.
	\end{enumerate}

\QuickQ{}
	What are the constraints for
	Table~\ref{tab:advsync:Ordering With Multiple Locks}?
\QuickA{}
	All CPUs must see the following ordering constraints:
	\begin{enumerate}
	\item	LOCK M precedes B, C, and D.
	\item	UNLOCK M follows A, B, and C.
	\item	LOCK Q precedes F, G, and H.
	\item	UNLOCK Q follows E, F, and G.
	\end{enumerate}

\QuickQAC{chp:Parallel Real-Time Computing}{Parallel Real-Time Computing}
\QuickQ{}
	But what about battery-powered systems?
	They don't require energy flowing into the system as a whole.
\QuickA{}
	Sooner or later, either the battery must be recharged, which
	requires energy to flow into the system, or the system will
	stop operating.

\QuickQ{}
	But given the results from queueing theory, won't low utilization
	merely improve the average response time rather than improving
	the worst-case response time, which is the only response time
	that many real-time systems care about?
\QuickA{}
	It depends.
	One situation where the worst-case response time is improved by
	lowering utilization is where there is only one real-time thread
	using the device in question, but where all the threads use
	the device in question.
	Restricting use of that device to the single real-time thread
	eliminates queueing delays, at least assuming that the
	real-time thread refrains from overdriving that device.

\QuickQ{}
	Formal verification is already quite capable, benefiting from
	decades of intensive study.
	Are additional advances \emph{really} required, or is this
	just a practitioner's excuse to continue to be lazy and ignore
	the awesome power of formal verification?
\QuickA{}
	Perhaps this situation is just a theoretician's excuse to avoid
	diving into the messy world of real software?
	Perhaps more constructively, the following advances are required:

	\begin{enumerate}
	\item	Formal verification needs to handle larger software
		artifacts.
		The largest verification efforts have been for systems
		of only about 10,000 lines of code, and those have been
		verifying much simpler properties than real-time latencies.
	\item	Hardware vendors will need to publish formal timing
		guarantees.
		This used to be common practice back when hardware was
		much simpler, but today's complex hardware results in
		excessively complex expressions for worst-case performance.
		Unfortunately, energy-efficiency concerns are pushing
		vendors in the direction of even more complexity.
	\item	Timing analysis needs to be integrated into development
		methodologies and IDEs.
	\end{enumerate}

	All that said, there is hope, given recent work formalizing
	the memory models of real computer
	systems~\cite{JadeAlglave2011ppcmem,Alglave:2013:SVW:2450268.2450306}.

\QuickQ{}
	Differentiating real-time from non-real-time based on what can
	``be achieved straightforwardly by non-real-time systems and
	applications'' is a travesty!
	There is absolutely no theoretical basis for such a distinction!!!
	Can't we do better than that???
\QuickA{}
	This distinction is admittedly unsatisfying from a strictly
	theoretical perspective.
	But on the other hand, it is exactly what the developer needs
	in order to decide whether the application can be cheaply and
	easily developed using standard non-real-time approaches, or
	whether the more difficult and expensive real-time approaches
	are required.
	In other words, theory is quite important, however, for those
	of us who like to get things done, theory supports practice,
	never the other way around.

\QuickQ{}
	But if you only allow one reader at a time to read-acquire
	a reader-writer lock, isn't that the same as an exclusive
	lock???
\QuickA{}
	Indeed it is, other than the API.
	And the API is important because it allows the Linux kernel
	to offer real-time capabilities without having the -rt patchset
	grow to ridiculous sizes.

	However, this approach clearly and severely limits read-side
	scalability.
	The Linux kernel's -rt patchset has been able to live with this
	limitation for several reasons: (1)~Real-time systems have
	traditionally been relatively small, (2)~Real-time systems
	have generally focused on process control, thus being unaffected
	by scalability limitations in the I/O subsystems, and
	(3)~Many of the Linux kernel's reader-writer locks have been
	converted to RCU.

	All that aside, it is quite possible that the Linux kernel
	will some day permit limited read-side parallelism for
	reader-writer locks subject to priority boosting.

\QuickQ{}
	Suppose that preemption occurs just after the load from
	\co{t->rcu_read_unlock_special.s} on line~17 of
	Figure~\ref{fig:rt:Preemptible Linux-Kernel RCU}.
	Mightn't that result in the task failing to invoke
	\co{rcu_read_unlock_special()}, thus failing to remove itself
	from the list of tasks blocking the current grace period,
	in turn causing that grace period to extend indefinitely?
\QuickA{}
	That is a real problem, and it is solved in RCU's scheduler hook.
	If that scheduler hook sees that the value of
	\co{t->rcu_read_lock_nesting} is negative, it invokes
	\co{rcu_read_unlock_special()} if needed before allowing
	the context switch to complete.

\QuickQ{}
	But isn't correct operation despite fail-stop bugs
	a valuable fault-tolerance property?
\QuickA{}
	Yes and no.

	Yes in that non-blocking algorithms can provide fault tolerance
	in the face of fail-stop bugs, but no in that this is grossly
	insufficient for practical fault tolerance.
	For example, suppose you had a wait-free queue, and further
	suppose that a thread has just dequeued an element.
	If that thread now succumbs to a fail-stop bug, the element
	it has just dequeued is effectively lost.
	True fault tolerance requires way more than mere non-blocking
	properties, and is beyond the scope of this book.

\QuickQ{}
	I couldn't help but spot the word ``includes'' before this list.
	Are there other constraints?
\QuickA{}
	Indeed there are, and lots of them.
	However, they tend to be specific to a given situation,
	and many of them can be thought of as refinements of some of
	the constraints listed above.
	For example, the many constraints on choices of data structure
	will help meeting the ``Bounded time spent in any given critical
	section'' constraint.

\QuickQ{}
	Given that real-time systems are often used for safety-critical
	applications, and given that runtime memory allocation is
	forbidden in many safety-critical situations, what is with
	the call to \co{malloc()}???
\QuickA{}
	In early 2016, situations forbidding runtime memory were
	also not soo excited with multithreaded computing.
	So the runtime memory allocation is not an additional
	obstacle to safety criticality.

\QuickQ{}
	Don't you need some kind of synchronization to protect
	\co{update_cal()}?
\QuickA{}
	Indeed you do, and you could use any of a number of techniques
	discussed earlier in this book.

\QuickQAC{chp:Ease of Use}{Ease of Use}
\QuickQ{}
	Can a similar algorithm be used when deleting elements?
\QuickA{}
	Yes.
	However, since each thread must hold the locks of three
	consecutive elements to delete the middle one, if there
	are $N$ threads, there must be $2N+1$ elements (rather than
	just $N+1$) in order to avoid deadlock.

\QuickQ{}
	Yetch!
	What ever possessed someone to come up with an algorithm
	that deserves to be shaved as much as this one does???
\QuickA{}
	That would be Paul.

	He was considering the \emph{Dining Philosopher's Problem}, which
	involves a rather unsanitary spaghetti dinner attended by
	five philosophers.
	Given that there are five plates and but five forks on the table, and
	given that each philosopher requires two forks at a time to eat,
	one is supposed to come up with a fork-allocation algorithm that
	avoids deadlock.
	Paul's response was ``Sheesh!  Just get five more forks!''.

	This in itself was OK, but Paul then applied this same solution to
	circular linked lists.

	This would not have been so bad either, but he had to go and tell
	someone about it!

\QuickQ{}
	Give an exception to this rule.
\QuickA{}
	One exception would be a difficult and complex algorithm that
	was the only one known to work in a given situation.
	Another exception would be a difficult and complex algorithm
	that was nonetheless the simplest of the set known to work in
	a given situation.
	However, even in these cases, it may be very worthwhile to spend
	a little time trying to come up with a simpler algorithm!
	After all, if you managed to invent the first algorithm
	to do some task, it shouldn't be that hard to go on to
	invent a simpler one.

\QuickQAC{chp:Conflicting Visions of the Future}{Conflicting Visions of the Future}
\QuickQ{}
	What about non-persistent primitives represented by data
	structures in \co{mmap()} regions of memory?
	What happens when there is an \co{exec()} within a critical
	section of such a primitive?
\QuickA{}
	If the \co{exec()}ed program maps those same regions of
	memory, then this program could in principle simply release
	the lock.
	The question as to whether this approach is sound from a
	software-engineering viewpoint is left as an exercise for
	the reader.

\QuickQ{}
	Why would it matter that oft-written variables shared the cache
	line with the lock variable?
\QuickA{}
	If the lock is in the same cacheline as some of the variables
	that it is protecting, then writes to those variables by one CPU
	will invalidate that cache line for all the other CPUs.
	These invalidations will
	generate large numbers of conflicts and retries, perhaps even
	degrading performance and scalability compared to locking.

\QuickQ{}
	Why are relatively small updates important to HTM performance
	and scalability?
\QuickA{}
	The larger the updates, the greater the probability of conflict,
	and thus the greater probability of retries, which degrade
	performance.

\QuickQ{}
	How could a red-black tree possibly efficiently enumerate all
	elements of the tree regardless of choice of synchronization
	mechanism???
\QuickA{}
	In many cases, the enumeration need not be exact.
	In these cases, hazard pointers or RCU may be used to protect
	readers with low probability of conflict with any given insertion
	or deletion.

\QuickQ{}
	But why can't a debugger emulate single stepping by setting
	breakpoints at successive lines of the transaction, relying
	on the retry to retrace the steps of the earlier instances
	of the transaction?
\QuickA{}
	This scheme might work with reasonably high probability, but it
	can fail in ways that would be quite surprising to most users.
	To see this, consider the following transaction:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\small
\begin{verbatim}
  1 begin_trans();
  2 if (a) {
  3   do_one_thing();
  4   do_another_thing();
  5 } else {
  6   do_a_third_thing();
  7   do_a_fourth_thing();
  8 }
  9 end_trans();
\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	Suppose that the user sets a breakpoint at line 3, which triggers,
	aborting the transaction and entering the debugger.
	Suppose that between the time that the breakpoint triggers
	and the debugger gets around to stopping all the threads, some
	other thread sets the value of \co{a} to zero.
	When the poor user attempts to single-step the program, surprise!
	The program is now in the else-clause instead of the then-clause.

	This is \emph{not} what I call an easy-to-use debugger.

\QuickQ{}
	But why would \emph{anyone} need an empty lock-based critical
	section???
\QuickA{}
	See the answer to the Quick Quiz in
	Section~\ref{sec:locking:Exclusive Locks}.

	However, it is claimed that given a strongly atomic HTM
	implementation without forward-progress guarantees, any
	memory-based locking design based on empty critical sections
	will operate correctly in the presence of transactional
	lock elision.
	Although I have not seen a proof of this statement, there
	is a straightforward rationale for this claim.
	The main idea is that in a strongly atomic HTM implementation,
	the results of a given transaction are not visible until
	after the transaction completes successfully.
	Therefore, if you can see that a transaction has started,
	it is guaranteed to have already completed, which means
	that a subsequent empty lock-based critical section will
	successfully ``wait'' on it---after all, there is no waiting
	required.

	This line of reasoning does not apply to weakly atomic
	systems (including many STM implementation), and it also
	does not apply to lock-based programs that use means other
	than memory to communicate.
	One such means is the passage of time (for example, in
	hard real-time systems) or flow of priority (for example,
	in soft real-time systems).

	Locking designs that rely on priority boosting are of particular
	interest.

\QuickQ{}
	Can't transactional lock elision trivially handle locking's
	time-based messaging semantics
	by simply choosing not to elide empty lock-based critical sections?
\QuickA{}
	It could do so, but this would be both unnecessary and
	insufficient.

	It would be unnecessary in cases where the empty critical section
	was due to conditional compilation.
	Here, it might well be that the only purpose of the lock was to
	protect data, so eliding it completely would be the right thing
	to do.
	In fact, leaving the empty lock-based critical section would
	degrade performance and scalability.

	On the other hand, it is possible for a non-empty lock-based
	critical section to be relying on both the data-protection
	and time-based and messaging semantics of locking.
	Using transactional lock elision in such a case would be
	incorrect, and would result in bugs.

\QuickQ{}
	Given modern hardware~\cite{PeterOkech2009InherentRandomness},
	how can anyone possibly expect parallel software relying
	on timing to work?
\QuickA{}
	The short answer is that on commonplace commodity hardware,
	synchronization designs based on any sort of fine-grained
	timing are foolhardy and cannot be expected to operate correctly
	under all conditions.

	That said, there are systems designed for hard real-time use
	that are much more deterministic.
	In the (very unlikely) event that you are using such a system,
	here is a toy example showing how time-based synchronization can
	work.
	Again, do \emph{not} try this on commodity microprocessors,
	as they have highly nondeterministic performance characteristics.

	This example uses multiple worker threads along with a control
	thread.
	Each worker thread corresponds to an outbound data feed, and
	records the current time (for example, from the
	\co{clock_gettime()} system call) in a per-thread
	\co{my_timestamp} variable after executing each unit
	of work.
	The real-time nature of this example results in the following
	set of constraints:

	\begin{enumerate}
	\item	It is a fatal error for a given worker thread to fail
		to update its timestamp for a time period of more than
		\co{MAX_LOOP_TIME}.
	\item	Locks are used sparingly to access and update global
		state.
	item	Locks are granted in strict FIFO order within
		a given thread priority.
	\end{enumerate}

	When worker threads complete their feed, they must disentangle
	themselves from the rest of the application and place a status
	value in a per-thread \co{my_status} variable that is initialized
	to -1.
	Threads do not exit; they instead are placed on a thread pool
	to accommodate later processing requirements.
	The control thread assigns (and re-assigns) worker threads as
	needed, and also maintains a histogram of thread statuses.
	The control thread runs at a real-time priority no higher than
	that of the worker threads.

	Worker threads' code is as follows:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
\begin{verbatim}
  1   int my_status = -1;  /* Thread local. */
  2 
  3   while (continue_working()) {
  4     enqueue_any_new_work();
  5     wp = dequeue_work();
  6     do_work(wp);
  7     my_timestamp = clock_gettime(...);
  8   }
  9 
 10   acquire_lock(&departing_thread_lock);
 11 
 12   /*
 13    * Disentangle from application, might
 14    * acquire other locks, can take much longer
 15    * than MAX_LOOP_TIME, especially if many
 16    * threads exit concurrently.
 17    */
 18   my_status = get_return_status();
 19   release_lock(&departing_thread_lock);
 20 
 21   /* thread awaits repurposing. */
\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	The control thread's code is as follows:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
\begin{verbatim}
  1   for (;;) {
  2     for_each_thread(t) {
  3       ct = clock_gettime(...);
  4       d = ct - per_thread(my_timestamp, t);
  5       if (d >= MAX_LOOP_TIME) {
  6         /* thread departing. */
  7         acquire_lock(&departing_thread_lock);
  8         release_lock(&departing_thread_lock);
  9         i = per_thread(my_status, t);
 10         status_hist[i]++; /* Bug if TLE! */
 11       }
 12     }
 13     /* Repurpose threads as needed. */
 14   }
\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	Line~5 uses the passage of time to deduce that the thread
	has exited, executing lines~6-10 if so.
	The empty lock-based critical section on lines~7 and~8
	guarantees that any thread in the process of exiting
	completes (remember that locks are granted in FIFO order!).

	Once again, do not try this sort of thing on commodity
	microprocessors.
	After all, it is difficult enough to get right on systems
	specifically designed for hard real-time use!

\QuickQ{}
	But the \co{boostee()} function in
	Figure~\ref{fig:future:Exploiting Priority Boosting}
	alternatively acquires its locks in reverse order!
	Won't this result in deadlock?
\QuickA{}
	No deadlock will result.
	To arrive at deadlock, two different threads must each
	acquire the two locks in oppposite orders, which does not
	happen in this example.
	However, deadlock detectors such as
	lockdep~\cite{JonathanCorbet2006lockdep}
	will flag this as a false positive.

\QuickQ{}
	So a bunch of people set out to supplant locking, and they
	mostly end up just optimizing locking???
\QuickA{}
	At least they accomplished something useful!
	And perhaps there will be additional HTM progress over time.

\QuickQAC{cha:app:Important Questions}{Important Questions}
\QuickQ{}
	What SMP coding errors can you see in these examples?
	See \url{time.c} for full code.
\QuickA{}
	\begin{enumerate}
	\item	Missing barrier() or volatile on tight loops.
	\item	Missing Memory barriers on update side.
	\item	Lack of synchronization between producer and consumer.
	\end{enumerate}

\QuickQ{}
	How could there be such a large gap between successive
	consumer reads?
	See \url{timelocked.c} for full code.
\QuickA{}
	\begin{enumerate}
	\item	The consumer might be preempted for long time periods.
	\item	A long-running interrupt might delay the consumer.
	\item	The producer might also be running on a faster CPU than is the
		consumer (for example, one of the CPUs might have had to
		decrease its
		clock frequency due to heat-dissipation or power-consumption
		constraints).
	\end{enumerate}

\QuickQ{}
	Suppose a portion of a program uses RCU read-side primitives
	as its only synchronization mechanism.
	Is this parallelism or concurrency?
\QuickA{}
	Yes.

\QuickQ{}
	In what part of the second (scheduler-based) perspective would
	the lock-based single-thread-per-CPU workload be considered
	``concurrent''?
\QuickA{}
	The people who would like to arbitrarily subdivide and interleave
	the workload.
	Of course, an arbitrary subdivision might end up separating
	a lock acquisition from the corresponding lock release, which
	would prevent any other thread from acquiring that lock.
	If the locks were pure spinlocks, this could even result in
	deadlock.

\QuickQAC{app:primitives:Synchronization Primitives}{Synchronization Primitives}
\QuickQ{}
	Give an example of a parallel program that could be written
	without synchronization primitives.
\QuickA{}
	There are many examples.
	One of the simplest would be a parametric study using a
	single independent variable.
	If the program {\tt run\_study} took a single argument,
	then we could use the following bash script to run two
	instances in parallel, as might be appropriate on a
	two-CPU system:

	{ \scriptsize \tt run\_study 1 > 1.out\& run\_study 2 > 2.out; wait}

	One could of course argue that the bash ampersand operator and
	the ``wait'' primitive are in fact synchronization primitives.
	If so, then consider that
	this script could be run manually in two separate
	command windows, so that the only synchronization would be
	supplied by the user himself or herself.

\QuickQ{}
	What problems could occur if the variable {\tt counter} were
	incremented without the protection of {\tt mutex}?
\QuickA{}
	On CPUs with load-store architectures, incrementing {\tt counter}
	might compile into something like the following:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
LOAD counter,r0
INC r0
STORE r0,counter
\end{verbatim}
\end{minipage}
\vspace{5pt}

	On such machines, two threads might simultaneously load the
	value of {\tt counter}, each increment it, and each store the
	result.
	The new value of {\tt counter} will then only be one greater
	than before, despite two threads each incrementing it.

\QuickQ{}
	How could you work around the lack of a per-thread-variable
	API on systems that do not provide it?
\QuickA{}
	One approach would be to create an array indexed by
	{\tt smp\_thread\_id()}, and another would be to use a hash
	table to map from {\tt smp\_thread\_id()} to an array
	index --- which is in fact what this
	set of APIs does in pthread environments.

	Another approach would be for the parent to allocate a structure
	containing fields for each desired per-thread variable, then
	pass this to the child during thread creation.
	However, this approach can impose large software-engineering
	costs in large systems.
	To see this, imagine if all global variables in a large system
	had to be declared in a single file, regardless of whether or
	not they were C static variables!

\QuickQAC{chp:app:whymb:Why Memory Barriers?}{Why Memory Barriers?}
\QuickQ{}
	Writeback 메세지는 어디서 와서 어디로 가나요?
	\iffalse
	Where does a writeback message originate from and where does
	it go to?
	\fi
\QuickA{}
	Writeback 메세지는 해당 CPU 에서, 또는 일부 설계에서는 해당 CPU 의
	캐시의 해당 레벨에서 발생합니다---또는 심지어 여러 CPU 들 사이에 공유된
	캐시에서도.
	핵심은 해당 캐시는 현재 데이터 아이템을 위한 공간이 없어서 공간을
	만들기 위해 캐시에서 일부 데이터를 제거해야 한다는 겁니다.
	다른 캐시나 메모리에 데이터의 복사본이 일부 존재한다면, 그 부분은
	writeback 메세지 필요 없이 그냥 버려질 수도 있습니다.

	반면, 만약 제거될 데이터의 모든 부분이 수정된 상태여서 최신 버전이 이
	캐시에만 존재한다면, 그런 데이터 아이템들은 어딘가 다른곳에
	복사되어야만 합니다.
	이 복사 과정이 ``writeback 메세지'' 를 통해 이루어집니다.
	\iffalse

	The writeback message originates from a given CPU, or in some
	designs from a given level of a given CPU's cache---or even
	from a cache that might be shared among several CPUs.
	The key point is that a given cache does not have room for
	a given data item, so some other piece of data must be ejected
	from the cache to make room.
	If there is some other piece of data that is duplicated in some
	other cache or in memory, then that piece of data may be simply
	discarded, with no writeback message required.

	On the other hand, if every piece of data that might be ejected
	has been modified so that the only up-to-date copy is in this
	cache, then one of those data items must be copied somewhere
	else.
	This copy operation is undertaken using a ``writeback message''.
	\fi

	Writeback 메세지의 목적지는 새 값을 쓸 수 있는 곳이어야 합니다.
	이는 메인 메모리가 될 수도 있지만, 다른 캐시일 수도 있습니다.
	만약 그게 캐시라면, 그 캐시는 보통 같은 CPU 의 높은 레벨 캐시로, 예를
	들어, 레벨-1 캐시는 레벨-2 캐시에 writeback 을 할 수도 있습니다.
	하지만, 일부 하드웨어 설계는 CPU 간 writeback 을 허용해서, CPU~0 의
	캐시는 CPU~1 에 writeback 메세지를 날릴 수 있ㅅ브니다.
	이는 보통 CPU~1 이 어떻게든, 예를 들어, 최근에 읽기 리퀘스트를
	했다던지와 같이 그 데이터에 흥미를 표했다면 행해질 수 있습니다.

	한마디로, writeback 메세지는 공간이 부족한 시스템의 어떤 부분에서
	보내질 수 있고, 그 데이터를 수용할 수 있는 시스템의 다른 부분에서 받게
	됩니다.
	\iffalse

	The destination of the writeback message has to be something
	that is able to store the new value.
	This might be main memory, but it also might be some other cache.
	If it is a cache, it is normally a higher-level cache for the
	same CPU, for example, a level-1 cache might write back to a
	level-2 cache.
	However, some hardware designs permit cross-CPU writebacks,
	so that CPU~0's cache might send a writeback message to CPU~1.
	This would normally be done if CPU~1 had somehow indicated
	an interest in the data, for example, by having recently
	issued a read request.

	In short, a writeback message is sent from some part of the
	system that is short of space, and is received by some other
	part of the system that can accommodate the data.
	\fi

\QuickQ{}
	두개의 CPU 들이 같은 캐시 라인을 동시에 무효화 하려고 하면 어떻게
	되나요?
	\iffalse

	What happens if two CPUs attempt to invalidate the
	same cache line concurrently?
	\fi
\QuickA{}
	그 CPU 들 중 하나가 먼저 공유 버스에의 액세스를 얻고, 그 CPU 가
	``이깁니다''.
	이기지 못한 CPU 는 해당 캐시라인의 카피를 무효화 시키고 ``invalidate
	acknowlege'' 메세지를 이긴 CPU 에게 보내야 합니다. \\
	물론, 진 CPU 는 곧바로 ``read invalidate'' 요청을 보낼 것이라 예상할 수
	있고, 따라서 이긴 CPU 의 승리는 덧없는 것이 될 겁니다.
	\iffalse

	One of the CPUs gains access
	to the shared bus first,
	and that CPU ``wins''.  The other CPU must invalidate its copy of the
	cache line and transmit an ``invalidate acknowledge'' message
	to the other CPU. \\
	Of course, the losing CPU can be expected to immediately issue a
	``read invalidate'' transaction, so the winning CPU's victory will
	be quite ephemeral.
	\fi

\QuickQ{}
	커다란 멀티프로세서에서 ``invalidate'' 메세지가 생기면, 모든 CPU 가
	``invalidatge acknowledge'' 응답을 보내야만 합니다.
	그로 인한 ``invalidate acknowledge'' 의 ``폭풍우'' 가 시스템 버스를
	완전히 뒤덮지 않을까요?
	\iffalse

	When an ``invalidate'' message appears in a large multiprocessor,
	every CPU must give an ``invalidate acknowledge'' response.
	Wouldn't the resulting ``storm'' of ``invalidate acknowledge''
	responses totally saturate the system bus?
	\fi
\QuickA{}
	커다란 규모의 멀티프로세서가 그렇게 구현되어 있다면 그렇겠죠.
	커다란 멀티프로세서들, 특히 NUMA 구조의 경우에는, ``dicrectory-based''
	라 불리는 캐시 일관성 프로토콜을 사용해서 이런 문제 등의 여러 문제들이
	나타나지 않게 합니다.
	\iffalse

	It might, if large-scale multiprocessors were in fact implemented
	that way.  Larger multiprocessors, particularly NUMA machines,
	tend to use so-called ``directory-based'' cache-coherence
	protocols to avoid this and other problems.
	\fi

\QuickQ{}
	SMP 머신들이 실제로 메세지 전달을 어떻게든 사용한다면, 애초에 왜 SMP 에
	신경을 쓰는거죠?
	\iffalse

	If SMP machines are really using message passing
	anyway, why bother with SMP at all?
	\fi
\QuickA{}
	이 주제에 대해서는 지난 수십년 동안 상당한 논쟁이 있었습니다.
	한 대답은 캐시 일관성 프로토콜은 상당히 간단해서 하드웨어만으로
	구현되어 소프트웨어 메세지 전달로는 얻을 수 없는 대역폭과 대기시간을
	얻을 수 있다는 겁니다.
	또다른 대답은 진정한 사실은 커다란 SMP 머신과 작은 SMP 머신의
	클러스터의 상대적 가격으로 인한 경제 규모에서 찾을 수 있습니다.
	세번째 대답은 SMP 프로그래밍 모델이 분산 시스템의 것보다 쉽다는
	겁니다만 HPC 클러스터들과 MPI 의 출현을 가지고 반박할 수도 있겠습니다.
	그렇게 논쟁은 계속되는거죠.
	\iffalse

	There has been quite a bit of controversy on this topic over
	the past few decades.  One answer is that the cache-coherence
	protocols are quite simple, and therefore can be implemented
	directly in hardware, gaining bandwidths and latencies
	unattainable by software message passing.  Another answer is that
	the real truth is to be found in economics due to the relative
	prices of large SMP machines and that of clusters of smaller
	SMP machines.  A third answer is that the SMP programming
	model is easier to use than that of distributed systems, but
	a rebuttal might note the appearance of HPC clusters and MPI.
	And so the argument continues.
	\fi

\QuickQ{}
	앞에 설명된 지연되는 상태 변경들은 하드웨어에서 어떻게 처리하나요?
	\iffalse

	How does the hardware handle the delayed transitions
	described above?
	\fi
\QuickA{}
	추가적인 상태를 더해서 처리합니다만, 한번에 일부 라인들만 상태 변경을
	한다는 사실 때문에 이 추가적인 상태들이 실제로 캐시 라인에 쓰여질
	필요는 없습니다.
	상태 변경을 지연해야 하는 요구사항은 실제 세계 캐시 일관성 프로토콜을
	이 부록에서 설명된 지나치게 간략화된 MESI 프로토콜에 비해 훨씬 복잡하게
	만드는 문제 중 하나입니다.
	Henessy 와 Patterson 의 컴퓨터 구조에 대한 오래된 소개
	문서~\cite{Hennessy95a} 에서 이런 문제들을 다룹니다.
	\iffalse

	Usually by adding additional states, though these additional
	states need not be actually stored with the cache line, due to
	the fact that only a few lines at a time will be transitioning.
	The need to delay transitions is but one issue that results in
	real-world cache coherence protocols being much more complex than
	the over-simplified MESI protocol described in this appendix.
	Hennessy and Patterson's classic introduction to computer
	architecture~\cite{Hennessy95a} covers many of these issues.
	\fi

\QuickQ{}
	What sequence of operations would put the CPUs' caches
	all back into the ``invalid'' state?
\QuickA{}
	There is no such sequence, at least in absence of special
	``flush my cache'' instructions in the CPU's instruction set.
	Most CPUs do have such instructions.

\QuickQ{}
	But if the main purpose of store buffers is to hide acknowledgment
	latencies in multiprocessor cache-coherence protocols, why
	do uniprocessors also have store buffers?
\QuickA{}
	Because the purpose of store buffers is not just to hide
	acknowledgement latencies in multiprocessor cache-coherence protocols,
	but to hide memory latencies in general.
	Because memory is much slower than is cache on uniprocessors,
	store buffers on uniprocessors can help to hide write-miss
	latencies.

\QuickQ{}
	In step~1 above, why does CPU~0 need to issue a ``read invalidate''
	rather than a simple ``invalidate''?
\QuickA{}
	Because the cache line in question contains more than just the
	variable \co{a}.

\QuickQ{}
	In step~1 of the first scenario in
	Section~\ref{sec:app:whymb:Invalidate Queues and Memory Barriers},
	why is an ``invalidate'' sent instead of a ''read invalidate''
	message?
	Doesn't CPU~0 need the values of the other variables that share
	this cache line with ``a''?
\QuickA{}
	CPU~0 already has the values of these variables, given that it
	has a read-only copy of the cache line containing ``a''.
	Therefore, all CPU~0 need do is to cause the other CPUs to discard
	their copies of this cache line.
	An ``invalidate'' message therefore suffices.

\QuickQ{}
	Say what???
	Why do we need a memory barrier here, given that the CPU cannot
	possibly execute the \co{assert()} until after the
	\co{while} loop completes?
\QuickA{}
	CPUs are free to speculatively execute, which can have the effect
	of executing the assertion before the \co{while} loop completes.
	Furthermore, compilers normally assume that only the currently
	executing thread is updating the variables, and this assumption
	allows the compiler to hoist the load of \co{a} to precede the
	loop.

	In fact, some compilers would transform the loop to a branch
	around an infinite loop as follows:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
  1 void foo(void)
  2 {
  3   a = 1;
  4   smp_mb();
  5   b = 1;
  6 }
  7 
  8 void bar(void)
  9 {
 10   if (b == 0)
 11     for (;;)
 12       continue;
 13   smp_mb();
 14   assert(a == 1);
 15 }
\end{verbatim}
\end{minipage}
\vspace{5pt}

	Given this optimization, the assertion could clearly fire.
	You should use volatile casts or (where available) C++
	relaxed atomics to prevent the compiler from optimizing
	your parallel code into oblivion.

	In short, both compilers and CPUs are quite aggressive about
	optimizing, so you must clearly communicate your constraints
	to them, using compiler directives and memory barriers.


\QuickQ{}
	Does the guarantee that each CPU sees its own memory accesses
	in order also guarantee that each user-level thread will see
	its own memory accesses in order?
	Why or why not?
\QuickA{}
	No.  Consider the case where a thread migrates from one CPU to
	another, and where the destination CPU perceives the source
	CPU's recent memory operations out of order.  To preserve
	user-mode sanity, kernel hackers must use memory barriers in
	the context-switch path.  However, the locking already required
	to safely do a context switch should automatically provide
	the memory barriers needed to cause the user-level task to see
	its own accesses in order.  That said, if you are designing a
	super-optimized scheduler, either in the kernel or at user level,
	please keep this scenario in mind!

\QuickQ{}
	Could this code be fixed by inserting a memory barrier
	between CPU~1's ``while'' and assignment to ``c''?
	Why or why not?
\QuickA{}
	No.  Such a memory barrier would only force ordering local to CPU~1.
	It would have no effect on the relative ordering of CPU~0's and
	CPU~1's accesses, so the assertion could still fail.
	However, all mainstream computer systems provide one mechanism
	or another to provide ``transitivity'', which provides
	intuitive causal ordering: if B saw the effects of A's accesses,
	and C saw the effects of B's accesses, then C must also see
	the effects of A's accesses.
	In short, hardware designers have taken at least a little pity
	on software developers.

\QuickQ{}
	Suppose that lines~3-5 for CPUs~1 and 2 in
	Table~\ref{tab:app:whymb:Memory Barrier Example 3}
	are in an interrupt
	handler, and that the CPU~2's line~9 is run at process level.
	What changes, if any, are required to enable the code to work
	correctly, in other words, to prevent the assertion from firing?
\QuickA{}
	The assertion will need to written to ensure that the load of
	``e'' precedes that of ``a''.
	In the Linux kernel, the barrier() primitive may be used to accomplish
	this in much the same way that the memory barrier was used in the
	assertions in the previous examples.

\QuickQ{}
	If CPU~2 executed an \co{assert(e==0||c==1)} in the example in
	Table~\ref{tab:app:whymb:Memory Barrier Example 3},
	would this assert ever trigger?
\QuickA{}
	The result depends on whether the CPU supports ``transitivity.''
	In other words, CPU~0 stored to ``e'' after seeing CPU~1's
	store to ``c'', with a memory barrier between CPU~0's load
	from ``c'' and store to ``e''.
	If some other CPU sees CPU~0's store to ``e'', is it also
	guaranteed to see CPU~1's store?

	All CPUs I am aware of claim to provide transitivity.

\QuickQ{}
	Why is Alpha's \co{smp_read_barrier_depends()} an
	\co{smp_mb()} rather than \co{smp_rmb()}?
\QuickA{}
	First, Alpha has only \co{mb} and \co{wmb} instructions,
	so \co{smp_rmb()} would be implemented by the Alpha \co{mb}
	instruction in either case.

	More importantly, \co{smp_read_barrier_depends()} must
	order subsequent stores.
	For example, consider the following code:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
  1 p = global_pointer;
  2 smp_read_barrier_depends();
  3 if (do_something_with(p->a, p->b) == 0)
  4   p->hey_look = 1;
\end{verbatim}
\end{minipage}
\vspace{5pt}

	Here the store to \co{p->hey_look} must be ordered,
	not just the loads from \co{p->a} and \co{p->b}.

